{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mathematical expression of the algorithm**:\n",
    "\n",
    "For one example $x^{(i)}$:\n",
    "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
    "$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$ \n",
    "$$ \\mathcal{L}(yhat^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(yhat^{(i)}) - (1-y^{(i)} )  \\log(1-yhat^{(i)})\\tag{3}$$\n",
    "\n",
    "The cost is then computing:\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(yhat^{(i)}, y^{(i)})\\tag{6}$$\n",
    "\n",
    "Gradient Computing:\n",
    "- $$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(yhat-Y)^T\\tag{7}$$\n",
    "- $$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (yhat^{(i)}-y^{(i)})\\tag{8}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.randn(3,2)\n",
    "\n",
    "B = np.sum(A, axis = 1, keepdims = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight and bias initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_b_init(dim: int):\n",
    "    w = tf.zeros([dim, 1], dtype=tf.float64)\n",
    "    b = 0.0\n",
    "    return w, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 1), dtype=float64, numpy=\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.]])>,\n",
       " 0.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, b = w_b_init(3)\n",
    "w, b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate $z$ for all $x^{(i)}$:\n",
    "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_calc(w: tf.Tensor, b: tf.float64, x: tf.Tensor):\n",
    "    z = tf.tensordot(tf.transpose(w), x, axes=1) + b\n",
    "    return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float64, numpy=array([0.])>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w , b = w_b_init(3)\n",
    "x = tf.Variable([3, 2, 4], dtype=tf.float64)\n",
    "z_calc(w, b, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid Funtion\n",
    "compute $sigmoid(z) = \\frac{1}{1 + e^{-z}}$ for $z = w^T x + b$ to make predictions. Use np.exp() or tf.exp()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z: tf.Tensor) -> tf.Tensor:\n",
    "    s = 1/(1 + tf.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float64, numpy=array([0.5])>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = z_calc(w, b, x)\n",
    "sigmoid(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the Cost :\n",
    " $J = -\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}\\log(yhat^{(i)})+(1-y^{(i)})\\log(1-yhat^{(i)}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(yhat:tf.Tensor, y:tf.Tensor):\n",
    "    loss = tf.reduce_sum((y * tf.math.log(yhat)) + ((1-y) * tf.math.log(1-yhat)))\n",
    "    c = (-1/yhat.shape[0]) * loss\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation:\n",
    "- You get X\n",
    "- You compute $yhat = \\sigma(w^T X + b) $\n",
    "- You calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}\\log(yhat^{(i)})+(1-y^{(i)})\\log(1-yhat^{(i)}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(w: tf.Tensor, b: tf.float64, x: tf.Tensor, y: tf.Tensor):\n",
    "    z = z_calc(w, b, x)\n",
    "    yhat = sigmoid(z)\n",
    "    c = cost(yhat, y)\n",
    "    return yhat, tf.squeeze(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation: \n",
    "\n",
    "- $$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(yhat-y)^T\\tag{7}$$\n",
    "- $$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (yhat^{(i)}-y^{(i)})\\tag{8}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(x: tf.Tensor, y: tf.Tensor, yhat: tf.Tensor):\n",
    "    m = y.shape[0]\n",
    "    dw = (1/m) * tf.tensordot(x, tf.transpose(yhat - y), axes=1)\n",
    "    db = (1/m) * tf.reduce_sum(yhat - y)\n",
    "    return {'dw': dw, 'db': db}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, x, y, num_iterations=100, learning_rate=0.009):\n",
    "\n",
    "    for epoch in range(num_iterations):\n",
    "        yhat, loss = forward_propagation(w, b, x, y)\n",
    "        grads = back_propagation(x, y, yhat)\n",
    "        w = w - learning_rate * grads['dw']\n",
    "        b = b - learning_rate * grads['db']\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch: {epoch} => Loss: {loss}\")\n",
    "    return w, b, grads, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 => Loss: 0.47701613123077213\n",
      "Epoch: 10 => Loss: 0.4164531274535226\n",
      "Epoch: 20 => Loss: 0.367523498948689\n",
      "Epoch: 30 => Loss: 0.3275456740982752\n",
      "Epoch: 40 => Loss: 0.2945102061378818\n",
      "Epoch: 50 => Loss: 0.26691094275228944\n",
      "Epoch: 60 => Loss: 0.24361332072642702\n",
      "Epoch: 70 => Loss: 0.2237560544308519\n",
      "Epoch: 80 => Loss: 0.20667926683465648\n",
      "Epoch: 90 => Loss: 0.19187237397489834\n",
      "Epoch: 100 => Loss: 0.17893631895908704\n",
      "Epoch: 110 => Loss: 0.167556102524381\n",
      "Epoch: 120 => Loss: 0.15748067635971635\n",
      "Epoch: 130 => Loss: 0.14850811044843798\n",
      "Epoch: 140 => Loss: 0.14047455659043903\n",
      "Epoch: 150 => Loss: 0.13324596273006736\n",
      "Epoch: 160 => Loss: 0.12671179611607164\n",
      "Epoch: 170 => Loss: 0.12078024571170254\n",
      "Epoch: 180 => Loss: 0.1153745232473739\n",
      "Epoch: 190 => Loss: 0.11042998727123679\n",
      "Epoch: 200 => Loss: 0.10589188895569433\n",
      "Epoch: 210 => Loss: 0.10171359152781864\n",
      "Epoch: 220 => Loss: 0.09785515338863691\n",
      "Epoch: 230 => Loss: 0.09428219267242786\n",
      "Epoch: 240 => Loss: 0.0909649712233753\n",
      "Epoch: 250 => Loss: 0.08787765085935872\n",
      "Epoch: 260 => Loss: 0.08499768584279371\n",
      "Epoch: 270 => Loss: 0.08230532373964133\n",
      "Epoch: 280 => Loss: 0.07978319306936461\n",
      "Epoch: 290 => Loss: 0.07741596086766094\n",
      "Epoch: 300 => Loss: 0.07519004688783387\n",
      "Epoch: 310 => Loss: 0.07309338393745153\n",
      "Epoch: 320 => Loss: 0.07111521599074429\n",
      "Epoch: 330 => Loss: 0.06924592738615201\n",
      "Epoch: 340 => Loss: 0.06747689772540502\n",
      "Epoch: 350 => Loss: 0.06580037811986968\n",
      "Epoch: 360 => Loss: 0.06420938524502097\n",
      "Epoch: 370 => Loss: 0.0626976103128104\n",
      "Epoch: 380 => Loss: 0.06125934059089404\n",
      "Epoch: 390 => Loss: 0.05988939151509914\n",
      "Epoch: 400 => Loss: 0.0585830477787184\n",
      "Epoch: 410 => Loss: 0.057336012055799135\n",
      "Epoch: 420 => Loss: 0.05614436023856987\n",
      "Epoch: 430 => Loss: 0.055004502251607475\n",
      "Epoch: 440 => Loss: 0.053913147655264164\n",
      "Epoch: 450 => Loss: 0.05286727537452195\n",
      "Epoch: 460 => Loss: 0.051864106991828564\n",
      "Epoch: 470 => Loss: 0.05090108312752093\n",
      "Epoch: 480 => Loss: 0.049975842502370374\n",
      "Epoch: 490 => Loss: 0.049086203336117865\n",
      "Epoch: 500 => Loss: 0.04823014678566774\n",
      "Epoch: 510 => Loss: 0.04740580216853033\n",
      "Epoch: 520 => Loss: 0.046611433752524134\n",
      "Epoch: 530 => Loss: 0.04584542892272014\n",
      "Epoch: 540 => Loss: 0.04510628756209828\n",
      "Epoch: 550 => Loss: 0.044392612504070755\n",
      "Epoch: 560 => Loss: 0.04370310093355805\n",
      "Epoch: 570 => Loss: 0.04303653662916018\n",
      "Epoch: 580 => Loss: 0.04239178295258551\n",
      "Epoch: 590 => Loss: 0.041767776503209536\n",
      "Epoch: 600 => Loss: 0.04116352136573491\n",
      "Epoch: 610 => Loss: 0.04057808388766301\n",
      "Epoch: 620 => Loss: 0.040010587930845484\n",
      "Epoch: 630 => Loss: 0.03946021054794815\n",
      "Epoch: 640 => Loss: 0.038926178040383515\n",
      "Epoch: 650 => Loss: 0.038407762359235945\n",
      "Epoch: 660 => Loss: 0.03790427781505852\n",
      "Epoch: 670 => Loss: 0.037415078066228305\n",
      "Epoch: 680 => Loss: 0.03693955335888209\n",
      "Epoch: 690 => Loss: 0.0364771279943852\n",
      "Epoch: 700 => Loss: 0.036027258002866956\n",
      "Epoch: 710 => Loss: 0.035589429003629255\n",
      "Epoch: 720 => Loss: 0.03516315423523979\n",
      "Epoch: 730 => Loss: 0.03474797273990311\n",
      "Epoch: 740 => Loss: 0.03434344768826324\n",
      "Epoch: 750 => Loss: 0.033949164832198\n",
      "Epoch: 760 => Loss: 0.03356473107439836\n",
      "Epoch: 770 => Loss: 0.03318977314462895\n",
      "Epoch: 780 => Loss: 0.032823936373549155\n",
      "Epoch: 790 => Loss: 0.032466883555850486\n",
      "Epoch: 800 => Loss: 0.03211829389525189\n",
      "Epoch: 810 => Loss: 0.03177786202459049\n",
      "Epoch: 820 => Loss: 0.03144529709487566\n",
      "Epoch: 830 => Loss: 0.031120321927740648\n",
      "Epoch: 840 => Loss: 0.030802672226227887\n",
      "Epoch: 850 => Loss: 0.030492095839295054\n",
      "Epoch: 860 => Loss: 0.030188352075857208\n",
      "Epoch: 870 => Loss: 0.029891211064517594\n",
      "Epoch: 880 => Loss: 0.029600453155509118\n",
      "Epoch: 890 => Loss: 0.029315868361641132\n",
      "Epoch: 900 => Loss: 0.0290372558353327\n",
      "Epoch: 910 => Loss: 0.028764423379056325\n",
      "Epoch: 920 => Loss: 0.028497186986743347\n",
      "Epoch: 930 => Loss: 0.02823537041389277\n",
      "Epoch: 940 => Loss: 0.0279788047743308\n",
      "Epoch: 950 => Loss: 0.02772732816171082\n",
      "Epoch: 960 => Loss: 0.027480785294017086\n",
      "Epoch: 970 => Loss: 0.027239027179461688\n",
      "Epoch: 980 => Loss: 0.027001910802295304\n",
      "Epoch: 990 => Loss: 0.026769298827165833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 1), dtype=float64, numpy=\n",
       " array([[-0.23587415],\n",
       "        [ 2.44161554]])>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=2.101151936178372>,\n",
       " {'dw': <tf.Tensor: shape=(2, 1), dtype=float64, numpy=\n",
       "  array([[ 0.03980444],\n",
       "         [-0.02466517]])>,\n",
       "  'db': <tf.Tensor: shape=(), dtype=float64, numpy=-0.01797094547599213>},\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=0.026563690163066245>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w =  np.array([[1.], [2]])\n",
    "b = 1.5\n",
    "X = np.array([[1., -2., -1.], [3., 0.5, -3.2]])\n",
    "Y = np.array([[1, 1, 0]])\n",
    "optimize(w, b, X, Y, num_iterations=1000, learning_rate = 0.009)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "- Initialize $$ w,b $$\n",
    "- Forward Propagation:\n",
    "    - You get X\n",
    "    - You compute $yhat = \\sigma(w^T X + b) $\n",
    "    - You calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}\\log(yhat^{(i)})+(1-y^{(i)})\\log(1-yhat^{(i)}))$\n",
    "- Back Propagation: \n",
    "    - $$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(yhat-y)^T\\tag{7}$$\n",
    "    - $$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (yhat^{(i)}-y^{(i)})\\tag{8}$$\n",
    "- Update weights:\n",
    "    - $$ w = w - {\\alpha} * \\frac{\\partial J}{\\partial w} $$\n",
    "    - $$ b = b- {\\alpha}  * \\frac{\\partial J}{\\partial b} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, dim: int):\n",
    "        self.w, self.b = tf.random.normal(\n",
    "            [dim, 1], dtype=tf.float64, seed=42), 0.0\n",
    "        self.params = dict()\n",
    "        self.cost = None\n",
    "        self.losses = list()\n",
    "        self.metric = [[0.0, ], [0.0, ]]\n",
    "\n",
    "    def forward(self, x: tf.Tensor):\n",
    "        z = tf.tensordot(tf.transpose(self.w), x, axes=1) + self.b\n",
    "        return 1/(1 + tf.exp(-z))\n",
    "\n",
    "    def forward_propagation(self, w: tf.Tensor, b: tf.float64, x: tf.Tensor, y: tf.Tensor):\n",
    "        yhat = self.forward(x)\n",
    "        loss = tf.reduce_sum((y * tf.math.log(yhat)) +\n",
    "                             ((1-y) * tf.math.log(1-yhat)))\n",
    "        cost = (-1/y.shape[0]) * loss\n",
    "        return yhat, tf.squeeze(cost)\n",
    "\n",
    "    def back_propagation(self, x: tf.Tensor, y: tf.Tensor, yhat: tf.Tensor):\n",
    "        m = y.shape[0]\n",
    "        dw = (1/m) * tf.tensordot(x, tf.transpose(yhat - y), axes=1)\n",
    "        db = (1/m) * tf.reduce_sum(yhat - y)\n",
    "        return {'dw': dw, 'db': db}\n",
    "\n",
    "    def evaluate(self, ytrain, ytrain_pred, ytest, ytest_pred):\n",
    "        train_acc = 100 - np.mean(np.abs(ytrain_pred - ytrain)) * 100\n",
    "        test_acc = 100 - np.mean(np.abs(ytest_pred - ytest)) * 100\n",
    "        return train_acc, test_acc\n",
    "\n",
    "    def optimize(self, x, y, validation=None, num_iterations=100, learning_rate=0.009):\n",
    "        for epoch in range(num_iterations):\n",
    "            yhat, self.cost = self.forward_propagation(\n",
    "                self.w, self.b, x, y)\n",
    "            grads = self.back_propagation(x, y, yhat)\n",
    "            self.w = self.w - learning_rate * grads['dw']\n",
    "            self.b = self.b - learning_rate * grads['db']\n",
    "            self.losses.append(self.cost)\n",
    "            if epoch % 10 == 0:\n",
    "                ytest_pred = self.predict(validation[0])\n",
    "                train_acc, test_acc = self.evaluate(\n",
    "                    y, tf.round(yhat), validation[1], ytest_pred)\n",
    "                print(\n",
    "                    f\"Epoch: {epoch} => Train Accuraccy: {train_acc:.2f}% || Test Accuracy:{test_acc:.2f}% || Loss: {self.cost}\")\n",
    "                self.metric[0].append(train_acc)\n",
    "                self.metric[1].append(test_acc)\n",
    "        self.params['w'] = self.w\n",
    "        self.params['b'] = self.b\n",
    "        return self.w, self.b, grads, self.cost\n",
    "\n",
    "    def predict(self, x: tf.Tensor):\n",
    "        return tf.round(self.forward(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 => Train Accuraccy: 25.00% || Test Accuracy:50.00% || Loss: 1.315912193032579\n",
      "Epoch: 10 => Train Accuraccy: 25.00% || Test Accuracy:50.00% || Loss: 1.130260498619716\n",
      "Epoch: 20 => Train Accuraccy: 25.00% || Test Accuracy:50.00% || Loss: 0.9856022946926002\n",
      "Epoch: 30 => Train Accuraccy: 25.00% || Test Accuracy:50.00% || Loss: 0.8726790689316903\n",
      "Epoch: 40 => Train Accuraccy: 50.00% || Test Accuracy:100.00% || Loss: 0.7836236894709461\n",
      "Epoch: 50 => Train Accuraccy: 75.00% || Test Accuracy:100.00% || Loss: 0.7122113259589877\n",
      "Epoch: 60 => Train Accuraccy: 75.00% || Test Accuracy:100.00% || Loss: 0.653783792902152\n",
      "Epoch: 70 => Train Accuraccy: 75.00% || Test Accuracy:100.00% || Loss: 0.6049739804328483\n",
      "Epoch: 80 => Train Accuraccy: 75.00% || Test Accuracy:100.00% || Loss: 0.5633875293801394\n",
      "Epoch: 90 => Train Accuraccy: 75.00% || Test Accuracy:100.00% || Loss: 0.5273272263749174\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 1), dtype=float64, numpy=\n",
       " array([[0.67251638],\n",
       "        [0.15194723]])>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=0.19555566275624162>,\n",
       " {'dw': <tf.Tensor: shape=(2, 1), dtype=float64, numpy=\n",
       "  array([[ 0.20437155],\n",
       "         [-0.50945549]])>,\n",
       "  'db': <tf.Tensor: shape=(), dtype=float64, numpy=-0.18376851092327232>},\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=0.4985905148418581>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.Variable([\n",
    "    [1., -2., -1., 3.4],\n",
    "    [3., 0.5, -3.2, 2.5]\n",
    "], dtype=tf.float64)\n",
    "Y = np.array([1, 1, 0, 1])\n",
    "xv = tf.Variable([\n",
    "    [-1.3, 3.1],\n",
    "    [-3.6, 2.1]\n",
    "], dtype=tf.float64)\n",
    "yv = np.array([0, 1])\n",
    "model = Model(2)\n",
    "model.optimize(X, Y, validation=(xv, yv), num_iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x16b6b329390>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model.losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "x, y = datasets.load_breast_cancer(return_X_y=True)\n",
    "x = StandardScaler().fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(\n",
    "    x, y, random_state=33, test_size=0.3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = tf.Variable(xtrain.T, dtype=tf.float64)\n",
    "xtest = tf.Variable(xtest.T, dtype=tf.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 => Train Accuraccy: 20.35% || Test Accuracy:22.81% || Loss: 4.673907349138036\n",
      "Epoch: 10 => Train Accuraccy: 20.60% || Test Accuracy:23.39% || Loss: 4.610840534485748\n",
      "Epoch: 20 => Train Accuraccy: 20.85% || Test Accuracy:23.98% || Loss: 4.548237367216999\n",
      "Epoch: 30 => Train Accuraccy: 20.85% || Test Accuracy:23.98% || Loss: 4.486076032546552\n",
      "Epoch: 40 => Train Accuraccy: 21.36% || Test Accuracy:23.98% || Loss: 4.4244155631487585\n",
      "Epoch: 50 => Train Accuraccy: 21.86% || Test Accuracy:24.56% || Loss: 4.36321941987452\n",
      "Epoch: 60 => Train Accuraccy: 22.11% || Test Accuracy:25.15% || Loss: 4.302535794281704\n",
      "Epoch: 70 => Train Accuraccy: 22.36% || Test Accuracy:25.73% || Loss: 4.242339023295303\n",
      "Epoch: 80 => Train Accuraccy: 22.86% || Test Accuracy:26.32% || Loss: 4.182657266027158\n",
      "Epoch: 90 => Train Accuraccy: 22.86% || Test Accuracy:26.32% || Loss: 4.123491579346591\n",
      "Epoch: 100 => Train Accuraccy: 22.86% || Test Accuracy:26.32% || Loss: 4.0648501188421085\n",
      "Epoch: 110 => Train Accuraccy: 22.86% || Test Accuracy:26.32% || Loss: 4.006744154049806\n",
      "Epoch: 120 => Train Accuraccy: 23.62% || Test Accuracy:26.90% || Loss: 3.9491742305298394\n",
      "Epoch: 130 => Train Accuraccy: 24.12% || Test Accuracy:27.49% || Loss: 3.892151135451012\n",
      "Epoch: 140 => Train Accuraccy: 25.13% || Test Accuracy:28.07% || Loss: 3.835679482238543\n",
      "Epoch: 150 => Train Accuraccy: 25.38% || Test Accuracy:28.07% || Loss: 3.7797628548522537\n",
      "Epoch: 160 => Train Accuraccy: 25.63% || Test Accuracy:28.07% || Loss: 3.724407889285877\n",
      "Epoch: 170 => Train Accuraccy: 25.63% || Test Accuracy:28.65% || Loss: 3.6696176765443367\n",
      "Epoch: 180 => Train Accuraccy: 26.13% || Test Accuracy:29.24% || Loss: 3.6153960761885444\n",
      "Epoch: 190 => Train Accuraccy: 26.63% || Test Accuracy:29.82% || Loss: 3.5617466611930517\n",
      "Epoch: 200 => Train Accuraccy: 26.88% || Test Accuracy:30.99% || Loss: 3.50867307433519\n",
      "Epoch: 210 => Train Accuraccy: 27.14% || Test Accuracy:30.99% || Loss: 3.456178183870617\n",
      "Epoch: 220 => Train Accuraccy: 27.64% || Test Accuracy:31.58% || Loss: 3.404265495502711\n",
      "Epoch: 230 => Train Accuraccy: 28.39% || Test Accuracy:31.58% || Loss: 3.352938183457305\n",
      "Epoch: 240 => Train Accuraccy: 28.64% || Test Accuracy:32.75% || Loss: 3.302199538349673\n",
      "Epoch: 250 => Train Accuraccy: 28.89% || Test Accuracy:32.75% || Loss: 3.2520529026898646\n",
      "Epoch: 260 => Train Accuraccy: 29.65% || Test Accuracy:32.75% || Loss: 3.2025018345183476\n",
      "Epoch: 270 => Train Accuraccy: 30.40% || Test Accuracy:33.92% || Loss: 3.153549976958337\n",
      "Epoch: 280 => Train Accuraccy: 31.41% || Test Accuracy:35.67% || Loss: 3.1052012455544493\n",
      "Epoch: 290 => Train Accuraccy: 31.41% || Test Accuracy:35.67% || Loss: 3.057459760394865\n",
      "Epoch: 300 => Train Accuraccy: 31.41% || Test Accuracy:36.26% || Loss: 3.010329905065829\n",
      "Epoch: 310 => Train Accuraccy: 31.91% || Test Accuracy:36.26% || Loss: 2.963816385301606\n",
      "Epoch: 320 => Train Accuraccy: 32.16% || Test Accuracy:36.26% || Loss: 2.9179242503420566\n",
      "Epoch: 330 => Train Accuraccy: 33.17% || Test Accuracy:37.43% || Loss: 2.8726588607000036\n",
      "Epoch: 340 => Train Accuraccy: 33.42% || Test Accuracy:38.01% || Loss: 2.8280259000283943\n",
      "Epoch: 350 => Train Accuraccy: 33.92% || Test Accuracy:39.18% || Loss: 2.7840312855371234\n",
      "Epoch: 360 => Train Accuraccy: 34.92% || Test Accuracy:40.35% || Loss: 2.7406810987088805\n",
      "Epoch: 370 => Train Accuraccy: 34.92% || Test Accuracy:40.35% || Loss: 2.6979814670963287\n",
      "Epoch: 380 => Train Accuraccy: 34.92% || Test Accuracy:40.35% || Loss: 2.6559383949555606\n",
      "Epoch: 390 => Train Accuraccy: 35.43% || Test Accuracy:40.94% || Loss: 2.614557609516669\n",
      "Epoch: 400 => Train Accuraccy: 35.93% || Test Accuracy:42.11% || Loss: 2.573844376163072\n",
      "Epoch: 410 => Train Accuraccy: 36.18% || Test Accuracy:42.69% || Loss: 2.533803329180761\n",
      "Epoch: 420 => Train Accuraccy: 36.68% || Test Accuracy:42.69% || Loss: 2.4944383197953135\n",
      "Epoch: 430 => Train Accuraccy: 37.19% || Test Accuracy:43.86% || Loss: 2.4557522923240254\n",
      "Epoch: 440 => Train Accuraccy: 37.44% || Test Accuracy:44.44% || Loss: 2.417747190914828\n",
      "Epoch: 450 => Train Accuraccy: 38.19% || Test Accuracy:44.44% || Loss: 2.380423893991756\n",
      "Epoch: 460 => Train Accuraccy: 38.94% || Test Accuracy:45.03% || Loss: 2.34378217600299\n",
      "Epoch: 470 => Train Accuraccy: 39.45% || Test Accuracy:45.03% || Loss: 2.307820686489495\n",
      "Epoch: 480 => Train Accuraccy: 40.70% || Test Accuracy:45.03% || Loss: 2.2725369522530645\n",
      "Epoch: 490 => Train Accuraccy: 40.95% || Test Accuracy:46.20% || Loss: 2.237927398972358\n",
      "Epoch: 500 => Train Accuraccy: 41.71% || Test Accuracy:46.78% || Loss: 2.203987400777328\n",
      "Epoch: 510 => Train Accuraccy: 42.46% || Test Accuracy:46.78% || Loss: 2.1707113620675305\n",
      "Epoch: 520 => Train Accuraccy: 42.96% || Test Accuracy:47.95% || Loss: 2.138092831536736\n",
      "Epoch: 530 => Train Accuraccy: 43.22% || Test Accuracy:49.12% || Loss: 2.106124644638532\n",
      "Epoch: 540 => Train Accuraccy: 43.72% || Test Accuracy:50.29% || Loss: 2.0747990833685206\n",
      "Epoch: 550 => Train Accuraccy: 43.97% || Test Accuracy:50.88% || Loss: 2.044108038424804\n",
      "Epoch: 560 => Train Accuraccy: 44.72% || Test Accuracy:51.46% || Loss: 2.0140431577790436\n",
      "Epoch: 570 => Train Accuraccy: 45.23% || Test Accuracy:52.05% || Loss: 1.9845959682680403\n",
      "Epoch: 580 => Train Accuraccy: 45.73% || Test Accuracy:52.63% || Loss: 1.9557579621934038\n",
      "Epoch: 590 => Train Accuraccy: 46.23% || Test Accuracy:53.22% || Loss: 1.927520646929556\n",
      "Epoch: 600 => Train Accuraccy: 46.73% || Test Accuracy:54.97% || Loss: 1.8998755617319432\n",
      "Epoch: 610 => Train Accuraccy: 47.49% || Test Accuracy:54.97% || Loss: 1.8728142701116677\n",
      "Epoch: 620 => Train Accuraccy: 47.49% || Test Accuracy:54.97% || Loss: 1.8463283383839415\n",
      "Epoch: 630 => Train Accuraccy: 48.49% || Test Accuracy:54.97% || Loss: 1.820409310779535\n",
      "Epoch: 640 => Train Accuraccy: 48.74% || Test Accuracy:56.14% || Loss: 1.795048689538445\n",
      "Epoch: 650 => Train Accuraccy: 48.99% || Test Accuracy:57.31% || Loss: 1.7702379252809906\n",
      "Epoch: 660 => Train Accuraccy: 49.50% || Test Accuracy:57.89% || Loss: 1.7459684195027048\n",
      "Epoch: 670 => Train Accuraccy: 49.50% || Test Accuracy:57.89% || Loss: 1.722231538148832\n",
      "Epoch: 680 => Train Accuraccy: 50.00% || Test Accuracy:57.89% || Loss: 1.6990186330655197\n",
      "Epoch: 690 => Train Accuraccy: 50.50% || Test Accuracy:57.89% || Loss: 1.6763210673188162\n",
      "Epoch: 700 => Train Accuraccy: 50.75% || Test Accuracy:58.48% || Loss: 1.6541302402866247\n",
      "Epoch: 710 => Train Accuraccy: 51.26% || Test Accuracy:58.48% || Loss: 1.6324376092296182\n",
      "Epoch: 720 => Train Accuraccy: 51.76% || Test Accuracy:59.06% || Loss: 1.6112347050408808\n",
      "Epoch: 730 => Train Accuraccy: 52.51% || Test Accuracy:59.06% || Loss: 1.59051314092595\n",
      "Epoch: 740 => Train Accuraccy: 52.51% || Test Accuracy:59.06% || Loss: 1.570264613685936\n",
      "Epoch: 750 => Train Accuraccy: 52.76% || Test Accuracy:60.23% || Loss: 1.55048089785153\n",
      "Epoch: 760 => Train Accuraccy: 53.02% || Test Accuracy:60.82% || Loss: 1.5311538333728298\n",
      "Epoch: 770 => Train Accuraccy: 53.52% || Test Accuracy:61.99% || Loss: 1.5122753078034872\n",
      "Epoch: 780 => Train Accuraccy: 54.02% || Test Accuracy:61.99% || Loss: 1.493837234135541\n",
      "Epoch: 790 => Train Accuraccy: 54.27% || Test Accuracy:61.99% || Loss: 1.4758315256674732\n",
      "Epoch: 800 => Train Accuraccy: 54.27% || Test Accuracy:61.99% || Loss: 1.4582500695408114\n",
      "Epoch: 810 => Train Accuraccy: 55.03% || Test Accuracy:63.16% || Loss: 1.441084700856123\n",
      "Epoch: 820 => Train Accuraccy: 55.53% || Test Accuracy:63.16% || Loss: 1.424327179487879\n",
      "Epoch: 830 => Train Accuraccy: 55.78% || Test Accuracy:63.74% || Loss: 1.4079691717938636\n",
      "Epoch: 840 => Train Accuraccy: 56.03% || Test Accuracy:63.74% || Loss: 1.392002239259673\n",
      "Epoch: 850 => Train Accuraccy: 56.28% || Test Accuracy:63.74% || Loss: 1.3764178357043504\n",
      "Epoch: 860 => Train Accuraccy: 57.04% || Test Accuracy:64.33% || Loss: 1.3612073140057834\n",
      "Epoch: 870 => Train Accuraccy: 58.04% || Test Accuracy:64.33% || Loss: 1.3463619424604079\n",
      "Epoch: 880 => Train Accuraccy: 58.79% || Test Accuracy:64.91% || Loss: 1.3318729299905012\n",
      "Epoch: 890 => Train Accuraccy: 59.05% || Test Accuracy:65.50% || Loss: 1.3177314585877906\n",
      "Epoch: 900 => Train Accuraccy: 59.55% || Test Accuracy:65.50% || Loss: 1.303928720752298\n",
      "Epoch: 910 => Train Accuraccy: 60.30% || Test Accuracy:65.50% || Loss: 1.2904559593205123\n",
      "Epoch: 920 => Train Accuraccy: 61.06% || Test Accuracy:66.08% || Loss: 1.2773045069898847\n",
      "Epoch: 930 => Train Accuraccy: 61.31% || Test Accuracy:66.08% || Loss: 1.2644658229952257\n",
      "Epoch: 940 => Train Accuraccy: 61.81% || Test Accuracy:66.67% || Loss: 1.251931524710816\n",
      "Epoch: 950 => Train Accuraccy: 61.81% || Test Accuracy:66.67% || Loss: 1.2396934123611298\n",
      "Epoch: 960 => Train Accuraccy: 62.06% || Test Accuracy:67.25% || Loss: 1.2277434854688878\n",
      "Epoch: 970 => Train Accuraccy: 62.31% || Test Accuracy:67.25% || Loss: 1.216073950130271\n",
      "Epoch: 980 => Train Accuraccy: 62.81% || Test Accuracy:67.25% || Loss: 1.2046772166905715\n",
      "Epoch: 990 => Train Accuraccy: 62.81% || Test Accuracy:67.25% || Loss: 1.1935458879307028\n",
      "Epoch: 1000 => Train Accuraccy: 63.82% || Test Accuracy:67.84% || Loss: 1.1826727384920055\n",
      "Epoch: 1010 => Train Accuraccy: 64.07% || Test Accuracy:67.84% || Loss: 1.172050686966004\n",
      "Epoch: 1020 => Train Accuraccy: 64.32% || Test Accuracy:68.42% || Loss: 1.1616727628138248\n",
      "Epoch: 1030 => Train Accuraccy: 64.57% || Test Accuracy:69.01% || Loss: 1.1515320709647354\n",
      "Epoch: 1040 => Train Accuraccy: 64.82% || Test Accuracy:69.59% || Loss: 1.1416217574426692\n",
      "Epoch: 1050 => Train Accuraccy: 64.82% || Test Accuracy:69.59% || Loss: 1.1319349795466118\n",
      "Epoch: 1060 => Train Accuraccy: 65.08% || Test Accuracy:70.18% || Loss: 1.1224648838611855\n",
      "Epoch: 1070 => Train Accuraccy: 65.08% || Test Accuracy:70.18% || Loss: 1.1132045946694968\n",
      "Epoch: 1080 => Train Accuraccy: 65.33% || Test Accuracy:70.18% || Loss: 1.1041472142514626\n",
      "Epoch: 1090 => Train Accuraccy: 65.58% || Test Accuracy:70.76% || Loss: 1.0952858352380406\n",
      "Epoch: 1100 => Train Accuraccy: 65.58% || Test Accuracy:70.76% || Loss: 1.086613563872183\n",
      "Epoch: 1110 => Train Accuraccy: 65.58% || Test Accuracy:70.76% || Loss: 1.0781235519184313\n",
      "Epoch: 1120 => Train Accuraccy: 65.83% || Test Accuracy:70.76% || Loss: 1.0698090342292006\n",
      "Epoch: 1130 => Train Accuraccy: 66.58% || Test Accuracy:71.35% || Loss: 1.0616633686934687\n",
      "Epoch: 1140 => Train Accuraccy: 66.83% || Test Accuracy:71.35% || Loss: 1.0536800754438211\n",
      "Epoch: 1150 => Train Accuraccy: 66.83% || Test Accuracy:71.93% || Loss: 1.0458528726870775\n",
      "Epoch: 1160 => Train Accuraccy: 67.59% || Test Accuracy:72.51% || Loss: 1.0381757072177658\n",
      "Epoch: 1170 => Train Accuraccy: 67.84% || Test Accuracy:73.68% || Loss: 1.03064277843467\n",
      "Epoch: 1180 => Train Accuraccy: 68.09% || Test Accuracy:73.68% || Loss: 1.023248555395206\n",
      "Epoch: 1190 => Train Accuraccy: 68.09% || Test Accuracy:73.68% || Loss: 1.01598778703472\n",
      "Epoch: 1200 => Train Accuraccy: 68.34% || Test Accuracy:73.68% || Loss: 1.008855506112748\n",
      "Epoch: 1210 => Train Accuraccy: 68.34% || Test Accuracy:74.27% || Loss: 1.0018470277226827\n",
      "Epoch: 1220 => Train Accuraccy: 68.34% || Test Accuracy:74.85% || Loss: 0.9949579433345098\n",
      "Epoch: 1230 => Train Accuraccy: 68.34% || Test Accuracy:74.85% || Loss: 0.9881841113625623\n",
      "Epoch: 1240 => Train Accuraccy: 68.34% || Test Accuracy:74.85% || Loss: 0.9815216451945228\n",
      "Epoch: 1250 => Train Accuraccy: 68.59% || Test Accuracy:74.85% || Loss: 0.9749668995143951\n",
      "Epoch: 1260 => Train Accuraccy: 68.84% || Test Accuracy:75.44% || Loss: 0.9685164556251642\n",
      "Epoch: 1270 => Train Accuraccy: 68.84% || Test Accuracy:75.44% || Loss: 0.9621671063444046\n",
      "Epoch: 1280 => Train Accuraccy: 68.84% || Test Accuracy:75.44% || Loss: 0.955915840919917\n",
      "Epoch: 1290 => Train Accuraccy: 69.35% || Test Accuracy:75.44% || Loss: 0.9497598302995858\n",
      "Epoch: 1300 => Train Accuraccy: 69.60% || Test Accuracy:75.44% || Loss: 0.9436964129932426\n",
      "Epoch: 1310 => Train Accuraccy: 69.85% || Test Accuracy:75.44% || Loss: 0.9377230816850175\n",
      "Epoch: 1320 => Train Accuraccy: 70.10% || Test Accuracy:75.44% || Loss: 0.9318374706916286\n",
      "Epoch: 1330 => Train Accuraccy: 70.35% || Test Accuracy:76.02% || Loss: 0.9260373443134384\n",
      "Epoch: 1340 => Train Accuraccy: 70.60% || Test Accuracy:76.02% || Loss: 0.9203205860888244\n",
      "Epoch: 1350 => Train Accuraccy: 70.60% || Test Accuracy:76.02% || Loss: 0.9146851889362798\n",
      "Epoch: 1360 => Train Accuraccy: 70.60% || Test Accuracy:76.02% || Loss: 0.9091292461505865\n",
      "Epoch: 1370 => Train Accuraccy: 70.60% || Test Accuracy:76.02% || Loss: 0.9036509432076948\n",
      "Epoch: 1380 => Train Accuraccy: 70.85% || Test Accuracy:76.02% || Loss: 0.8982485503260088\n",
      "Epoch: 1390 => Train Accuraccy: 71.11% || Test Accuracy:76.02% || Loss: 0.8929204157284184\n",
      "Epoch: 1400 => Train Accuraccy: 71.11% || Test Accuracy:76.02% || Loss: 0.8876649595486318\n",
      "Epoch: 1410 => Train Accuraccy: 71.36% || Test Accuracy:76.02% || Loss: 0.882480668326366\n",
      "Epoch: 1420 => Train Accuraccy: 71.61% || Test Accuracy:76.02% || Loss: 0.8773660900381609\n",
      "Epoch: 1430 => Train Accuraccy: 71.86% || Test Accuracy:76.02% || Loss: 0.8723198296135694\n",
      "Epoch: 1440 => Train Accuraccy: 71.86% || Test Accuracy:76.61% || Loss: 0.8673405448898459\n",
      "Epoch: 1450 => Train Accuraccy: 71.86% || Test Accuracy:76.61% || Loss: 0.8624269429618444\n",
      "Epoch: 1460 => Train Accuraccy: 71.86% || Test Accuracy:76.61% || Loss: 0.8575777768874844\n",
      "Epoch: 1470 => Train Accuraccy: 72.11% || Test Accuracy:77.19% || Loss: 0.8527918427125665\n",
      "Epoch: 1480 => Train Accuraccy: 72.61% || Test Accuracy:77.19% || Loss: 0.848067976782177\n",
      "Epoch: 1490 => Train Accuraccy: 72.86% || Test Accuracy:77.19% || Loss: 0.8434050533089643\n",
      "Epoch: 1500 => Train Accuraccy: 73.12% || Test Accuracy:77.19% || Loss: 0.8388019821715306\n",
      "Epoch: 1510 => Train Accuraccy: 73.62% || Test Accuracy:77.19% || Loss: 0.8342577069188064\n",
      "Epoch: 1520 => Train Accuraccy: 73.87% || Test Accuracy:77.19% || Loss: 0.8297712029587119\n",
      "Epoch: 1530 => Train Accuraccy: 73.87% || Test Accuracy:77.19% || Loss: 0.8253414759115789\n",
      "Epoch: 1540 => Train Accuraccy: 73.62% || Test Accuracy:77.19% || Loss: 0.8209675601107603\n",
      "Epoch: 1550 => Train Accuraccy: 73.87% || Test Accuracy:77.19% || Loss: 0.8166485172346285\n",
      "Epoch: 1560 => Train Accuraccy: 74.12% || Test Accuracy:77.19% || Loss: 0.8123834350556989\n",
      "Epoch: 1570 => Train Accuraccy: 74.37% || Test Accuracy:77.19% || Loss: 0.8081714262940186\n",
      "Epoch: 1580 => Train Accuraccy: 74.87% || Test Accuracy:77.19% || Loss: 0.8040116275631847\n",
      "Epoch: 1590 => Train Accuraccy: 74.87% || Test Accuracy:77.19% || Loss: 0.799903198398461\n",
      "Epoch: 1600 => Train Accuraccy: 74.87% || Test Accuracy:77.19% || Loss: 0.7958453203573891\n",
      "Epoch: 1610 => Train Accuraccy: 75.13% || Test Accuracy:77.78% || Loss: 0.791837196184194\n",
      "Epoch: 1620 => Train Accuraccy: 75.13% || Test Accuracy:78.36% || Loss: 0.7878780490299926\n",
      "Epoch: 1630 => Train Accuraccy: 75.13% || Test Accuracy:78.36% || Loss: 0.7839671217215225\n",
      "Epoch: 1640 => Train Accuraccy: 75.13% || Test Accuracy:78.36% || Loss: 0.7801036760716741\n",
      "Epoch: 1650 => Train Accuraccy: 75.38% || Test Accuracy:78.36% || Loss: 0.7762869922256861\n",
      "Epoch: 1660 => Train Accuraccy: 75.38% || Test Accuracy:78.36% || Loss: 0.7725163680373126\n",
      "Epoch: 1670 => Train Accuraccy: 75.38% || Test Accuracy:78.36% || Loss: 0.7687911184697516\n",
      "Epoch: 1680 => Train Accuraccy: 75.38% || Test Accuracy:78.36% || Loss: 0.765110575016528\n",
      "Epoch: 1690 => Train Accuraccy: 75.38% || Test Accuracy:78.36% || Loss: 0.7614740851379201\n",
      "Epoch: 1700 => Train Accuraccy: 75.38% || Test Accuracy:78.36% || Loss: 0.7578810117089032\n",
      "Epoch: 1710 => Train Accuraccy: 75.38% || Test Accuracy:78.36% || Loss: 0.7543307324749697\n",
      "Epoch: 1720 => Train Accuraccy: 75.38% || Test Accuracy:78.36% || Loss: 0.7508226395125488\n",
      "Epoch: 1730 => Train Accuraccy: 75.38% || Test Accuracy:78.36% || Loss: 0.7473561386911401\n",
      "Epoch: 1740 => Train Accuraccy: 75.38% || Test Accuracy:78.95% || Loss: 0.7439306491346735\n",
      "Epoch: 1750 => Train Accuraccy: 75.88% || Test Accuracy:79.53% || Loss: 0.7405456026800104\n",
      "Epoch: 1760 => Train Accuraccy: 75.88% || Test Accuracy:79.53% || Loss: 0.7372004433309443\n",
      "Epoch: 1770 => Train Accuraccy: 75.88% || Test Accuracy:79.53% || Loss: 0.7338946267064809\n",
      "Epoch: 1780 => Train Accuraccy: 75.88% || Test Accuracy:79.53% || Loss: 0.7306276194826862\n",
      "Epoch: 1790 => Train Accuraccy: 76.13% || Test Accuracy:80.70% || Loss: 0.7273988988278226\n",
      "Epoch: 1800 => Train Accuraccy: 76.38% || Test Accuracy:80.70% || Loss: 0.7242079518310459\n",
      "Epoch: 1810 => Train Accuraccy: 76.38% || Test Accuracy:80.70% || Loss: 0.7210542749254012\n",
      "Epoch: 1820 => Train Accuraccy: 76.63% || Test Accuracy:80.70% || Loss: 0.7179373733063769\n",
      "Epoch: 1830 => Train Accuraccy: 77.14% || Test Accuracy:80.70% || Loss: 0.7148567603477951\n",
      "Epoch: 1840 => Train Accuraccy: 77.14% || Test Accuracy:81.29% || Loss: 0.7118119570172697\n",
      "Epoch: 1850 => Train Accuraccy: 77.39% || Test Accuracy:81.29% || Loss: 0.7088024912939425\n",
      "Epoch: 1860 => Train Accuraccy: 77.39% || Test Accuracy:81.29% || Loss: 0.705827897591625\n",
      "Epoch: 1870 => Train Accuraccy: 77.39% || Test Accuracy:81.29% || Loss: 0.7028877161908215\n",
      "Epoch: 1880 => Train Accuraccy: 77.39% || Test Accuracy:81.29% || Loss: 0.6999814926834281\n",
      "Epoch: 1890 => Train Accuraccy: 77.64% || Test Accuracy:81.29% || Loss: 0.697108777434121\n",
      "Epoch: 1900 => Train Accuraccy: 77.64% || Test Accuracy:81.29% || Loss: 0.6942691250625888\n",
      "Epoch: 1910 => Train Accuraccy: 77.64% || Test Accuracy:81.29% || Loss: 0.6914620939508078\n",
      "Epoch: 1920 => Train Accuraccy: 78.14% || Test Accuracy:81.29% || Loss: 0.6886872457795068\n",
      "Epoch: 1930 => Train Accuraccy: 78.39% || Test Accuracy:81.29% || Loss: 0.6859441450978215\n",
      "Epoch: 1940 => Train Accuraccy: 78.39% || Test Accuracy:81.29% || Loss: 0.6832323589298607\n",
      "Epoch: 1950 => Train Accuraccy: 79.40% || Test Accuracy:81.87% || Loss: 0.6805514564215633\n",
      "Epoch: 1960 => Train Accuraccy: 79.40% || Test Accuracy:81.87% || Loss: 0.6779010085307803\n",
      "Epoch: 1970 => Train Accuraccy: 79.65% || Test Accuracy:82.46% || Loss: 0.6752805877629536\n",
      "Epoch: 1980 => Train Accuraccy: 79.65% || Test Accuracy:82.46% || Loss: 0.6726897679542039\n",
      "Epoch: 1990 => Train Accuraccy: 79.65% || Test Accuracy:82.46% || Loss: 0.6701281241029489\n",
      "Epoch: 2000 => Train Accuraccy: 79.65% || Test Accuracy:82.46% || Loss: 0.6675952322505052\n",
      "Epoch: 2010 => Train Accuraccy: 79.65% || Test Accuracy:83.04% || Loss: 0.6650906694104187\n",
      "Epoch: 2020 => Train Accuraccy: 79.65% || Test Accuracy:83.63% || Loss: 0.6626140135455535\n",
      "Epoch: 2030 => Train Accuraccy: 80.40% || Test Accuracy:83.63% || Loss: 0.6601648435913142\n",
      "Epoch: 2040 => Train Accuraccy: 80.40% || Test Accuracy:84.21% || Loss: 0.657742739522712\n",
      "Epoch: 2050 => Train Accuraccy: 80.40% || Test Accuracy:84.21% || Loss: 0.6553472824624298\n",
      "Epoch: 2060 => Train Accuraccy: 80.40% || Test Accuracy:84.21% || Loss: 0.6529780548265128\n",
      "Epoch: 2070 => Train Accuraccy: 80.40% || Test Accuracy:84.21% || Loss: 0.6506346405038946\n",
      "Epoch: 2080 => Train Accuraccy: 80.65% || Test Accuracy:84.21% || Loss: 0.6483166250656349\n",
      "Epoch: 2090 => Train Accuraccy: 80.65% || Test Accuracy:84.21% || Loss: 0.6460235959994932\n",
      "Epoch: 2100 => Train Accuraccy: 81.41% || Test Accuracy:84.21% || Loss: 0.6437551429653242\n",
      "Epoch: 2110 => Train Accuraccy: 81.66% || Test Accuracy:84.21% || Loss: 0.6415108580667306\n",
      "Epoch: 2120 => Train Accuraccy: 81.66% || Test Accuracy:84.21% || Loss: 0.6392903361344405\n",
      "Epoch: 2130 => Train Accuraccy: 81.66% || Test Accuracy:84.21% || Loss: 0.637093175016999\n",
      "Epoch: 2140 => Train Accuraccy: 81.91% || Test Accuracy:84.21% || Loss: 0.6349189758745575\n",
      "Epoch: 2150 => Train Accuraccy: 81.91% || Test Accuracy:84.21% || Loss: 0.6327673434717983\n",
      "Epoch: 2160 => Train Accuraccy: 81.91% || Test Accuracy:84.21% || Loss: 0.6306378864663393\n",
      "Epoch: 2170 => Train Accuraccy: 81.91% || Test Accuracy:84.21% || Loss: 0.6285302176893046\n",
      "Epoch: 2180 => Train Accuraccy: 82.16% || Test Accuracy:84.21% || Loss: 0.6264439544151328\n",
      "Epoch: 2190 => Train Accuraccy: 82.16% || Test Accuracy:84.21% || Loss: 0.6243787186180672\n",
      "Epoch: 2200 => Train Accuraccy: 82.16% || Test Accuracy:84.21% || Loss: 0.6223341372131951\n",
      "Epoch: 2210 => Train Accuraccy: 82.16% || Test Accuracy:84.21% || Loss: 0.6203098422802618\n",
      "Epoch: 2220 => Train Accuraccy: 82.16% || Test Accuracy:84.21% || Loss: 0.6183054712689074\n",
      "Epoch: 2230 => Train Accuraccy: 82.16% || Test Accuracy:84.21% || Loss: 0.6163206671843108\n",
      "Epoch: 2240 => Train Accuraccy: 82.66% || Test Accuracy:84.21% || Loss: 0.6143550787525752\n",
      "Epoch: 2250 => Train Accuraccy: 82.66% || Test Accuracy:84.21% || Loss: 0.6124083605655183\n",
      "Epoch: 2260 => Train Accuraccy: 82.66% || Test Accuracy:84.21% || Loss: 0.6104801732047871\n",
      "Epoch: 2270 => Train Accuraccy: 82.66% || Test Accuracy:84.21% || Loss: 0.6085701833454997\n",
      "Epoch: 2280 => Train Accuraccy: 83.17% || Test Accuracy:84.21% || Loss: 0.606678063839817\n",
      "Epoch: 2290 => Train Accuraccy: 83.17% || Test Accuracy:84.21% || Loss: 0.6048034937810381\n",
      "Epoch: 2300 => Train Accuraccy: 83.42% || Test Accuracy:84.80% || Loss: 0.602946158548979\n",
      "Epoch: 2310 => Train Accuraccy: 83.42% || Test Accuracy:84.80% || Loss: 0.6011057498375103\n",
      "Epoch: 2320 => Train Accuraccy: 83.42% || Test Accuracy:84.80% || Loss: 0.5992819656652303\n",
      "Epoch: 2330 => Train Accuraccy: 83.42% || Test Accuracy:84.80% || Loss: 0.5974745103703286\n",
      "Epoch: 2340 => Train Accuraccy: 83.67% || Test Accuracy:84.80% || Loss: 0.5956830945907348\n",
      "Epoch: 2350 => Train Accuraccy: 83.67% || Test Accuracy:84.80% || Loss: 0.5939074352306837\n",
      "Epoch: 2360 => Train Accuraccy: 83.67% || Test Accuracy:84.80% || Loss: 0.5921472554148328\n",
      "Epoch: 2370 => Train Accuraccy: 83.67% || Test Accuracy:84.80% || Loss: 0.5904022844310679\n",
      "Epoch: 2380 => Train Accuraccy: 83.92% || Test Accuracy:84.80% || Loss: 0.588672257663109\n",
      "Epoch: 2390 => Train Accuraccy: 83.92% || Test Accuracy:84.80% || Loss: 0.5869569165140042\n",
      "Epoch: 2400 => Train Accuraccy: 84.17% || Test Accuracy:84.80% || Loss: 0.5852560083215513\n",
      "Epoch: 2410 => Train Accuraccy: 84.17% || Test Accuracy:84.80% || Loss: 0.5835692862666569\n",
      "Epoch: 2420 => Train Accuraccy: 84.17% || Test Accuracy:84.80% || Loss: 0.5818965092755747\n",
      "Epoch: 2430 => Train Accuraccy: 84.17% || Test Accuracy:84.80% || Loss: 0.580237441916928\n",
      "Epoch: 2440 => Train Accuraccy: 84.17% || Test Accuracy:84.80% || Loss: 0.5785918542943442\n",
      "Epoch: 2450 => Train Accuraccy: 84.17% || Test Accuracy:84.80% || Loss: 0.5769595219354944\n",
      "Epoch: 2460 => Train Accuraccy: 84.17% || Test Accuracy:84.80% || Loss: 0.5753402256782476\n",
      "Epoch: 2470 => Train Accuraccy: 84.17% || Test Accuracy:84.80% || Loss: 0.5737337515546228\n",
      "Epoch: 2480 => Train Accuraccy: 84.17% || Test Accuracy:84.80% || Loss: 0.572139890673126\n",
      "Epoch: 2490 => Train Accuraccy: 84.17% || Test Accuracy:84.80% || Loss: 0.570558439100045\n",
      "Epoch: 2500 => Train Accuraccy: 84.17% || Test Accuracy:84.80% || Loss: 0.5689891977401932\n",
      "Epoch: 2510 => Train Accuraccy: 84.17% || Test Accuracy:84.80% || Loss: 0.5674319722175524\n",
      "Epoch: 2520 => Train Accuraccy: 84.42% || Test Accuracy:84.80% || Loss: 0.56588657275623\n",
      "Epoch: 2530 => Train Accuraccy: 84.67% || Test Accuracy:84.80% || Loss: 0.5643528140620735\n",
      "Epoch: 2540 => Train Accuraccy: 84.67% || Test Accuracy:84.80% || Loss: 0.5628305152052718\n",
      "Epoch: 2550 => Train Accuraccy: 84.67% || Test Accuracy:84.80% || Loss: 0.5613194995042179\n",
      "Epoch: 2560 => Train Accuraccy: 84.67% || Test Accuracy:84.80% || Loss: 0.5598195944108711\n",
      "Epoch: 2570 => Train Accuraccy: 84.67% || Test Accuracy:85.38% || Loss: 0.5583306313978358\n",
      "Epoch: 2580 => Train Accuraccy: 84.67% || Test Accuracy:85.38% || Loss: 0.5568524458473264\n",
      "Epoch: 2590 => Train Accuraccy: 84.67% || Test Accuracy:85.38% || Loss: 0.5553848769421772\n",
      "Epoch: 2600 => Train Accuraccy: 84.67% || Test Accuracy:85.38% || Loss: 0.5539277675590167\n",
      "Epoch: 2610 => Train Accuraccy: 84.67% || Test Accuracy:85.38% || Loss: 0.5524809641637124\n",
      "Epoch: 2620 => Train Accuraccy: 84.67% || Test Accuracy:85.38% || Loss: 0.5510443167091621\n",
      "Epoch: 2630 => Train Accuraccy: 84.67% || Test Accuracy:85.38% || Loss: 0.5496176785355045\n",
      "Epoch: 2640 => Train Accuraccy: 84.67% || Test Accuracy:85.38% || Loss: 0.5482009062727828\n",
      "Epoch: 2650 => Train Accuraccy: 84.67% || Test Accuracy:85.38% || Loss: 0.5467938597461054\n",
      "Epoch: 2660 => Train Accuraccy: 84.67% || Test Accuracy:85.38% || Loss: 0.5453964018833081\n",
      "Epoch: 2670 => Train Accuraccy: 84.67% || Test Accuracy:85.38% || Loss: 0.5440083986251383\n",
      "Epoch: 2680 => Train Accuraccy: 84.67% || Test Accuracy:85.38% || Loss: 0.5426297188379462\n",
      "Epoch: 2690 => Train Accuraccy: 84.67% || Test Accuracy:85.38% || Loss: 0.5412602342288751\n",
      "Epoch: 2700 => Train Accuraccy: 84.67% || Test Accuracy:85.38% || Loss: 0.5398998192635315\n",
      "Epoch: 2710 => Train Accuraccy: 84.67% || Test Accuracy:85.38% || Loss: 0.5385483510861006\n",
      "Epoch: 2720 => Train Accuraccy: 84.67% || Test Accuracy:85.38% || Loss: 0.5372057094418846\n",
      "Epoch: 2730 => Train Accuraccy: 84.92% || Test Accuracy:85.38% || Loss: 0.5358717766022156\n",
      "Epoch: 2740 => Train Accuraccy: 84.92% || Test Accuracy:85.38% || Loss: 0.5345464372917051\n",
      "Epoch: 2750 => Train Accuraccy: 84.92% || Test Accuracy:85.38% || Loss: 0.5332295786177883\n",
      "Epoch: 2760 => Train Accuraccy: 84.92% || Test Accuracy:85.38% || Loss: 0.5319210900025043\n",
      "Epoch: 2770 => Train Accuraccy: 84.92% || Test Accuracy:85.38% || Loss: 0.5306208631164734\n",
      "Epoch: 2780 => Train Accuraccy: 84.92% || Test Accuracy:85.38% || Loss: 0.5293287918150092\n",
      "Epoch: 2790 => Train Accuraccy: 84.92% || Test Accuracy:85.38% || Loss: 0.5280447720763204\n",
      "Epoch: 2800 => Train Accuraccy: 85.18% || Test Accuracy:85.38% || Loss: 0.5267687019417348\n",
      "Epoch: 2810 => Train Accuraccy: 85.18% || Test Accuracy:85.38% || Loss: 0.5255004814579075\n",
      "Epoch: 2820 => Train Accuraccy: 85.43% || Test Accuracy:85.96% || Loss: 0.5242400126209422\n",
      "Epoch: 2830 => Train Accuraccy: 85.43% || Test Accuracy:85.96% || Loss: 0.5229871993223766\n",
      "Epoch: 2840 => Train Accuraccy: 85.68% || Test Accuracy:85.96% || Loss: 0.5217419472969783\n",
      "Epoch: 2850 => Train Accuraccy: 85.68% || Test Accuracy:85.96% || Loss: 0.52050416407229\n",
      "Epoch: 2860 => Train Accuraccy: 85.68% || Test Accuracy:85.96% || Loss: 0.5192737589198766\n",
      "Epoch: 2870 => Train Accuraccy: 86.18% || Test Accuracy:85.96% || Loss: 0.5180506428082123\n",
      "Epoch: 2880 => Train Accuraccy: 86.18% || Test Accuracy:85.96% || Loss: 0.5168347283571623\n",
      "Epoch: 2890 => Train Accuraccy: 86.43% || Test Accuracy:85.96% || Loss: 0.5156259297940036\n",
      "Epoch: 2900 => Train Accuraccy: 86.43% || Test Accuracy:85.96% || Loss: 0.5144241629109302\n",
      "Epoch: 2910 => Train Accuraccy: 86.43% || Test Accuracy:85.96% || Loss: 0.5132293450240012\n",
      "Epoch: 2920 => Train Accuraccy: 86.43% || Test Accuracy:85.96% || Loss: 0.512041394933473\n",
      "Epoch: 2930 => Train Accuraccy: 86.43% || Test Accuracy:85.96% || Loss: 0.5108602328854767\n",
      "Epoch: 2940 => Train Accuraccy: 86.43% || Test Accuracy:85.96% || Loss: 0.509685780534986\n",
      "Epoch: 2950 => Train Accuraccy: 86.43% || Test Accuracy:85.96% || Loss: 0.5085179609100394\n",
      "Epoch: 2960 => Train Accuraccy: 86.43% || Test Accuracy:85.96% || Loss: 0.5073566983771607\n",
      "Epoch: 2970 => Train Accuraccy: 86.43% || Test Accuracy:86.55% || Loss: 0.5062019186079489\n",
      "Epoch: 2980 => Train Accuraccy: 86.43% || Test Accuracy:86.55% || Loss: 0.505053548546782\n",
      "Epoch: 2990 => Train Accuraccy: 86.68% || Test Accuracy:86.55% || Loss: 0.5039115163796072\n",
      "Epoch: 3000 => Train Accuraccy: 86.68% || Test Accuracy:86.55% || Loss: 0.5027757515037655\n",
      "Epoch: 3010 => Train Accuraccy: 86.68% || Test Accuracy:86.55% || Loss: 0.5016461844988246\n",
      "Epoch: 3020 => Train Accuraccy: 86.68% || Test Accuracy:86.55% || Loss: 0.5005227470983746\n",
      "Epoch: 3030 => Train Accuraccy: 86.93% || Test Accuracy:86.55% || Loss: 0.49940537216275654\n",
      "Epoch: 3040 => Train Accuraccy: 86.93% || Test Accuracy:86.55% || Loss: 0.49829399365268545\n",
      "Epoch: 3050 => Train Accuraccy: 86.93% || Test Accuracy:86.55% || Loss: 0.49718854660373896\n",
      "Epoch: 3060 => Train Accuraccy: 86.93% || Test Accuracy:86.55% || Loss: 0.49608896710167294\n",
      "Epoch: 3070 => Train Accuraccy: 86.93% || Test Accuracy:86.55% || Loss: 0.494995192258541\n",
      "Epoch: 3080 => Train Accuraccy: 87.19% || Test Accuracy:86.55% || Loss: 0.49390716018958053\n",
      "Epoch: 3090 => Train Accuraccy: 87.19% || Test Accuracy:86.55% || Loss: 0.4928248099908441\n",
      "Epoch: 3100 => Train Accuraccy: 87.19% || Test Accuracy:86.55% || Loss: 0.4917480817175391\n",
      "Epoch: 3110 => Train Accuraccy: 87.19% || Test Accuracy:86.55% || Loss: 0.4906769163630573\n",
      "Epoch: 3120 => Train Accuraccy: 87.19% || Test Accuracy:86.55% || Loss: 0.48961125583866405\n",
      "Epoch: 3130 => Train Accuraccy: 87.19% || Test Accuracy:86.55% || Loss: 0.48855104295382196\n",
      "Epoch: 3140 => Train Accuraccy: 87.19% || Test Accuracy:86.55% || Loss: 0.48749622139712667\n",
      "Epoch: 3150 => Train Accuraccy: 87.19% || Test Accuracy:86.55% || Loss: 0.48644673571783315\n",
      "Epoch: 3160 => Train Accuraccy: 87.19% || Test Accuracy:86.55% || Loss: 0.4854025313079428\n",
      "Epoch: 3170 => Train Accuraccy: 87.19% || Test Accuracy:87.13% || Loss: 0.4843635543848418\n",
      "Epoch: 3180 => Train Accuraccy: 87.19% || Test Accuracy:87.13% || Loss: 0.48332975197445793\n",
      "Epoch: 3190 => Train Accuraccy: 87.19% || Test Accuracy:87.13% || Loss: 0.48230107189492444\n",
      "Epoch: 3200 => Train Accuraccy: 87.19% || Test Accuracy:87.13% || Loss: 0.481277462740726\n",
      "Epoch: 3210 => Train Accuraccy: 87.19% || Test Accuracy:87.13% || Loss: 0.4802588738673187\n",
      "Epoch: 3220 => Train Accuraccy: 87.19% || Test Accuracy:87.13% || Loss: 0.4792452553761878\n",
      "Epoch: 3230 => Train Accuraccy: 87.19% || Test Accuracy:87.13% || Loss: 0.478236558100347\n",
      "Epoch: 3240 => Train Accuraccy: 87.19% || Test Accuracy:87.13% || Loss: 0.4772327335902507\n",
      "Epoch: 3250 => Train Accuraccy: 87.19% || Test Accuracy:87.13% || Loss: 0.4762337341001026\n",
      "Epoch: 3260 => Train Accuraccy: 87.19% || Test Accuracy:87.13% || Loss: 0.4752395125745553\n",
      "Epoch: 3270 => Train Accuraccy: 87.19% || Test Accuracy:87.13% || Loss: 0.4742500226357779\n",
      "Epoch: 3280 => Train Accuraccy: 87.19% || Test Accuracy:87.13% || Loss: 0.47326521857087755\n",
      "Epoch: 3290 => Train Accuraccy: 87.19% || Test Accuracy:87.13% || Loss: 0.47228505531967147\n",
      "Epoch: 3300 => Train Accuraccy: 87.19% || Test Accuracy:87.13% || Loss: 0.4713094884627811\n",
      "Epoch: 3310 => Train Accuraccy: 87.19% || Test Accuracy:87.13% || Loss: 0.4703384742100525\n",
      "Epoch: 3320 => Train Accuraccy: 87.19% || Test Accuracy:87.13% || Loss: 0.46937196938927944\n",
      "Epoch: 3330 => Train Accuraccy: 87.19% || Test Accuracy:87.13% || Loss: 0.46840993143522114\n",
      "Epoch: 3340 => Train Accuraccy: 87.19% || Test Accuracy:87.13% || Loss: 0.4674523183789091\n",
      "Epoch: 3350 => Train Accuraccy: 87.19% || Test Accuracy:87.13% || Loss: 0.466499088837222\n",
      "Epoch: 3360 => Train Accuraccy: 87.19% || Test Accuracy:87.72% || Loss: 0.46555020200273123\n",
      "Epoch: 3370 => Train Accuraccy: 87.19% || Test Accuracy:87.72% || Loss: 0.46460561763379704\n",
      "Epoch: 3380 => Train Accuraccy: 87.19% || Test Accuracy:87.72% || Loss: 0.4636652960449138\n",
      "Epoch: 3390 => Train Accuraccy: 87.19% || Test Accuracy:87.72% || Loss: 0.46272919809728574\n",
      "Epoch: 3400 => Train Accuraccy: 87.19% || Test Accuracy:87.72% || Loss: 0.46179728518964297\n",
      "Epoch: 3410 => Train Accuraccy: 87.19% || Test Accuracy:87.72% || Loss: 0.4608695192492643\n",
      "Epoch: 3420 => Train Accuraccy: 87.19% || Test Accuracy:87.72% || Loss: 0.45994586272322363\n",
      "Epoch: 3430 => Train Accuraccy: 87.19% || Test Accuracy:87.72% || Loss: 0.4590262785698376\n",
      "Epoch: 3440 => Train Accuraccy: 87.19% || Test Accuracy:87.72% || Loss: 0.4581107302503109\n",
      "Epoch: 3450 => Train Accuraccy: 87.19% || Test Accuracy:87.72% || Loss: 0.45719918172057694\n",
      "Epoch: 3460 => Train Accuraccy: 87.19% || Test Accuracy:87.72% || Loss: 0.4562915974233184\n",
      "Epoch: 3470 => Train Accuraccy: 87.19% || Test Accuracy:87.72% || Loss: 0.45538794228017254\n",
      "Epoch: 3480 => Train Accuraccy: 87.19% || Test Accuracy:87.72% || Loss: 0.45448818168410715\n",
      "Epoch: 3490 => Train Accuraccy: 87.19% || Test Accuracy:87.72% || Loss: 0.45359228149196296\n",
      "Epoch: 3500 => Train Accuraccy: 87.19% || Test Accuracy:87.72% || Loss: 0.4527002080171589\n",
      "Epoch: 3510 => Train Accuraccy: 87.19% || Test Accuracy:87.72% || Loss: 0.45181192802255604\n",
      "Epoch: 3520 => Train Accuraccy: 87.19% || Test Accuracy:87.72% || Loss: 0.45092740871347214\n",
      "Epoch: 3530 => Train Accuraccy: 87.19% || Test Accuracy:87.72% || Loss: 0.45004661773083937\n",
      "Epoch: 3540 => Train Accuraccy: 87.19% || Test Accuracy:87.72% || Loss: 0.44916952314451386\n",
      "Epoch: 3550 => Train Accuraccy: 87.19% || Test Accuracy:87.72% || Loss: 0.4482960934467127\n",
      "Epoch: 3560 => Train Accuraccy: 87.19% || Test Accuracy:88.30% || Loss: 0.44742629754559504\n",
      "Epoch: 3570 => Train Accuraccy: 87.19% || Test Accuracy:88.30% || Loss: 0.4465601047589636\n",
      "Epoch: 3580 => Train Accuraccy: 87.19% || Test Accuracy:88.89% || Loss: 0.445697484808099\n",
      "Epoch: 3590 => Train Accuraccy: 87.19% || Test Accuracy:88.89% || Loss: 0.4448384078117151\n",
      "Epoch: 3600 => Train Accuraccy: 87.19% || Test Accuracy:88.89% || Loss: 0.44398284428003065\n",
      "Epoch: 3610 => Train Accuraccy: 87.19% || Test Accuracy:88.89% || Loss: 0.4431307651089599\n",
      "Epoch: 3620 => Train Accuraccy: 87.19% || Test Accuracy:88.89% || Loss: 0.44228214157441287\n",
      "Epoch: 3630 => Train Accuraccy: 87.19% || Test Accuracy:88.89% || Loss: 0.44143694532670646\n",
      "Epoch: 3640 => Train Accuraccy: 87.19% || Test Accuracy:88.89% || Loss: 0.4405951483850797\n",
      "Epoch: 3650 => Train Accuraccy: 87.19% || Test Accuracy:88.89% || Loss: 0.43975672313231595\n",
      "Epoch: 3660 => Train Accuraccy: 87.19% || Test Accuracy:88.89% || Loss: 0.43892164230946135\n",
      "Epoch: 3670 => Train Accuraccy: 87.19% || Test Accuracy:88.89% || Loss: 0.4380898790106429\n",
      "Epoch: 3680 => Train Accuraccy: 87.19% || Test Accuracy:88.89% || Loss: 0.43726140667798447\n",
      "Epoch: 3690 => Train Accuraccy: 87.19% || Test Accuracy:88.89% || Loss: 0.43643619909661047\n",
      "Epoch: 3700 => Train Accuraccy: 87.19% || Test Accuracy:89.47% || Loss: 0.4356142303897447\n",
      "Epoch: 3710 => Train Accuraccy: 87.19% || Test Accuracy:89.47% || Loss: 0.43479547501389537\n",
      "Epoch: 3720 => Train Accuraccy: 87.19% || Test Accuracy:89.47% || Loss: 0.433979907754129\n",
      "Epoch: 3730 => Train Accuraccy: 87.19% || Test Accuracy:90.06% || Loss: 0.4331675037194204\n",
      "Epoch: 3740 => Train Accuraccy: 87.19% || Test Accuracy:90.06% || Loss: 0.4323582383380973\n",
      "Epoch: 3750 => Train Accuraccy: 87.19% || Test Accuracy:90.06% || Loss: 0.4315520873533534\n",
      "Epoch: 3760 => Train Accuraccy: 87.19% || Test Accuracy:90.06% || Loss: 0.4307490268188476\n",
      "Epoch: 3770 => Train Accuraccy: 87.19% || Test Accuracy:90.06% || Loss: 0.42994903309437577\n",
      "Epoch: 3780 => Train Accuraccy: 87.19% || Test Accuracy:90.06% || Loss: 0.429152082841617\n",
      "Epoch: 3790 => Train Accuraccy: 87.19% || Test Accuracy:90.06% || Loss: 0.42835815301995583\n",
      "Epoch: 3800 => Train Accuraccy: 87.19% || Test Accuracy:90.06% || Loss: 0.42756722088237414\n",
      "Epoch: 3810 => Train Accuraccy: 87.19% || Test Accuracy:90.06% || Loss: 0.42677926397141164\n",
      "Epoch: 3820 => Train Accuraccy: 87.19% || Test Accuracy:90.06% || Loss: 0.42599426011519925\n",
      "Epoch: 3830 => Train Accuraccy: 87.19% || Test Accuracy:90.06% || Loss: 0.42521218742355266\n",
      "Epoch: 3840 => Train Accuraccy: 87.19% || Test Accuracy:90.06% || Loss: 0.42443302428413693\n",
      "Epoch: 3850 => Train Accuraccy: 87.19% || Test Accuracy:90.06% || Loss: 0.4236567493586947\n",
      "Epoch: 3860 => Train Accuraccy: 87.19% || Test Accuracy:90.06% || Loss: 0.4228833415793333\n",
      "Epoch: 3870 => Train Accuraccy: 87.44% || Test Accuracy:90.06% || Loss: 0.4221127801448794\n",
      "Epoch: 3880 => Train Accuraccy: 87.44% || Test Accuracy:90.06% || Loss: 0.4213450445172862\n",
      "Epoch: 3890 => Train Accuraccy: 87.44% || Test Accuracy:90.06% || Loss: 0.4205801144181095\n",
      "Epoch: 3900 => Train Accuraccy: 87.69% || Test Accuracy:90.06% || Loss: 0.41981796982503083\n",
      "Epoch: 3910 => Train Accuraccy: 87.69% || Test Accuracy:90.64% || Loss: 0.4190585909684479\n",
      "Epoch: 3920 => Train Accuraccy: 87.69% || Test Accuracy:90.64% || Loss: 0.41830195832811135\n",
      "Epoch: 3930 => Train Accuraccy: 87.94% || Test Accuracy:90.64% || Loss: 0.4175480526298241\n",
      "Epoch: 3940 => Train Accuraccy: 87.94% || Test Accuracy:90.64% || Loss: 0.41679685484218704\n",
      "Epoch: 3950 => Train Accuraccy: 87.94% || Test Accuracy:90.64% || Loss: 0.41604834617340364\n",
      "Epoch: 3960 => Train Accuraccy: 87.94% || Test Accuracy:90.64% || Loss: 0.4153025080681302\n",
      "Epoch: 3970 => Train Accuraccy: 87.94% || Test Accuracy:90.64% || Loss: 0.41455932220438235\n",
      "Epoch: 3980 => Train Accuraccy: 87.94% || Test Accuracy:90.64% || Loss: 0.41381877049048565\n",
      "Epoch: 3990 => Train Accuraccy: 87.94% || Test Accuracy:90.64% || Loss: 0.4130808350620783\n",
      "Epoch: 4000 => Train Accuraccy: 87.94% || Test Accuracy:90.64% || Loss: 0.4123454982791626\n",
      "Epoch: 4010 => Train Accuraccy: 87.94% || Test Accuracy:90.64% || Loss: 0.4116127427231964\n",
      "Epoch: 4020 => Train Accuraccy: 87.94% || Test Accuracy:90.64% || Loss: 0.4108825511942407\n",
      "Epoch: 4030 => Train Accuraccy: 87.94% || Test Accuracy:90.64% || Loss: 0.41015490670814253\n",
      "Epoch: 4040 => Train Accuraccy: 88.19% || Test Accuracy:90.64% || Loss: 0.4094297924937703\n",
      "Epoch: 4050 => Train Accuraccy: 88.19% || Test Accuracy:90.64% || Loss: 0.40870719199028627\n",
      "Epoch: 4060 => Train Accuraccy: 88.19% || Test Accuracy:90.64% || Loss: 0.4079870888444653\n",
      "Epoch: 4070 => Train Accuraccy: 88.19% || Test Accuracy:90.64% || Loss: 0.4072694669080521\n",
      "Epoch: 4080 => Train Accuraccy: 88.19% || Test Accuracy:90.64% || Loss: 0.40655431023516647\n",
      "Epoch: 4090 => Train Accuraccy: 88.19% || Test Accuracy:90.64% || Loss: 0.4058416030797391\n",
      "Epoch: 4100 => Train Accuraccy: 88.19% || Test Accuracy:90.64% || Loss: 0.4051313298929957\n",
      "Epoch: 4110 => Train Accuraccy: 88.19% || Test Accuracy:90.64% || Loss: 0.40442347532097495\n",
      "Epoch: 4120 => Train Accuraccy: 88.19% || Test Accuracy:90.64% || Loss: 0.4037180242020854\n",
      "Epoch: 4130 => Train Accuraccy: 88.44% || Test Accuracy:90.64% || Loss: 0.40301496156470185\n",
      "Epoch: 4140 => Train Accuraccy: 88.44% || Test Accuracy:90.64% || Loss: 0.4023142726247945\n",
      "Epoch: 4150 => Train Accuraccy: 88.44% || Test Accuracy:90.64% || Loss: 0.4016159427836005\n",
      "Epoch: 4160 => Train Accuraccy: 88.44% || Test Accuracy:90.64% || Loss: 0.4009199576253234\n",
      "Epoch: 4170 => Train Accuraccy: 88.44% || Test Accuracy:90.64% || Loss: 0.40022630291487227\n",
      "Epoch: 4180 => Train Accuraccy: 88.44% || Test Accuracy:90.64% || Loss: 0.39953496459563426\n",
      "Epoch: 4190 => Train Accuraccy: 88.69% || Test Accuracy:90.64% || Loss: 0.3988459287872797\n",
      "Epoch: 4200 => Train Accuraccy: 88.69% || Test Accuracy:90.64% || Loss: 0.39815918178359905\n",
      "Epoch: 4210 => Train Accuraccy: 88.69% || Test Accuracy:90.64% || Loss: 0.3974747100503728\n",
      "Epoch: 4220 => Train Accuraccy: 88.69% || Test Accuracy:90.64% || Loss: 0.39679250022327817\n",
      "Epoch: 4230 => Train Accuraccy: 88.69% || Test Accuracy:90.64% || Loss: 0.39611253910581423\n",
      "Epoch: 4240 => Train Accuraccy: 88.69% || Test Accuracy:90.64% || Loss: 0.39543481366727123\n",
      "Epoch: 4250 => Train Accuraccy: 88.69% || Test Accuracy:90.64% || Loss: 0.39475931104072143\n",
      "Epoch: 4260 => Train Accuraccy: 88.69% || Test Accuracy:90.64% || Loss: 0.3940860185210403\n",
      "Epoch: 4270 => Train Accuraccy: 88.69% || Test Accuracy:90.64% || Loss: 0.39341492356295926\n",
      "Epoch: 4280 => Train Accuraccy: 88.69% || Test Accuracy:90.64% || Loss: 0.3927460137791432\n",
      "Epoch: 4290 => Train Accuraccy: 88.69% || Test Accuracy:90.64% || Loss: 0.3920792769382969\n",
      "Epoch: 4300 => Train Accuraccy: 88.69% || Test Accuracy:90.64% || Loss: 0.3914147009632992\n",
      "Epoch: 4310 => Train Accuraccy: 88.69% || Test Accuracy:90.64% || Loss: 0.39075227392936085\n",
      "Epoch: 4320 => Train Accuraccy: 88.69% || Test Accuracy:90.64% || Loss: 0.39009198406221296\n",
      "Epoch: 4330 => Train Accuraccy: 88.69% || Test Accuracy:90.64% || Loss: 0.38943381973631497\n",
      "Epoch: 4340 => Train Accuraccy: 88.69% || Test Accuracy:90.64% || Loss: 0.38877776947309123\n",
      "Epoch: 4350 => Train Accuraccy: 88.69% || Test Accuracy:90.64% || Loss: 0.3881238219391921\n",
      "Epoch: 4360 => Train Accuraccy: 88.69% || Test Accuracy:90.64% || Loss: 0.38747196594477645\n",
      "Epoch: 4370 => Train Accuraccy: 88.94% || Test Accuracy:90.64% || Loss: 0.3868221904418192\n",
      "Epoch: 4380 => Train Accuraccy: 88.94% || Test Accuracy:90.64% || Loss: 0.3861744845224404\n",
      "Epoch: 4390 => Train Accuraccy: 88.94% || Test Accuracy:90.64% || Loss: 0.3855288374172572\n",
      "Epoch: 4400 => Train Accuraccy: 88.94% || Test Accuracy:90.64% || Loss: 0.3848852384937581\n",
      "Epoch: 4410 => Train Accuraccy: 88.94% || Test Accuracy:90.64% || Loss: 0.3842436772546988\n",
      "Epoch: 4420 => Train Accuraccy: 88.94% || Test Accuracy:90.64% || Loss: 0.3836041433365152\n",
      "Epoch: 4430 => Train Accuraccy: 88.94% || Test Accuracy:90.64% || Loss: 0.3829666265077611\n",
      "Epoch: 4440 => Train Accuraccy: 88.94% || Test Accuracy:90.64% || Loss: 0.38233111666756475\n",
      "Epoch: 4450 => Train Accuraccy: 88.94% || Test Accuracy:90.64% || Loss: 0.3816976038441022\n",
      "Epoch: 4460 => Train Accuraccy: 88.94% || Test Accuracy:90.64% || Loss: 0.3810660781930935\n",
      "Epoch: 4470 => Train Accuraccy: 88.94% || Test Accuracy:90.64% || Loss: 0.38043652999631283\n",
      "Epoch: 4480 => Train Accuraccy: 88.94% || Test Accuracy:90.64% || Loss: 0.3798089496601192\n",
      "Epoch: 4490 => Train Accuraccy: 88.94% || Test Accuracy:90.64% || Loss: 0.37918332771400715\n",
      "Epoch: 4500 => Train Accuraccy: 88.94% || Test Accuracy:90.64% || Loss: 0.37855965480916715\n",
      "Epoch: 4510 => Train Accuraccy: 88.94% || Test Accuracy:90.64% || Loss: 0.37793792171707086\n",
      "Epoch: 4520 => Train Accuraccy: 88.94% || Test Accuracy:90.64% || Loss: 0.37731811932806664\n",
      "Epoch: 4530 => Train Accuraccy: 88.94% || Test Accuracy:90.64% || Loss: 0.3767002386499942\n",
      "Epoch: 4540 => Train Accuraccy: 88.94% || Test Accuracy:90.64% || Loss: 0.37608427080681117\n",
      "Epoch: 4550 => Train Accuraccy: 88.94% || Test Accuracy:90.64% || Loss: 0.3754702070372383\n",
      "Epoch: 4560 => Train Accuraccy: 88.94% || Test Accuracy:90.64% || Loss: 0.3748580386934161\n",
      "Epoch: 4570 => Train Accuraccy: 88.94% || Test Accuracy:90.64% || Loss: 0.3742477572395782\n",
      "Epoch: 4580 => Train Accuraccy: 88.94% || Test Accuracy:90.64% || Loss: 0.3736393542507329\n",
      "Epoch: 4590 => Train Accuraccy: 89.20% || Test Accuracy:90.64% || Loss: 0.37303282141136535\n",
      "Epoch: 4600 => Train Accuraccy: 89.20% || Test Accuracy:90.64% || Loss: 0.37242815051414785\n",
      "Epoch: 4610 => Train Accuraccy: 89.20% || Test Accuracy:90.64% || Loss: 0.37182533345866\n",
      "Epoch: 4620 => Train Accuraccy: 89.20% || Test Accuracy:90.64% || Loss: 0.37122436225012745\n",
      "Epoch: 4630 => Train Accuraccy: 89.20% || Test Accuracy:90.64% || Loss: 0.3706252289981665\n",
      "Epoch: 4640 => Train Accuraccy: 89.45% || Test Accuracy:90.64% || Loss: 0.3700279259155411\n",
      "Epoch: 4650 => Train Accuraccy: 89.45% || Test Accuracy:90.64% || Loss: 0.3694324453169343\n",
      "Epoch: 4660 => Train Accuraccy: 89.45% || Test Accuracy:90.64% || Loss: 0.3688387796177213\n",
      "Epoch: 4670 => Train Accuraccy: 89.45% || Test Accuracy:90.64% || Loss: 0.3682469213327613\n",
      "Epoch: 4680 => Train Accuraccy: 89.45% || Test Accuracy:90.64% || Loss: 0.3676568630751952\n",
      "Epoch: 4690 => Train Accuraccy: 89.45% || Test Accuracy:90.64% || Loss: 0.3670685975552503\n",
      "Epoch: 4700 => Train Accuraccy: 89.70% || Test Accuracy:90.64% || Loss: 0.36648211757905536\n",
      "Epoch: 4710 => Train Accuraccy: 89.70% || Test Accuracy:90.64% || Loss: 0.36589741604746634\n",
      "Epoch: 4720 => Train Accuraccy: 89.70% || Test Accuracy:90.64% || Loss: 0.365314485954896\n",
      "Epoch: 4730 => Train Accuraccy: 89.70% || Test Accuracy:90.64% || Loss: 0.3647333203881525\n",
      "Epoch: 4740 => Train Accuraccy: 89.70% || Test Accuracy:90.64% || Loss: 0.36415391252528617\n",
      "Epoch: 4750 => Train Accuraccy: 89.70% || Test Accuracy:91.23% || Loss: 0.36357625563444124\n",
      "Epoch: 4760 => Train Accuraccy: 89.70% || Test Accuracy:91.23% || Loss: 0.3630003430727141\n",
      "Epoch: 4770 => Train Accuraccy: 89.70% || Test Accuracy:91.23% || Loss: 0.3624261682850219\n",
      "Epoch: 4780 => Train Accuraccy: 89.70% || Test Accuracy:91.23% || Loss: 0.3618537248029674\n",
      "Epoch: 4790 => Train Accuraccy: 89.70% || Test Accuracy:91.23% || Loss: 0.36128300624372\n",
      "Epoch: 4800 => Train Accuraccy: 89.70% || Test Accuracy:91.23% || Loss: 0.360714006308893\n",
      "Epoch: 4810 => Train Accuraccy: 89.70% || Test Accuracy:91.23% || Loss: 0.3601467187834302\n",
      "Epoch: 4820 => Train Accuraccy: 89.70% || Test Accuracy:91.23% || Loss: 0.359581137534497\n",
      "Epoch: 4830 => Train Accuraccy: 89.70% || Test Accuracy:91.23% || Loss: 0.3590172565103699\n",
      "Epoch: 4840 => Train Accuraccy: 89.70% || Test Accuracy:91.23% || Loss: 0.35845506973933644\n",
      "Epoch: 4850 => Train Accuraccy: 89.70% || Test Accuracy:91.23% || Loss: 0.35789457132859653\n",
      "Epoch: 4860 => Train Accuraccy: 89.70% || Test Accuracy:91.23% || Loss: 0.357335755463161\n",
      "Epoch: 4870 => Train Accuraccy: 89.70% || Test Accuracy:91.23% || Loss: 0.35677861640476305\n",
      "Epoch: 4880 => Train Accuraccy: 89.70% || Test Accuracy:91.23% || Loss: 0.3562231484907606\n",
      "Epoch: 4890 => Train Accuraccy: 89.70% || Test Accuracy:91.23% || Loss: 0.355669346133051\n",
      "Epoch: 4900 => Train Accuraccy: 89.70% || Test Accuracy:91.23% || Loss: 0.35511720381698075\n",
      "Epoch: 4910 => Train Accuraccy: 89.70% || Test Accuracy:91.23% || Loss: 0.3545667161002578\n",
      "Epoch: 4920 => Train Accuraccy: 89.70% || Test Accuracy:91.23% || Loss: 0.3540178776118684\n",
      "Epoch: 4930 => Train Accuraccy: 89.70% || Test Accuracy:91.23% || Loss: 0.35347068305098867\n",
      "Epoch: 4940 => Train Accuraccy: 89.70% || Test Accuracy:91.23% || Loss: 0.35292512718590524\n",
      "Epoch: 4950 => Train Accuraccy: 89.70% || Test Accuracy:91.23% || Loss: 0.3523812048529257\n",
      "Epoch: 4960 => Train Accuraccy: 89.70% || Test Accuracy:91.23% || Loss: 0.35183891095530034\n",
      "Epoch: 4970 => Train Accuraccy: 89.70% || Test Accuracy:91.23% || Loss: 0.35129824046213426\n",
      "Epoch: 4980 => Train Accuraccy: 89.70% || Test Accuracy:91.23% || Loss: 0.3507591884073049\n",
      "Epoch: 4990 => Train Accuraccy: 89.70% || Test Accuracy:91.23% || Loss: 0.3502217498883777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(30, 1), dtype=float64, numpy=\n",
       " array([[ 0.01202555],\n",
       "        [-0.12073776],\n",
       "        [-2.57376662],\n",
       "        [ 0.9314192 ],\n",
       "        [ 0.02250823],\n",
       "        [-0.07088151],\n",
       "        [-0.52228708],\n",
       "        [ 0.44812821],\n",
       "        [ 0.19276412],\n",
       "        [ 0.52171078],\n",
       "        [-0.27829762],\n",
       "        [-0.93028466],\n",
       "        [ 0.45459687],\n",
       "        [ 0.02204154],\n",
       "        [ 1.45012121],\n",
       "        [-1.51643219],\n",
       "        [ 0.25083488],\n",
       "        [ 1.0496372 ],\n",
       "        [-0.73458509],\n",
       "        [-0.33740646],\n",
       "        [-0.28372685],\n",
       "        [-0.97666384],\n",
       "        [-1.61622363],\n",
       "        [-1.70586701],\n",
       "        [-0.18739892],\n",
       "        [ 1.81363319],\n",
       "        [-0.46015321],\n",
       "        [-0.03206563],\n",
       "        [ 0.81100565],\n",
       "        [-3.65905586]])>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=0.35088333376210823>,\n",
       " {'dw': <tf.Tensor: shape=(30, 1), dtype=float64, numpy=\n",
       "  array([[ 0.0432324 ],\n",
       "         [-0.01266052],\n",
       "         [ 0.04183334],\n",
       "         [ 0.04212303],\n",
       "         [ 0.04794096],\n",
       "         [ 0.00442735],\n",
       "         [ 0.01286972],\n",
       "         [ 0.04763496],\n",
       "         [ 0.03173555],\n",
       "         [-0.05401116],\n",
       "         [ 0.02771574],\n",
       "         [-0.06326272],\n",
       "         [ 0.0233833 ],\n",
       "         [ 0.02901783],\n",
       "         [ 0.04837603],\n",
       "         [-0.04561619],\n",
       "         [-0.05004525],\n",
       "         [ 0.01447634],\n",
       "         [ 0.00122144],\n",
       "         [-0.0850613 ],\n",
       "         [ 0.0475794 ],\n",
       "         [-0.00687573],\n",
       "         [ 0.0443108 ],\n",
       "         [ 0.0437107 ],\n",
       "         [ 0.06999376],\n",
       "         [-0.00182179],\n",
       "         [ 0.00125565],\n",
       "         [ 0.04906638],\n",
       "         [ 0.0459233 ],\n",
       "         [-0.04565599]])>,\n",
       "  'db': <tf.Tensor: shape=(), dtype=float64, numpy=-0.03259899562515583>},\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=0.34973943079314257>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(xtrain.shape[0])\n",
    "model.optimize(xtrain, ytrain, validation=(xtest, ytest),\n",
    "               num_iterations=5000, learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(model.losses)\n",
    "plt.plot(model.metric[0], color='darkorange')\n",
    "plt.plot(model.metric[1], color='lightgreen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w': <tf.Tensor: shape=(30, 1), dtype=float64, numpy=\n",
       " array([[ 0.01202555],\n",
       "        [-0.12073776],\n",
       "        [-2.57376662],\n",
       "        [ 0.9314192 ],\n",
       "        [ 0.02250823],\n",
       "        [-0.07088151],\n",
       "        [-0.52228708],\n",
       "        [ 0.44812821],\n",
       "        [ 0.19276412],\n",
       "        [ 0.52171078],\n",
       "        [-0.27829762],\n",
       "        [-0.93028466],\n",
       "        [ 0.45459687],\n",
       "        [ 0.02204154],\n",
       "        [ 1.45012121],\n",
       "        [-1.51643219],\n",
       "        [ 0.25083488],\n",
       "        [ 1.0496372 ],\n",
       "        [-0.73458509],\n",
       "        [-0.33740646],\n",
       "        [-0.28372685],\n",
       "        [-0.97666384],\n",
       "        [-1.61622363],\n",
       "        [-1.70586701],\n",
       "        [-0.18739892],\n",
       "        [ 1.81363319],\n",
       "        [-0.46015321],\n",
       "        [-0.03206563],\n",
       "        [ 0.81100565],\n",
       "        [-3.65905586]])>,\n",
       " 'b': <tf.Tensor: shape=(), dtype=float64, numpy=0.35088333376210823>}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(171,), dtype=bool, numpy=\n",
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        True, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True, False,  True, False,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True, False, False, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "        True,  True,  True,  True,  True, False, False, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True])>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(xtest)[0] == ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0,\n",
       "  20.35175879396985,\n",
       "  20.603015075376888,\n",
       "  20.85427135678391,\n",
       "  20.85427135678391,\n",
       "  21.356783919597987,\n",
       "  21.859296482412063,\n",
       "  22.110552763819086,\n",
       "  22.36180904522614,\n",
       "  22.8643216080402,\n",
       "  22.8643216080402,\n",
       "  22.8643216080402,\n",
       "  22.8643216080402,\n",
       "  23.618090452261313,\n",
       "  24.120603015075375,\n",
       "  25.125628140703512,\n",
       "  25.376884422110564,\n",
       "  25.628140703517587,\n",
       "  25.628140703517587,\n",
       "  26.130653266331663,\n",
       "  26.63316582914574,\n",
       "  26.884422110552762,\n",
       "  27.1356783919598,\n",
       "  27.63819095477386,\n",
       "  28.391959798994975,\n",
       "  28.643216080402013,\n",
       "  28.894472361809036,\n",
       "  29.64824120603015,\n",
       "  30.402010050251263,\n",
       "  31.4070351758794,\n",
       "  31.4070351758794,\n",
       "  31.4070351758794,\n",
       "  31.909547738693462,\n",
       "  32.1608040201005,\n",
       "  33.16582914572864,\n",
       "  33.417085427135675,\n",
       "  33.91959798994975,\n",
       "  34.92462311557789,\n",
       "  34.92462311557789,\n",
       "  34.92462311557789,\n",
       "  35.42713567839196,\n",
       "  35.929648241206024,\n",
       "  36.18090452261307,\n",
       "  36.68341708542714,\n",
       "  37.1859296482412,\n",
       "  37.437185929648244,\n",
       "  38.19095477386934,\n",
       "  38.94472361809045,\n",
       "  39.44723618090452,\n",
       "  40.7035175879397,\n",
       "  40.95477386934674,\n",
       "  41.708542713567844,\n",
       "  42.462311557788944,\n",
       "  42.96482412060302,\n",
       "  43.21608040201005,\n",
       "  43.71859296482412,\n",
       "  43.96984924623115,\n",
       "  44.72361809045226,\n",
       "  45.226130653266324,\n",
       "  45.7286432160804,\n",
       "  46.231155778894475,\n",
       "  46.733668341708544,\n",
       "  47.48743718592965,\n",
       "  47.48743718592965,\n",
       "  48.492462311557794,\n",
       "  48.743718592964825,\n",
       "  48.994974874371856,\n",
       "  49.497487437185924,\n",
       "  49.497487437185924,\n",
       "  50.0,\n",
       "  50.502512562814076,\n",
       "  50.753768844221106,\n",
       "  51.256281407035175,\n",
       "  51.75879396984925,\n",
       "  52.51256281407035,\n",
       "  52.51256281407035,\n",
       "  52.76381909547739,\n",
       "  53.015075376884425,\n",
       "  53.517587939698494,\n",
       "  54.02010050251256,\n",
       "  54.2713567839196,\n",
       "  54.2713567839196,\n",
       "  55.0251256281407,\n",
       "  55.527638190954775,\n",
       "  55.778894472361806,\n",
       "  56.030150753768844,\n",
       "  56.28140703517588,\n",
       "  57.03517587939699,\n",
       "  58.040201005025125,\n",
       "  58.79396984924623,\n",
       "  59.04522613065327,\n",
       "  59.54773869346734,\n",
       "  60.301507537688444,\n",
       "  61.05527638190954,\n",
       "  61.30653266331658,\n",
       "  61.80904522613066,\n",
       "  61.80904522613066,\n",
       "  62.06030150753769,\n",
       "  62.31155778894472,\n",
       "  62.814070351758794,\n",
       "  62.814070351758794,\n",
       "  63.81909547738693,\n",
       "  64.07035175879398,\n",
       "  64.321608040201,\n",
       "  64.57286432160805,\n",
       "  64.82412060301507,\n",
       "  64.82412060301507,\n",
       "  65.07537688442211,\n",
       "  65.07537688442211,\n",
       "  65.32663316582915,\n",
       "  65.57788944723617,\n",
       "  65.57788944723617,\n",
       "  65.57788944723617,\n",
       "  65.82914572864323,\n",
       "  66.58291457286433,\n",
       "  66.83417085427135,\n",
       "  66.83417085427135,\n",
       "  67.58793969849246,\n",
       "  67.8391959798995,\n",
       "  68.09045226130654,\n",
       "  68.09045226130654,\n",
       "  68.34170854271358,\n",
       "  68.34170854271358,\n",
       "  68.34170854271358,\n",
       "  68.34170854271358,\n",
       "  68.34170854271358,\n",
       "  68.5929648241206,\n",
       "  68.84422110552764,\n",
       "  68.84422110552764,\n",
       "  68.84422110552764,\n",
       "  69.34673366834171,\n",
       "  69.59798994974875,\n",
       "  69.84924623115577,\n",
       "  70.10050251256281,\n",
       "  70.35175879396985,\n",
       "  70.60301507537689,\n",
       "  70.60301507537689,\n",
       "  70.60301507537689,\n",
       "  70.60301507537689,\n",
       "  70.85427135678393,\n",
       "  71.10552763819095,\n",
       "  71.10552763819095,\n",
       "  71.35678391959799,\n",
       "  71.60804020100502,\n",
       "  71.85929648241206,\n",
       "  71.85929648241206,\n",
       "  71.85929648241206,\n",
       "  71.85929648241206,\n",
       "  72.1105527638191,\n",
       "  72.61306532663316,\n",
       "  72.8643216080402,\n",
       "  73.11557788944724,\n",
       "  73.61809045226131,\n",
       "  73.86934673366834,\n",
       "  73.86934673366834,\n",
       "  73.61809045226131,\n",
       "  73.86934673366834,\n",
       "  74.12060301507537,\n",
       "  74.37185929648241,\n",
       "  74.87437185929649,\n",
       "  74.87437185929649,\n",
       "  74.87437185929649,\n",
       "  75.12562814070353,\n",
       "  75.12562814070353,\n",
       "  75.12562814070353,\n",
       "  75.12562814070353,\n",
       "  75.37688442211055,\n",
       "  75.37688442211055,\n",
       "  75.37688442211055,\n",
       "  75.37688442211055,\n",
       "  75.37688442211055,\n",
       "  75.37688442211055,\n",
       "  75.37688442211055,\n",
       "  75.37688442211055,\n",
       "  75.37688442211055,\n",
       "  75.37688442211055,\n",
       "  75.87939698492463,\n",
       "  75.87939698492463,\n",
       "  75.87939698492463,\n",
       "  75.87939698492463,\n",
       "  76.13065326633165,\n",
       "  76.3819095477387,\n",
       "  76.3819095477387,\n",
       "  76.63316582914572,\n",
       "  77.1356783919598,\n",
       "  77.1356783919598,\n",
       "  77.38693467336684,\n",
       "  77.38693467336684,\n",
       "  77.38693467336684,\n",
       "  77.38693467336684,\n",
       "  77.63819095477388,\n",
       "  77.63819095477388,\n",
       "  77.63819095477388,\n",
       "  78.14070351758794,\n",
       "  78.39195979899498,\n",
       "  78.39195979899498,\n",
       "  79.39698492462311,\n",
       "  79.39698492462311,\n",
       "  79.64824120603015,\n",
       "  79.64824120603015,\n",
       "  79.64824120603015,\n",
       "  79.64824120603015,\n",
       "  79.64824120603015,\n",
       "  79.64824120603015,\n",
       "  80.40201005025125,\n",
       "  80.40201005025125,\n",
       "  80.40201005025125,\n",
       "  80.40201005025125,\n",
       "  80.40201005025125,\n",
       "  80.65326633165829,\n",
       "  80.65326633165829,\n",
       "  81.4070351758794,\n",
       "  81.65829145728644,\n",
       "  81.65829145728644,\n",
       "  81.65829145728644,\n",
       "  81.90954773869346,\n",
       "  81.90954773869346,\n",
       "  81.90954773869346,\n",
       "  81.90954773869346,\n",
       "  82.1608040201005,\n",
       "  82.1608040201005,\n",
       "  82.1608040201005,\n",
       "  82.1608040201005,\n",
       "  82.1608040201005,\n",
       "  82.1608040201005,\n",
       "  82.66331658291458,\n",
       "  82.66331658291458,\n",
       "  82.66331658291458,\n",
       "  82.66331658291458,\n",
       "  83.16582914572865,\n",
       "  83.16582914572865,\n",
       "  83.41708542713567,\n",
       "  83.41708542713567,\n",
       "  83.41708542713567,\n",
       "  83.41708542713567,\n",
       "  83.66834170854271,\n",
       "  83.66834170854271,\n",
       "  83.66834170854271,\n",
       "  83.66834170854271,\n",
       "  83.91959798994975,\n",
       "  83.91959798994975,\n",
       "  84.17085427135679,\n",
       "  84.17085427135679,\n",
       "  84.17085427135679,\n",
       "  84.17085427135679,\n",
       "  84.17085427135679,\n",
       "  84.17085427135679,\n",
       "  84.17085427135679,\n",
       "  84.17085427135679,\n",
       "  84.17085427135679,\n",
       "  84.17085427135679,\n",
       "  84.17085427135679,\n",
       "  84.17085427135679,\n",
       "  84.42211055276383,\n",
       "  84.67336683417085,\n",
       "  84.67336683417085,\n",
       "  84.67336683417085,\n",
       "  84.67336683417085,\n",
       "  84.67336683417085,\n",
       "  84.67336683417085,\n",
       "  84.67336683417085,\n",
       "  84.67336683417085,\n",
       "  84.67336683417085,\n",
       "  84.67336683417085,\n",
       "  84.67336683417085,\n",
       "  84.67336683417085,\n",
       "  84.67336683417085,\n",
       "  84.67336683417085,\n",
       "  84.67336683417085,\n",
       "  84.67336683417085,\n",
       "  84.67336683417085,\n",
       "  84.67336683417085,\n",
       "  84.67336683417085,\n",
       "  84.67336683417085,\n",
       "  84.92462311557789,\n",
       "  84.92462311557789,\n",
       "  84.92462311557789,\n",
       "  84.92462311557789,\n",
       "  84.92462311557789,\n",
       "  84.92462311557789,\n",
       "  84.92462311557789,\n",
       "  85.17587939698493,\n",
       "  85.17587939698493,\n",
       "  85.42713567839196,\n",
       "  85.42713567839196,\n",
       "  85.678391959799,\n",
       "  85.678391959799,\n",
       "  85.678391959799,\n",
       "  86.18090452261306,\n",
       "  86.18090452261306,\n",
       "  86.4321608040201,\n",
       "  86.4321608040201,\n",
       "  86.4321608040201,\n",
       "  86.4321608040201,\n",
       "  86.4321608040201,\n",
       "  86.4321608040201,\n",
       "  86.4321608040201,\n",
       "  86.4321608040201,\n",
       "  86.4321608040201,\n",
       "  86.4321608040201,\n",
       "  86.68341708542714,\n",
       "  86.68341708542714,\n",
       "  86.68341708542714,\n",
       "  86.68341708542714,\n",
       "  86.93467336683418,\n",
       "  86.93467336683418,\n",
       "  86.93467336683418,\n",
       "  86.93467336683418,\n",
       "  86.93467336683418,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.43718592964824,\n",
       "  87.43718592964824,\n",
       "  87.43718592964824,\n",
       "  87.68844221105527,\n",
       "  87.68844221105527,\n",
       "  87.68844221105527,\n",
       "  87.93969849246231,\n",
       "  87.93969849246231,\n",
       "  87.93969849246231,\n",
       "  87.93969849246231,\n",
       "  87.93969849246231,\n",
       "  87.93969849246231,\n",
       "  87.93969849246231,\n",
       "  87.93969849246231,\n",
       "  87.93969849246231,\n",
       "  87.93969849246231,\n",
       "  87.93969849246231,\n",
       "  88.19095477386935,\n",
       "  88.19095477386935,\n",
       "  88.19095477386935,\n",
       "  88.19095477386935,\n",
       "  88.19095477386935,\n",
       "  88.19095477386935,\n",
       "  88.19095477386935,\n",
       "  88.19095477386935,\n",
       "  88.19095477386935,\n",
       "  88.44221105527637,\n",
       "  88.44221105527637,\n",
       "  88.44221105527637,\n",
       "  88.44221105527637,\n",
       "  88.44221105527637,\n",
       "  88.44221105527637,\n",
       "  88.69346733668341,\n",
       "  88.69346733668341,\n",
       "  88.69346733668341,\n",
       "  88.69346733668341,\n",
       "  88.69346733668341,\n",
       "  88.69346733668341,\n",
       "  88.69346733668341,\n",
       "  88.69346733668341,\n",
       "  88.69346733668341,\n",
       "  88.69346733668341,\n",
       "  88.69346733668341,\n",
       "  88.69346733668341,\n",
       "  88.69346733668341,\n",
       "  88.69346733668341,\n",
       "  88.69346733668341,\n",
       "  88.69346733668341,\n",
       "  88.69346733668341,\n",
       "  88.69346733668341,\n",
       "  88.94472361809045,\n",
       "  88.94472361809045,\n",
       "  88.94472361809045,\n",
       "  88.94472361809045,\n",
       "  88.94472361809045,\n",
       "  88.94472361809045,\n",
       "  88.94472361809045,\n",
       "  88.94472361809045,\n",
       "  88.94472361809045,\n",
       "  88.94472361809045,\n",
       "  88.94472361809045,\n",
       "  88.94472361809045,\n",
       "  88.94472361809045,\n",
       "  88.94472361809045,\n",
       "  88.94472361809045,\n",
       "  88.94472361809045,\n",
       "  88.94472361809045,\n",
       "  88.94472361809045,\n",
       "  88.94472361809045,\n",
       "  88.94472361809045,\n",
       "  88.94472361809045,\n",
       "  88.94472361809045,\n",
       "  89.19597989949749,\n",
       "  89.19597989949749,\n",
       "  89.19597989949749,\n",
       "  89.19597989949749,\n",
       "  89.19597989949749,\n",
       "  89.44723618090453,\n",
       "  89.44723618090453,\n",
       "  89.44723618090453,\n",
       "  89.44723618090453,\n",
       "  89.44723618090453,\n",
       "  89.44723618090453,\n",
       "  89.69849246231156,\n",
       "  89.69849246231156,\n",
       "  89.69849246231156,\n",
       "  89.69849246231156,\n",
       "  89.69849246231156,\n",
       "  89.69849246231156,\n",
       "  89.69849246231156,\n",
       "  89.69849246231156,\n",
       "  89.69849246231156,\n",
       "  89.69849246231156,\n",
       "  89.69849246231156,\n",
       "  89.69849246231156,\n",
       "  89.69849246231156,\n",
       "  89.69849246231156,\n",
       "  89.69849246231156,\n",
       "  89.69849246231156,\n",
       "  89.69849246231156,\n",
       "  89.69849246231156,\n",
       "  89.69849246231156,\n",
       "  89.69849246231156,\n",
       "  89.69849246231156,\n",
       "  89.69849246231156,\n",
       "  89.69849246231156,\n",
       "  89.69849246231156,\n",
       "  89.69849246231156,\n",
       "  89.69849246231156,\n",
       "  89.69849246231156,\n",
       "  89.69849246231156,\n",
       "  89.69849246231156,\n",
       "  89.69849246231156],\n",
       " [0.0,\n",
       "  22.807017543859658,\n",
       "  23.391812865497073,\n",
       "  23.976608187134502,\n",
       "  23.976608187134502,\n",
       "  23.976608187134502,\n",
       "  24.56140350877193,\n",
       "  25.14619883040936,\n",
       "  25.730994152046776,\n",
       "  26.31578947368422,\n",
       "  26.31578947368422,\n",
       "  26.31578947368422,\n",
       "  26.31578947368422,\n",
       "  26.900584795321635,\n",
       "  27.485380116959064,\n",
       "  28.070175438596493,\n",
       "  28.070175438596493,\n",
       "  28.070175438596493,\n",
       "  28.654970760233923,\n",
       "  29.239766081871338,\n",
       "  29.82456140350878,\n",
       "  30.994152046783626,\n",
       "  30.994152046783626,\n",
       "  31.578947368421055,\n",
       "  31.578947368421055,\n",
       "  32.7485380116959,\n",
       "  32.7485380116959,\n",
       "  32.7485380116959,\n",
       "  33.91812865497076,\n",
       "  35.672514619883046,\n",
       "  35.672514619883046,\n",
       "  36.25730994152047,\n",
       "  36.25730994152047,\n",
       "  36.25730994152047,\n",
       "  37.42690058479532,\n",
       "  38.01169590643275,\n",
       "  39.18128654970761,\n",
       "  40.35087719298246,\n",
       "  40.35087719298246,\n",
       "  40.35087719298246,\n",
       "  40.93567251461988,\n",
       "  42.10526315789473,\n",
       "  42.69005847953217,\n",
       "  42.69005847953217,\n",
       "  43.85964912280702,\n",
       "  44.44444444444444,\n",
       "  44.44444444444444,\n",
       "  45.02923976608187,\n",
       "  45.02923976608187,\n",
       "  45.02923976608187,\n",
       "  46.19883040935673,\n",
       "  46.783625730994146,\n",
       "  46.783625730994146,\n",
       "  47.953216374269005,\n",
       "  49.122807017543856,\n",
       "  50.292397660818715,\n",
       "  50.877192982456144,\n",
       "  51.46198830409357,\n",
       "  52.046783625730995,\n",
       "  52.631578947368425,\n",
       "  53.216374269005854,\n",
       "  54.97076023391813,\n",
       "  54.97076023391813,\n",
       "  54.97076023391813,\n",
       "  54.97076023391813,\n",
       "  56.140350877192986,\n",
       "  57.30994152046784,\n",
       "  57.89473684210527,\n",
       "  57.89473684210527,\n",
       "  57.89473684210527,\n",
       "  57.89473684210527,\n",
       "  58.47953216374269,\n",
       "  58.47953216374269,\n",
       "  59.06432748538012,\n",
       "  59.06432748538012,\n",
       "  59.06432748538012,\n",
       "  60.23391812865497,\n",
       "  60.8187134502924,\n",
       "  61.98830409356725,\n",
       "  61.98830409356725,\n",
       "  61.98830409356725,\n",
       "  61.98830409356725,\n",
       "  63.15789473684211,\n",
       "  63.15789473684211,\n",
       "  63.74269005847953,\n",
       "  63.74269005847953,\n",
       "  63.74269005847953,\n",
       "  64.32748538011697,\n",
       "  64.32748538011697,\n",
       "  64.91228070175438,\n",
       "  65.49707602339181,\n",
       "  65.49707602339181,\n",
       "  65.49707602339181,\n",
       "  66.08187134502924,\n",
       "  66.08187134502924,\n",
       "  66.66666666666667,\n",
       "  66.66666666666667,\n",
       "  67.2514619883041,\n",
       "  67.2514619883041,\n",
       "  67.2514619883041,\n",
       "  67.2514619883041,\n",
       "  67.83625730994152,\n",
       "  67.83625730994152,\n",
       "  68.42105263157895,\n",
       "  69.00584795321637,\n",
       "  69.5906432748538,\n",
       "  69.5906432748538,\n",
       "  70.17543859649123,\n",
       "  70.17543859649123,\n",
       "  70.17543859649123,\n",
       "  70.76023391812865,\n",
       "  70.76023391812865,\n",
       "  70.76023391812865,\n",
       "  70.76023391812865,\n",
       "  71.34502923976609,\n",
       "  71.34502923976609,\n",
       "  71.9298245614035,\n",
       "  72.51461988304094,\n",
       "  73.6842105263158,\n",
       "  73.6842105263158,\n",
       "  73.6842105263158,\n",
       "  73.6842105263158,\n",
       "  74.26900584795322,\n",
       "  74.85380116959064,\n",
       "  74.85380116959064,\n",
       "  74.85380116959064,\n",
       "  74.85380116959064,\n",
       "  75.43859649122807,\n",
       "  75.43859649122807,\n",
       "  75.43859649122807,\n",
       "  75.43859649122807,\n",
       "  75.43859649122807,\n",
       "  75.43859649122807,\n",
       "  75.43859649122807,\n",
       "  76.0233918128655,\n",
       "  76.0233918128655,\n",
       "  76.0233918128655,\n",
       "  76.0233918128655,\n",
       "  76.0233918128655,\n",
       "  76.0233918128655,\n",
       "  76.0233918128655,\n",
       "  76.0233918128655,\n",
       "  76.0233918128655,\n",
       "  76.0233918128655,\n",
       "  76.0233918128655,\n",
       "  76.60818713450293,\n",
       "  76.60818713450293,\n",
       "  76.60818713450293,\n",
       "  77.19298245614036,\n",
       "  77.19298245614036,\n",
       "  77.19298245614036,\n",
       "  77.19298245614036,\n",
       "  77.19298245614036,\n",
       "  77.19298245614036,\n",
       "  77.19298245614036,\n",
       "  77.19298245614036,\n",
       "  77.19298245614036,\n",
       "  77.19298245614036,\n",
       "  77.19298245614036,\n",
       "  77.19298245614036,\n",
       "  77.19298245614036,\n",
       "  77.19298245614036,\n",
       "  77.77777777777777,\n",
       "  78.3625730994152,\n",
       "  78.3625730994152,\n",
       "  78.3625730994152,\n",
       "  78.3625730994152,\n",
       "  78.3625730994152,\n",
       "  78.3625730994152,\n",
       "  78.3625730994152,\n",
       "  78.3625730994152,\n",
       "  78.3625730994152,\n",
       "  78.3625730994152,\n",
       "  78.3625730994152,\n",
       "  78.3625730994152,\n",
       "  78.94736842105263,\n",
       "  79.53216374269006,\n",
       "  79.53216374269006,\n",
       "  79.53216374269006,\n",
       "  79.53216374269006,\n",
       "  80.70175438596492,\n",
       "  80.70175438596492,\n",
       "  80.70175438596492,\n",
       "  80.70175438596492,\n",
       "  80.70175438596492,\n",
       "  81.28654970760235,\n",
       "  81.28654970760235,\n",
       "  81.28654970760235,\n",
       "  81.28654970760235,\n",
       "  81.28654970760235,\n",
       "  81.28654970760235,\n",
       "  81.28654970760235,\n",
       "  81.28654970760235,\n",
       "  81.28654970760235,\n",
       "  81.28654970760235,\n",
       "  81.28654970760235,\n",
       "  81.87134502923976,\n",
       "  81.87134502923976,\n",
       "  82.45614035087719,\n",
       "  82.45614035087719,\n",
       "  82.45614035087719,\n",
       "  82.45614035087719,\n",
       "  83.04093567251462,\n",
       "  83.62573099415205,\n",
       "  83.62573099415205,\n",
       "  84.21052631578948,\n",
       "  84.21052631578948,\n",
       "  84.21052631578948,\n",
       "  84.21052631578948,\n",
       "  84.21052631578948,\n",
       "  84.21052631578948,\n",
       "  84.21052631578948,\n",
       "  84.21052631578948,\n",
       "  84.21052631578948,\n",
       "  84.21052631578948,\n",
       "  84.21052631578948,\n",
       "  84.21052631578948,\n",
       "  84.21052631578948,\n",
       "  84.21052631578948,\n",
       "  84.21052631578948,\n",
       "  84.21052631578948,\n",
       "  84.21052631578948,\n",
       "  84.21052631578948,\n",
       "  84.21052631578948,\n",
       "  84.21052631578948,\n",
       "  84.21052631578948,\n",
       "  84.21052631578948,\n",
       "  84.21052631578948,\n",
       "  84.21052631578948,\n",
       "  84.21052631578948,\n",
       "  84.21052631578948,\n",
       "  84.7953216374269,\n",
       "  84.7953216374269,\n",
       "  84.7953216374269,\n",
       "  84.7953216374269,\n",
       "  84.7953216374269,\n",
       "  84.7953216374269,\n",
       "  84.7953216374269,\n",
       "  84.7953216374269,\n",
       "  84.7953216374269,\n",
       "  84.7953216374269,\n",
       "  84.7953216374269,\n",
       "  84.7953216374269,\n",
       "  84.7953216374269,\n",
       "  84.7953216374269,\n",
       "  84.7953216374269,\n",
       "  84.7953216374269,\n",
       "  84.7953216374269,\n",
       "  84.7953216374269,\n",
       "  84.7953216374269,\n",
       "  84.7953216374269,\n",
       "  84.7953216374269,\n",
       "  84.7953216374269,\n",
       "  84.7953216374269,\n",
       "  84.7953216374269,\n",
       "  84.7953216374269,\n",
       "  84.7953216374269,\n",
       "  84.7953216374269,\n",
       "  85.38011695906432,\n",
       "  85.38011695906432,\n",
       "  85.38011695906432,\n",
       "  85.38011695906432,\n",
       "  85.38011695906432,\n",
       "  85.38011695906432,\n",
       "  85.38011695906432,\n",
       "  85.38011695906432,\n",
       "  85.38011695906432,\n",
       "  85.38011695906432,\n",
       "  85.38011695906432,\n",
       "  85.38011695906432,\n",
       "  85.38011695906432,\n",
       "  85.38011695906432,\n",
       "  85.38011695906432,\n",
       "  85.38011695906432,\n",
       "  85.38011695906432,\n",
       "  85.38011695906432,\n",
       "  85.38011695906432,\n",
       "  85.38011695906432,\n",
       "  85.38011695906432,\n",
       "  85.38011695906432,\n",
       "  85.38011695906432,\n",
       "  85.38011695906432,\n",
       "  85.38011695906432,\n",
       "  85.96491228070175,\n",
       "  85.96491228070175,\n",
       "  85.96491228070175,\n",
       "  85.96491228070175,\n",
       "  85.96491228070175,\n",
       "  85.96491228070175,\n",
       "  85.96491228070175,\n",
       "  85.96491228070175,\n",
       "  85.96491228070175,\n",
       "  85.96491228070175,\n",
       "  85.96491228070175,\n",
       "  85.96491228070175,\n",
       "  85.96491228070175,\n",
       "  85.96491228070175,\n",
       "  85.96491228070175,\n",
       "  86.54970760233918,\n",
       "  86.54970760233918,\n",
       "  86.54970760233918,\n",
       "  86.54970760233918,\n",
       "  86.54970760233918,\n",
       "  86.54970760233918,\n",
       "  86.54970760233918,\n",
       "  86.54970760233918,\n",
       "  86.54970760233918,\n",
       "  86.54970760233918,\n",
       "  86.54970760233918,\n",
       "  86.54970760233918,\n",
       "  86.54970760233918,\n",
       "  86.54970760233918,\n",
       "  86.54970760233918,\n",
       "  86.54970760233918,\n",
       "  86.54970760233918,\n",
       "  86.54970760233918,\n",
       "  86.54970760233918,\n",
       "  86.54970760233918,\n",
       "  87.13450292397661,\n",
       "  87.13450292397661,\n",
       "  87.13450292397661,\n",
       "  87.13450292397661,\n",
       "  87.13450292397661,\n",
       "  87.13450292397661,\n",
       "  87.13450292397661,\n",
       "  87.13450292397661,\n",
       "  87.13450292397661,\n",
       "  87.13450292397661,\n",
       "  87.13450292397661,\n",
       "  87.13450292397661,\n",
       "  87.13450292397661,\n",
       "  87.13450292397661,\n",
       "  87.13450292397661,\n",
       "  87.13450292397661,\n",
       "  87.13450292397661,\n",
       "  87.13450292397661,\n",
       "  87.13450292397661,\n",
       "  87.71929824561404,\n",
       "  87.71929824561404,\n",
       "  87.71929824561404,\n",
       "  87.71929824561404,\n",
       "  87.71929824561404,\n",
       "  87.71929824561404,\n",
       "  87.71929824561404,\n",
       "  87.71929824561404,\n",
       "  87.71929824561404,\n",
       "  87.71929824561404,\n",
       "  87.71929824561404,\n",
       "  87.71929824561404,\n",
       "  87.71929824561404,\n",
       "  87.71929824561404,\n",
       "  87.71929824561404,\n",
       "  87.71929824561404,\n",
       "  87.71929824561404,\n",
       "  87.71929824561404,\n",
       "  87.71929824561404,\n",
       "  87.71929824561404,\n",
       "  88.30409356725147,\n",
       "  88.30409356725147,\n",
       "  88.88888888888889,\n",
       "  88.88888888888889,\n",
       "  88.88888888888889,\n",
       "  88.88888888888889,\n",
       "  88.88888888888889,\n",
       "  88.88888888888889,\n",
       "  88.88888888888889,\n",
       "  88.88888888888889,\n",
       "  88.88888888888889,\n",
       "  88.88888888888889,\n",
       "  88.88888888888889,\n",
       "  88.88888888888889,\n",
       "  89.47368421052632,\n",
       "  89.47368421052632,\n",
       "  89.47368421052632,\n",
       "  90.05847953216374,\n",
       "  90.05847953216374,\n",
       "  90.05847953216374,\n",
       "  90.05847953216374,\n",
       "  90.05847953216374,\n",
       "  90.05847953216374,\n",
       "  90.05847953216374,\n",
       "  90.05847953216374,\n",
       "  90.05847953216374,\n",
       "  90.05847953216374,\n",
       "  90.05847953216374,\n",
       "  90.05847953216374,\n",
       "  90.05847953216374,\n",
       "  90.05847953216374,\n",
       "  90.05847953216374,\n",
       "  90.05847953216374,\n",
       "  90.05847953216374,\n",
       "  90.05847953216374,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  91.2280701754386,\n",
       "  91.2280701754386,\n",
       "  91.2280701754386,\n",
       "  91.2280701754386,\n",
       "  91.2280701754386,\n",
       "  91.2280701754386,\n",
       "  91.2280701754386,\n",
       "  91.2280701754386,\n",
       "  91.2280701754386,\n",
       "  91.2280701754386,\n",
       "  91.2280701754386,\n",
       "  91.2280701754386,\n",
       "  91.2280701754386,\n",
       "  91.2280701754386,\n",
       "  91.2280701754386,\n",
       "  91.2280701754386,\n",
       "  91.2280701754386,\n",
       "  91.2280701754386,\n",
       "  91.2280701754386,\n",
       "  91.2280701754386,\n",
       "  91.2280701754386,\n",
       "  91.2280701754386,\n",
       "  91.2280701754386,\n",
       "  91.2280701754386,\n",
       "  91.2280701754386]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2be5faf79681da6f2a61fdfdd5405d65d042280f7fba6178067603e3a2925119"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
