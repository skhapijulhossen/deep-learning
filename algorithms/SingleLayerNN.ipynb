{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps:\n",
    " - Define Structure\n",
    " - initialize parameters\n",
    " - Forward Propagation\n",
    "   $$Z^{[1]} =  W^{[1]} X + b^{[1]} $$ \n",
    "   $$A^{[1]} = \\tanh(Z^{[1]}) $$\n",
    "   $$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]} $$\n",
    "   $$\\hat{Y} = A^{[2]} = \\sigma(Z^{[2]}) $$\n",
    "\n",
    "\n",
    " - compute cost\n",
    "    - $$J = - \\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(} \\small y^{(i)}\\log\\left(\\hat{y}^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- \\hat{y}^{[2] (i)}\\right) \\large{)} \\small $$\n",
    " - Backward Propagation\n",
    "  ---------------------------------------------------------------------\n",
    "  $$dZ^{[2]} =  A^{[2]} - Y  $$ \n",
    "  $$dW^{[2]} = \\frac {1} {m} dZ^{[2]} A^{[1]T}  $$\n",
    "  $$db^{[2]} = \\frac {1} {m} \\sum\\limits_{i = 1}^{m} dZ^{[2]}  $$\n",
    "  ---------------------------------------------------------------------\n",
    "  $$dW^{[2]} = \\frac {1} {m} dZ^{[2]} A^{[1]T}  $$\n",
    "  $$dZ^{[2]} = W^{[2]} A^{[1]} + b^{[2]} $$\n",
    "  $$\\hat{Y} = A^{[2]} = \\sigma(Z^{[2]}) $$\n",
    " - update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(33)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Structure and initializing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameters_initializer(input_dim: int, hidden_dim: int, output_dim: int):\n",
    "    W1 = tf.random.normal([hidden_dim, input_dim], dtype=tf.float64)\n",
    "    b1 = tf.zeros([hidden_dim, 1], dtype=tf.float64)\n",
    "    W2 = tf.random.normal([output_dim, hidden_dim], dtype=tf.float64)\n",
    "    b2 = tf.zeros([output_dim, 1], dtype=tf.float64)\n",
    "    parameters = {\n",
    "        'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2\n",
    "    }\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = parameters_initializer(2, 4, 1)\n",
    "params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## forward propagation\n",
    "---------------------------------------\n",
    "   $$Z^{[1]} =  W^{[1]} X + b^{[1]} $$ \n",
    "   $$A^{[1]} = \\tanh(Z^{[1]}) $$\n",
    "   $$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]} $$\n",
    "   $$\\hat{Y} = A^{[2]} = \\sigma(Z^{[2]}) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X: tf.Tensor, params: dict[str, tf.Tensor]):\n",
    "    W1, b1, W2, b2 = params.values()\n",
    "    Z1 = tf.tensordot(W1, X, axes=1) + b1\n",
    "    A1 = tf.nn.tanh(Z1)\n",
    "    Z2 = tf.tensordot(W2, A1, axes=1) + b2\n",
    "    A2 = tf.nn.sigmoid(Z2)\n",
    "    state = {\n",
    "        'Z1': Z1, 'A1': A1, 'Z2': Z2, 'A2': A2\n",
    "    }\n",
    "    return state, A2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1., -2., -1.], [3., 0.5, -3.2]])\n",
    "Y = np.array([[1, 1, 0]])\n",
    "outs, A2 = forward_propagation(X, params)\n",
    "outs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(parameters: dict, state: dict, X: tf.Tensor, Y: tf.Tensor):\n",
    "    W1, b1, W2, b2 = parameters.values()\n",
    "    Z1, A1, Z2, A2 = state.values()\n",
    "    m = X.shape[1]\n",
    "    # Gradients\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1/m) * tf.tensordot(dZ2, tf.transpose(A1), axes=1)\n",
    "    db2 = (1/m) * tf.reduce_sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = tf.tensordot(tf.transpose(W2), dZ2, axes=1) * (1 - tf.pow(A1, 2))\n",
    "    dW1 = (1/m) * tf.tensordot(dZ1, tf.transpose(X), axes=1)\n",
    "    db1 = (1/m) * tf.reduce_sum(dZ1, axis=1, keepdims=True)\n",
    "    grads = {\n",
    "        'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2\n",
    "    }\n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, A2 = forward_propagation(X, params)\n",
    "back_propagation(params, state, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Parameters:\n",
    "$$ w1 = w1 - {\\alpha} * \\frac{\\partial J}{\\partial w1} $$\n",
    "$$ b1 = b1- {\\alpha}  * \\frac{\\partial J}{\\partial b1} $$\n",
    "$$ w2 = w2 - {\\alpha} * \\frac{\\partial J}{\\partial w2} $$\n",
    "$$ b2 = b2- {\\alpha}  * \\frac{\\partial J}{\\partial b2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_update(parameters: dict, grads: dict, learning_rate=0.9):\n",
    "    W1, b1, W2, b2 = parameters.values()\n",
    "    dW1, db1, dW2, db2 = grads.values()\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    return dict(W1=W1, b1=b1, W2=W2, b2=b2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, A2 = forward_propagation(X, params)\n",
    "grads = back_propagation(params, state, X, Y)\n",
    "state_update(params, grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleLayerNN:\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, learning_rate=0.09):\n",
    "        self.params = parameters_initializer(input_dim, hidden_dim, output_dim)\n",
    "        self.state = dict()\n",
    "        self.grads = dict()\n",
    "        self.costs = []\n",
    "        self.learning_rate = learning_rate\n",
    "        self.performace = []\n",
    "\n",
    "    def parameters_initializer(input_dim: int, hidden_dim: int, output_dim: int):\n",
    "        W1 = tf.random.normal([hidden_dim, input_dim], dtYpe=tf.float64)\n",
    "        b1 = tf.zeros([hidden_dim, 1], dtYpe=tf.float64)\n",
    "        W2 = tf.random.normal([output_dim, hidden_dim], dtYpe=tf.float64)\n",
    "        b2 = tf.zeros([output_dim, 1], dtYpe=tf.float64)\n",
    "        params = {\n",
    "            'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2\n",
    "        }\n",
    "        return params\n",
    "\n",
    "    def forward_propagation(self, X: tf.Tensor):\n",
    "        W1, b1, W2, b2 = self.params.values()\n",
    "        Z1 = tf.tensordot(W1, X, axes=1) + b1\n",
    "        A1 = tf.nn.tanh(Z1)\n",
    "        Z2 = tf.tensordot(W2, A1, axes=1) + b2\n",
    "        A2 = tf.nn.sigmoid(Z2)\n",
    "        state = {\n",
    "            'Z1': Z1, 'A1': A1, 'Z2': Z2, 'A2': A2\n",
    "        }\n",
    "        return state, A2\n",
    "\n",
    "    def back_propagation(self, X: tf.Tensor, Y: tf.Tensor):\n",
    "        W1, b1, W2, b2 = self.params.values()\n",
    "        Z1, A1, Z2, A2 = self.state.values()\n",
    "        m = X.shape[1]\n",
    "        # Gradients\n",
    "        dZ2 = A2 - Y\n",
    "        dW2 = (1/m) * tf.tensordot(dZ2, tf.transpose(A1), axes=1)\n",
    "        db2 = (1/m) * tf.reduce_sum(dZ2, axis=1, keepdims=True)\n",
    "        dZ1 = tf.tensordot(tf.transpose(W2), dZ2, axes=1) * (1 - tf.pow(A1, 2))\n",
    "        dW1 = (1/m) * tf.tensordot(dZ1, tf.transpose(X), axes=1)\n",
    "        db1 = (1/m) * tf.reduce_sum(dZ1, axis=1, keepdims=True)\n",
    "        grads = {\n",
    "            'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2\n",
    "        }\n",
    "        return grads\n",
    "\n",
    "    def state_update(self):\n",
    "        W1, b1, W2, b2 = self.params.values()\n",
    "        dW1, db1, dW2, db2 = self.grads.values()\n",
    "        W1 = W1 - self.learning_rate * dW1\n",
    "        b1 = b1 - self.learning_rate * db1\n",
    "        W2 = W2 - self.learning_rate * dW2\n",
    "        b2 = b2 - self.learning_rate * db2\n",
    "        return dict(W1=W1, b1=b1, W2=W2, b2=b2)\n",
    "\n",
    "    def compute_cost(self, Y: tf.Tensor, Yhat: tf.Tensor):\n",
    "        loss = tf.reduce_sum((Y * tf.math.log(Yhat)) +\n",
    "                             ((1-Y) * tf.math.log(1-Yhat)))\n",
    "        cost = (-1/Y.shape[0]) * loss\n",
    "        return float(tf.squeeze(cost))\n",
    "\n",
    "    def optimize(self, X: tf.Tensor, Y: tf.Tensor, validation: tuple, epochs=100):\n",
    "        m = X.shape[1]\n",
    "        for epoch in range(epochs):\n",
    "            self.state, A2 = self.forward_propagation(X)\n",
    "            self.grads = self.back_propagation(X, Y)\n",
    "            self.params = self.state_update()\n",
    "            cost = self.compute_cost(Y, A2)\n",
    "            self.costs.append(cost)\n",
    "            if epoch % 10 == 0:\n",
    "                y_train_pred = self.predict(X)\n",
    "                y_test_pred = self.predict(validation[0])\n",
    "                train_acc = 100 - np.mean(np.abs(y_train_pred - Y)) * 100\n",
    "                test_acc = 100 - \\\n",
    "                    np.mean(np.abs(y_test_pred - validation[1])) * 100\n",
    "                self.performace.append([train_acc, test_acc])\n",
    "                print(\n",
    "                    f\"epoch: {epoch} || Cost: {cost: .5f} || Train Accuracy: {train_acc: .2f}% || Test Accuracy: {test_acc:.2f}%\")\n",
    "        return np.array(self.performace)\n",
    "\n",
    "    def predict(self, X: tf.Tensor):\n",
    "        _, A2 = self.forward_propagation(X)\n",
    "        return tf.round(A2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 || Cost:  2.17551 || Train Accuracy:  66.67% || Test Accuracy: 100.00%\n",
      "epoch: 10 || Cost:  0.24836 || Train Accuracy:  100.00% || Test Accuracy: 100.00%\n",
      "epoch: 20 || Cost:  0.16112 || Train Accuracy:  100.00% || Test Accuracy: 100.00%\n",
      "epoch: 30 || Cost:  0.12418 || Train Accuracy:  100.00% || Test Accuracy: 100.00%\n",
      "epoch: 40 || Cost:  0.10342 || Train Accuracy:  100.00% || Test Accuracy: 100.00%\n",
      "epoch: 50 || Cost:  0.08990 || Train Accuracy:  100.00% || Test Accuracy: 100.00%\n",
      "epoch: 60 || Cost:  0.08024 || Train Accuracy:  100.00% || Test Accuracy: 100.00%\n",
      "epoch: 70 || Cost:  0.07290 || Train Accuracy:  100.00% || Test Accuracy: 100.00%\n",
      "epoch: 80 || Cost:  0.06708 || Train Accuracy:  100.00% || Test Accuracy: 100.00%\n",
      "epoch: 90 || Cost:  0.06232 || Train Accuracy:  100.00% || Test Accuracy: 100.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 66.66666667, 100.        ],\n",
       "       [100.        , 100.        ],\n",
       "       [100.        , 100.        ],\n",
       "       [100.        , 100.        ],\n",
       "       [100.        , 100.        ],\n",
       "       [100.        , 100.        ],\n",
       "       [100.        , 100.        ],\n",
       "       [100.        , 100.        ],\n",
       "       [100.        , 100.        ],\n",
       "       [100.        , 100.        ]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xv = np.array([[1., -1.8, -1.5], [3.1, 1.5, -3.2]])\n",
    "Yv = np.array([[1, 1, 0]])\n",
    "nn = SingleLayerNN(2, 4, 1)\n",
    "nn.optimize(X, Y,validation=(Xv,Yv), epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "x, y = datasets.load_breast_cancer(return_X_y=True)\n",
    "x = StandardScaler().fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(\n",
    "    x, y, random_state=33, test_size=0.3\n",
    ")\n",
    "xtrain = tf.Variable(xtrain.T, dtype=tf.float64)\n",
    "xtest = tf.Variable(xtest.T, dtype=tf.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 || Cost:  1.48300 || Train Accuracy:  22.11% || Test Accuracy: 19.88%\n",
      "epoch: 10 || Cost:  0.95314 || Train Accuracy:  61.56% || Test Accuracy: 64.33%\n",
      "epoch: 20 || Cost:  0.68582 || Train Accuracy:  68.84% || Test Accuracy: 74.85%\n",
      "epoch: 30 || Cost:  0.53996 || Train Accuracy:  73.62% || Test Accuracy: 74.85%\n",
      "epoch: 40 || Cost:  0.44934 || Train Accuracy:  76.38% || Test Accuracy: 77.78%\n",
      "epoch: 50 || Cost:  0.38827 || Train Accuracy:  79.90% || Test Accuracy: 83.04%\n",
      "epoch: 60 || Cost:  0.34442 || Train Accuracy:  85.43% || Test Accuracy: 87.13%\n",
      "epoch: 70 || Cost:  0.31139 || Train Accuracy:  86.43% || Test Accuracy: 87.72%\n",
      "epoch: 80 || Cost:  0.28564 || Train Accuracy:  87.19% || Test Accuracy: 87.72%\n",
      "epoch: 90 || Cost:  0.26483 || Train Accuracy:  88.94% || Test Accuracy: 89.47%\n",
      "epoch: 100 || Cost:  0.24732 || Train Accuracy:  89.70% || Test Accuracy: 90.64%\n",
      "epoch: 110 || Cost:  0.23176 || Train Accuracy:  90.70% || Test Accuracy: 91.23%\n",
      "epoch: 120 || Cost:  0.21712 || Train Accuracy:  91.71% || Test Accuracy: 92.40%\n",
      "epoch: 130 || Cost:  0.20299 || Train Accuracy:  92.96% || Test Accuracy: 92.40%\n",
      "epoch: 140 || Cost:  0.19022 || Train Accuracy:  93.22% || Test Accuracy: 92.40%\n",
      "epoch: 150 || Cost:  0.17931 || Train Accuracy:  93.72% || Test Accuracy: 92.40%\n",
      "epoch: 160 || Cost:  0.16993 || Train Accuracy:  93.72% || Test Accuracy: 92.40%\n",
      "epoch: 170 || Cost:  0.16175 || Train Accuracy:  94.22% || Test Accuracy: 92.40%\n",
      "epoch: 180 || Cost:  0.15455 || Train Accuracy:  94.72% || Test Accuracy: 92.40%\n",
      "epoch: 190 || Cost:  0.14816 || Train Accuracy:  95.23% || Test Accuracy: 92.98%\n",
      "epoch: 200 || Cost:  0.14245 || Train Accuracy:  95.23% || Test Accuracy: 93.57%\n",
      "epoch: 210 || Cost:  0.13734 || Train Accuracy:  95.98% || Test Accuracy: 94.15%\n",
      "epoch: 220 || Cost:  0.13275 || Train Accuracy:  96.23% || Test Accuracy: 94.15%\n",
      "epoch: 230 || Cost:  0.12859 || Train Accuracy:  96.23% || Test Accuracy: 94.15%\n",
      "epoch: 240 || Cost:  0.12480 || Train Accuracy:  96.23% || Test Accuracy: 94.15%\n",
      "epoch: 250 || Cost:  0.12131 || Train Accuracy:  96.23% || Test Accuracy: 94.74%\n",
      "epoch: 260 || Cost:  0.11807 || Train Accuracy:  96.48% || Test Accuracy: 95.32%\n",
      "epoch: 270 || Cost:  0.11503 || Train Accuracy:  96.73% || Test Accuracy: 95.32%\n",
      "epoch: 280 || Cost:  0.11217 || Train Accuracy:  96.98% || Test Accuracy: 95.32%\n",
      "epoch: 290 || Cost:  0.10947 || Train Accuracy:  97.49% || Test Accuracy: 95.32%\n",
      "epoch: 300 || Cost:  0.10693 || Train Accuracy:  97.49% || Test Accuracy: 95.32%\n",
      "epoch: 310 || Cost:  0.10457 || Train Accuracy:  97.49% || Test Accuracy: 95.32%\n",
      "epoch: 320 || Cost:  0.10240 || Train Accuracy:  97.49% || Test Accuracy: 95.32%\n",
      "epoch: 330 || Cost:  0.10041 || Train Accuracy:  97.49% || Test Accuracy: 95.32%\n",
      "epoch: 340 || Cost:  0.09857 || Train Accuracy:  97.74% || Test Accuracy: 95.91%\n",
      "epoch: 350 || Cost:  0.09687 || Train Accuracy:  97.49% || Test Accuracy: 95.91%\n",
      "epoch: 360 || Cost:  0.09527 || Train Accuracy:  97.49% || Test Accuracy: 95.91%\n",
      "epoch: 370 || Cost:  0.09378 || Train Accuracy:  97.49% || Test Accuracy: 95.91%\n",
      "epoch: 380 || Cost:  0.09236 || Train Accuracy:  97.49% || Test Accuracy: 95.91%\n",
      "epoch: 390 || Cost:  0.09101 || Train Accuracy:  97.49% || Test Accuracy: 95.91%\n",
      "epoch: 400 || Cost:  0.08972 || Train Accuracy:  97.49% || Test Accuracy: 95.91%\n",
      "epoch: 410 || Cost:  0.08848 || Train Accuracy:  97.49% || Test Accuracy: 95.91%\n",
      "epoch: 420 || Cost:  0.08729 || Train Accuracy:  97.49% || Test Accuracy: 95.91%\n",
      "epoch: 430 || Cost:  0.08613 || Train Accuracy:  97.49% || Test Accuracy: 95.91%\n",
      "epoch: 440 || Cost:  0.08500 || Train Accuracy:  97.49% || Test Accuracy: 95.91%\n",
      "epoch: 450 || Cost:  0.08389 || Train Accuracy:  97.49% || Test Accuracy: 95.91%\n",
      "epoch: 460 || Cost:  0.08281 || Train Accuracy:  97.49% || Test Accuracy: 95.91%\n",
      "epoch: 470 || Cost:  0.08175 || Train Accuracy:  97.49% || Test Accuracy: 95.91%\n",
      "epoch: 480 || Cost:  0.08073 || Train Accuracy:  97.49% || Test Accuracy: 95.91%\n",
      "epoch: 490 || Cost:  0.07973 || Train Accuracy:  97.49% || Test Accuracy: 95.91%\n",
      "epoch: 500 || Cost:  0.07877 || Train Accuracy:  97.49% || Test Accuracy: 95.91%\n",
      "epoch: 510 || Cost:  0.07783 || Train Accuracy:  97.49% || Test Accuracy: 95.91%\n",
      "epoch: 520 || Cost:  0.07693 || Train Accuracy:  97.49% || Test Accuracy: 95.91%\n",
      "epoch: 530 || Cost:  0.07605 || Train Accuracy:  97.49% || Test Accuracy: 95.91%\n",
      "epoch: 540 || Cost:  0.07519 || Train Accuracy:  97.74% || Test Accuracy: 95.91%\n",
      "epoch: 550 || Cost:  0.07435 || Train Accuracy:  97.74% || Test Accuracy: 95.91%\n",
      "epoch: 560 || Cost:  0.07353 || Train Accuracy:  97.74% || Test Accuracy: 95.91%\n",
      "epoch: 570 || Cost:  0.07273 || Train Accuracy:  97.74% || Test Accuracy: 95.91%\n",
      "epoch: 580 || Cost:  0.07194 || Train Accuracy:  97.74% || Test Accuracy: 95.91%\n",
      "epoch: 590 || Cost:  0.07118 || Train Accuracy:  97.74% || Test Accuracy: 95.91%\n",
      "epoch: 600 || Cost:  0.07043 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 610 || Cost:  0.06971 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 620 || Cost:  0.06900 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 630 || Cost:  0.06832 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 640 || Cost:  0.06766 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 650 || Cost:  0.06701 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 660 || Cost:  0.06639 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 670 || Cost:  0.06577 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 680 || Cost:  0.06517 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 690 || Cost:  0.06459 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 700 || Cost:  0.06401 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 710 || Cost:  0.06345 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 720 || Cost:  0.06289 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 730 || Cost:  0.06235 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 740 || Cost:  0.06183 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 750 || Cost:  0.06132 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 760 || Cost:  0.06083 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 770 || Cost:  0.06036 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 780 || Cost:  0.05991 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 790 || Cost:  0.05948 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 800 || Cost:  0.05907 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 810 || Cost:  0.05867 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 820 || Cost:  0.05828 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 830 || Cost:  0.05791 || Train Accuracy:  97.99% || Test Accuracy: 96.49%\n",
      "epoch: 840 || Cost:  0.05755 || Train Accuracy:  97.99% || Test Accuracy: 96.49%\n",
      "epoch: 850 || Cost:  0.05719 || Train Accuracy:  97.99% || Test Accuracy: 96.49%\n",
      "epoch: 860 || Cost:  0.05684 || Train Accuracy:  97.99% || Test Accuracy: 96.49%\n",
      "epoch: 870 || Cost:  0.05649 || Train Accuracy:  97.99% || Test Accuracy: 96.49%\n",
      "epoch: 880 || Cost:  0.05615 || Train Accuracy:  97.99% || Test Accuracy: 96.49%\n",
      "epoch: 890 || Cost:  0.05581 || Train Accuracy:  97.99% || Test Accuracy: 96.49%\n",
      "epoch: 900 || Cost:  0.05547 || Train Accuracy:  98.24% || Test Accuracy: 96.49%\n",
      "epoch: 910 || Cost:  0.05514 || Train Accuracy:  98.24% || Test Accuracy: 96.49%\n",
      "epoch: 920 || Cost:  0.05481 || Train Accuracy:  98.24% || Test Accuracy: 96.49%\n",
      "epoch: 930 || Cost:  0.05450 || Train Accuracy:  98.24% || Test Accuracy: 96.49%\n",
      "epoch: 940 || Cost:  0.05420 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 950 || Cost:  0.05392 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 960 || Cost:  0.05365 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 970 || Cost:  0.05338 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 980 || Cost:  0.05313 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 990 || Cost:  0.05288 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1000 || Cost:  0.05264 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1010 || Cost:  0.05240 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1020 || Cost:  0.05217 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1030 || Cost:  0.05195 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1040 || Cost:  0.05172 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1050 || Cost:  0.05151 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1060 || Cost:  0.05129 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1070 || Cost:  0.05108 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1080 || Cost:  0.05087 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1090 || Cost:  0.05067 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1100 || Cost:  0.05046 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1110 || Cost:  0.05026 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1120 || Cost:  0.05007 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1130 || Cost:  0.04987 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1140 || Cost:  0.04968 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1150 || Cost:  0.04949 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1160 || Cost:  0.04930 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1170 || Cost:  0.04911 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1180 || Cost:  0.04893 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1190 || Cost:  0.04875 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1200 || Cost:  0.04857 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1210 || Cost:  0.04839 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1220 || Cost:  0.04821 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1230 || Cost:  0.04804 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1240 || Cost:  0.04786 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1250 || Cost:  0.04769 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1260 || Cost:  0.04752 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1270 || Cost:  0.04735 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1280 || Cost:  0.04718 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1290 || Cost:  0.04701 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1300 || Cost:  0.04685 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1310 || Cost:  0.04668 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1320 || Cost:  0.04652 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1330 || Cost:  0.04636 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1340 || Cost:  0.04620 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1350 || Cost:  0.04604 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1360 || Cost:  0.04588 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1370 || Cost:  0.04572 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1380 || Cost:  0.04557 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1390 || Cost:  0.04541 || Train Accuracy:  98.74% || Test Accuracy: 96.49%\n",
      "epoch: 1400 || Cost:  0.04526 || Train Accuracy:  98.74% || Test Accuracy: 96.49%\n",
      "epoch: 1410 || Cost:  0.04510 || Train Accuracy:  98.74% || Test Accuracy: 96.49%\n",
      "epoch: 1420 || Cost:  0.04495 || Train Accuracy:  98.74% || Test Accuracy: 96.49%\n",
      "epoch: 1430 || Cost:  0.04480 || Train Accuracy:  98.74% || Test Accuracy: 96.49%\n",
      "epoch: 1440 || Cost:  0.04464 || Train Accuracy:  98.74% || Test Accuracy: 96.49%\n",
      "epoch: 1450 || Cost:  0.04449 || Train Accuracy:  98.74% || Test Accuracy: 96.49%\n",
      "epoch: 1460 || Cost:  0.04434 || Train Accuracy:  98.74% || Test Accuracy: 96.49%\n",
      "epoch: 1470 || Cost:  0.04419 || Train Accuracy:  98.74% || Test Accuracy: 96.49%\n",
      "epoch: 1480 || Cost:  0.04404 || Train Accuracy:  98.74% || Test Accuracy: 96.49%\n",
      "epoch: 1490 || Cost:  0.04389 || Train Accuracy:  98.74% || Test Accuracy: 96.49%\n",
      "epoch: 1500 || Cost:  0.04375 || Train Accuracy:  98.74% || Test Accuracy: 96.49%\n",
      "epoch: 1510 || Cost:  0.04360 || Train Accuracy:  98.74% || Test Accuracy: 96.49%\n",
      "epoch: 1520 || Cost:  0.04345 || Train Accuracy:  98.74% || Test Accuracy: 96.49%\n",
      "epoch: 1530 || Cost:  0.04330 || Train Accuracy:  98.74% || Test Accuracy: 96.49%\n",
      "epoch: 1540 || Cost:  0.04315 || Train Accuracy:  98.74% || Test Accuracy: 96.49%\n",
      "epoch: 1550 || Cost:  0.04301 || Train Accuracy:  98.74% || Test Accuracy: 96.49%\n",
      "epoch: 1560 || Cost:  0.04286 || Train Accuracy:  98.74% || Test Accuracy: 96.49%\n",
      "epoch: 1570 || Cost:  0.04271 || Train Accuracy:  98.74% || Test Accuracy: 96.49%\n",
      "epoch: 1580 || Cost:  0.04256 || Train Accuracy:  98.74% || Test Accuracy: 96.49%\n",
      "epoch: 1590 || Cost:  0.04241 || Train Accuracy:  98.74% || Test Accuracy: 96.49%\n",
      "epoch: 1600 || Cost:  0.04226 || Train Accuracy:  98.74% || Test Accuracy: 96.49%\n",
      "epoch: 1610 || Cost:  0.04211 || Train Accuracy:  98.74% || Test Accuracy: 96.49%\n",
      "epoch: 1620 || Cost:  0.04196 || Train Accuracy:  98.74% || Test Accuracy: 96.49%\n",
      "epoch: 1630 || Cost:  0.04180 || Train Accuracy:  98.74% || Test Accuracy: 96.49%\n",
      "epoch: 1640 || Cost:  0.04165 || Train Accuracy:  98.74% || Test Accuracy: 96.49%\n",
      "epoch: 1650 || Cost:  0.04149 || Train Accuracy:  98.74% || Test Accuracy: 96.49%\n",
      "epoch: 1660 || Cost:  0.04133 || Train Accuracy:  98.74% || Test Accuracy: 96.49%\n",
      "epoch: 1670 || Cost:  0.04116 || Train Accuracy:  98.74% || Test Accuracy: 96.49%\n",
      "epoch: 1680 || Cost:  0.04099 || Train Accuracy:  98.74% || Test Accuracy: 96.49%\n",
      "epoch: 1690 || Cost:  0.04082 || Train Accuracy:  98.74% || Test Accuracy: 96.49%\n",
      "epoch: 1700 || Cost:  0.04064 || Train Accuracy:  98.74% || Test Accuracy: 96.49%\n",
      "epoch: 1710 || Cost:  0.04045 || Train Accuracy:  98.74% || Test Accuracy: 97.08%\n",
      "epoch: 1720 || Cost:  0.04025 || Train Accuracy:  98.74% || Test Accuracy: 97.08%\n",
      "epoch: 1730 || Cost:  0.04004 || Train Accuracy:  98.74% || Test Accuracy: 97.08%\n",
      "epoch: 1740 || Cost:  0.03982 || Train Accuracy:  98.74% || Test Accuracy: 97.08%\n",
      "epoch: 1750 || Cost:  0.03958 || Train Accuracy:  98.74% || Test Accuracy: 97.08%\n",
      "epoch: 1760 || Cost:  0.03933 || Train Accuracy:  98.74% || Test Accuracy: 97.08%\n",
      "epoch: 1770 || Cost:  0.03908 || Train Accuracy:  98.74% || Test Accuracy: 97.08%\n",
      "epoch: 1780 || Cost:  0.03883 || Train Accuracy:  98.74% || Test Accuracy: 97.08%\n",
      "epoch: 1790 || Cost:  0.03859 || Train Accuracy:  98.74% || Test Accuracy: 97.08%\n",
      "epoch: 1800 || Cost:  0.03836 || Train Accuracy:  98.74% || Test Accuracy: 97.08%\n",
      "epoch: 1810 || Cost:  0.03814 || Train Accuracy:  98.74% || Test Accuracy: 97.08%\n",
      "epoch: 1820 || Cost:  0.03794 || Train Accuracy:  98.74% || Test Accuracy: 97.08%\n",
      "epoch: 1830 || Cost:  0.03774 || Train Accuracy:  98.74% || Test Accuracy: 97.08%\n",
      "epoch: 1840 || Cost:  0.03756 || Train Accuracy:  98.74% || Test Accuracy: 97.08%\n",
      "epoch: 1850 || Cost:  0.03738 || Train Accuracy:  98.74% || Test Accuracy: 97.08%\n",
      "epoch: 1860 || Cost:  0.03720 || Train Accuracy:  98.74% || Test Accuracy: 97.08%\n",
      "epoch: 1870 || Cost:  0.03703 || Train Accuracy:  98.74% || Test Accuracy: 97.08%\n",
      "epoch: 1880 || Cost:  0.03687 || Train Accuracy:  98.74% || Test Accuracy: 97.08%\n",
      "epoch: 1890 || Cost:  0.03671 || Train Accuracy:  98.74% || Test Accuracy: 97.08%\n",
      "epoch: 1900 || Cost:  0.03656 || Train Accuracy:  98.99% || Test Accuracy: 97.66%\n",
      "epoch: 1910 || Cost:  0.03640 || Train Accuracy:  98.99% || Test Accuracy: 97.66%\n",
      "epoch: 1920 || Cost:  0.03625 || Train Accuracy:  98.99% || Test Accuracy: 97.66%\n",
      "epoch: 1930 || Cost:  0.03610 || Train Accuracy:  98.99% || Test Accuracy: 97.66%\n",
      "epoch: 1940 || Cost:  0.03596 || Train Accuracy:  98.99% || Test Accuracy: 97.66%\n",
      "epoch: 1950 || Cost:  0.03582 || Train Accuracy:  98.99% || Test Accuracy: 97.66%\n",
      "epoch: 1960 || Cost:  0.03568 || Train Accuracy:  98.99% || Test Accuracy: 97.66%\n",
      "epoch: 1970 || Cost:  0.03554 || Train Accuracy:  98.99% || Test Accuracy: 97.66%\n",
      "epoch: 1980 || Cost:  0.03540 || Train Accuracy:  98.99% || Test Accuracy: 97.66%\n",
      "epoch: 1990 || Cost:  0.03526 || Train Accuracy:  98.99% || Test Accuracy: 97.66%\n"
     ]
    }
   ],
   "source": [
    "nn2 = SingleLayerNN(30, 4, 1)\n",
    "acc = nn2.optimize(xtrain, ytrain,validation=(xtest, ytest), epochs=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(acc[:, 0], label='Train Accuracy')\n",
    "plt.plot(acc[:, 1], label='Test Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2be5faf79681da6f2a61fdfdd5405d65d042280f7fba6178067603e3a2925119"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
