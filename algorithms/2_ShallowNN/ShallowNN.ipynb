{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps:\n",
    " - Define Structure\n",
    " - initialize parameters\n",
    " - Forward Propagation\n",
    "   $$Z^{[1]} =  W^{[1]} X + b^{[1]} $$ \n",
    "   $$A^{[1]} = \\tanh(Z^{[1]}) $$\n",
    "   $$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]} $$\n",
    "   $$\\hat{Y} = A^{[2]} = \\sigma(Z^{[2]}) $$\n",
    "\n",
    "\n",
    " - compute cost\n",
    "    - $$J = - \\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(} \\small y^{(i)}\\log\\left(\\hat{y}^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- \\hat{y}^{[2] (i)}\\right) \\large{)} \\small $$\n",
    " - Backward Propagation\n",
    "  ---------------------------------------------------------------------\n",
    "  $$dZ^{[2]} =  A^{[2]} - Y  $$ \n",
    "  $$dW^{[2]} = \\frac {1} {m} dZ^{[2]} A^{[1]T}  $$\n",
    "  $$db^{[2]} = \\frac {1} {m} \\sum\\limits_{i = 1}^{m} dZ^{[2]}  $$\n",
    "  ---------------------------------------------------------------------\n",
    "  $$dW^{[2]} = \\frac {1} {m} dZ^{[2]} A^{[1]T}  $$\n",
    "  $$dZ^{[2]} = W^{[2]} A^{[1]} + b^{[2]} $$\n",
    "  $$\\hat{Y} = A^{[2]} = \\sigma(Z^{[2]}) $$\n",
    " - update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(33)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Structure and initializing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameters_initializer(input_dim: int, hidden_dim: int, output_dim: int):\n",
    "    W1 = tf.random.normal([hidden_dim, input_dim], dtype=tf.float64)\n",
    "    b1 = tf.zeros([hidden_dim, 1], dtype=tf.float64)\n",
    "    W2 = tf.random.normal([output_dim, hidden_dim], dtype=tf.float64)\n",
    "    b2 = tf.zeros([output_dim, 1], dtype=tf.float64)\n",
    "    parameters = {\n",
    "        'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2\n",
    "    }\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': <tf.Tensor: shape=(4, 2), dtype=float64, numpy=\n",
       " array([[ 0.66724593,  0.6416387 ],\n",
       "        [ 1.10669007,  0.842583  ],\n",
       "        [ 1.7161246 , -2.81539924],\n",
       "        [-0.49222419, -0.53460197]])>,\n",
       " 'b1': <tf.Tensor: shape=(4, 1), dtype=float64, numpy=\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]])>,\n",
       " 'W2': <tf.Tensor: shape=(1, 4), dtype=float64, numpy=array([[-0.97562285,  1.54872505,  1.79938454, -0.41321412]])>,\n",
       " 'b2': <tf.Tensor: shape=(1, 1), dtype=float64, numpy=array([[0.]])>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = parameters_initializer(2, 4, 1)\n",
    "params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## forward propagation\n",
    "---------------------------------------\n",
    "   $$Z^{[1]} =  W^{[1]} X + b^{[1]} $$ \n",
    "   $$A^{[1]} = \\tanh(Z^{[1]}) $$\n",
    "   $$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]} $$\n",
    "   $$\\hat{Y} = A^{[2]} = \\sigma(Z^{[2]}) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X: tf.Tensor, params: dict[str, tf.Tensor]):\n",
    "    W1, b1, W2, b2 = params.values()\n",
    "    Z1 = tf.tensordot(W1, X, axes=1) + b1\n",
    "    A1 = tf.nn.tanh(Z1)\n",
    "    Z2 = tf.tensordot(W2, A1, axes=1) + b2\n",
    "    A2 = tf.nn.sigmoid(Z2)\n",
    "    state = {\n",
    "        'Z1': Z1, 'A1': A1, 'Z2': Z2, 'A2': A2\n",
    "    }\n",
    "    return state, A2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Z1': <tf.Tensor: shape=(4, 3), dtype=float64, numpy=\n",
       " array([[ 2.59216203, -1.01367251, -2.72048977],\n",
       "        [ 3.63443905, -1.79208863, -3.80295565],\n",
       "        [-6.73007312, -4.83994881,  7.29315297],\n",
       "        [-2.09603009,  0.7171474 ,  2.20295048]])>,\n",
       " 'A1': <tf.Tensor: shape=(4, 3), dtype=float64, numpy=\n",
       " array([[ 0.98885501, -0.76727673, -0.99136696],\n",
       "        [ 0.99860718, -0.94598056, -0.99900549],\n",
       "        [-0.99999715, -0.99987495,  0.99999907],\n",
       "        [-0.9702199 ,  0.61513923,  0.97588413]])>,\n",
       " 'Z2': <tf.Tensor: shape=(1, 3), dtype=float64, numpy=array([[-0.81665242, -2.76983482,  0.8161492 ]])>,\n",
       " 'A2': <tf.Tensor: shape=(1, 3), dtype=float64, numpy=array([[0.30647472, 0.05897618, 0.69341831]])>}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[1., -2., -1.], [3., 0.5, -3.2]])\n",
    "Y = np.array([[1, 1, 0]])\n",
    "outs, A2 = forward_propagation(X, params)\n",
    "outs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(parameters: dict, state: dict, X: tf.Tensor, Y: tf.Tensor):\n",
    "    W1, b1, W2, b2 = parameters.values()\n",
    "    Z1, A1, Z2, A2 = state.values()\n",
    "    m = X.shape[1]\n",
    "    # Gradients\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1/m) * tf.tensordot(dZ2, tf.transpose(A1), axes=1)\n",
    "    db2 = (1/m) * tf.reduce_sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = tf.tensordot(tf.transpose(W2), dZ2, axes=1) * (1 - tf.pow(A1, 2))\n",
    "    dW1 = (1/m) * tf.tensordot(dZ1, tf.transpose(X), axes=1)\n",
    "    db1 = (1/m) * tf.reduce_sum(dZ1, axis=1, keepdims=True)\n",
    "    grads = {\n",
    "        'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2\n",
    "    }\n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dW1': <tf.Tensor: shape=(4, 2), dtype=float64, numpy=\n",
       " array([[-2.42854369e-01,  9.03360873e-02],\n",
       "        [ 1.00426159e-01, -3.08008253e-02],\n",
       "        [ 2.79157775e-04, -8.01598812e-05],\n",
       "        [-1.50982225e-01,  7.16622224e-02]])>,\n",
       " 'db1': <tf.Tensor: shape=(4, 1), dtype=float64, numpy=\n",
       " array([[ 0.12698769],\n",
       "        [-0.0513522 ],\n",
       "        [-0.00014275],\n",
       "        [ 0.08162272]])>,\n",
       " 'dW2': <tf.Tensor: shape=(1, 4), dtype=float64, numpy=array([[-0.21706742, -0.1650326 ,  0.77594904,  0.25690243]])>,\n",
       " 'db2': <tf.Tensor: shape=(1, 1), dtype=float64, numpy=array([[-0.31371026]])>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state, A2 = forward_propagation(X, params)\n",
    "back_propagation(params, state, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Parameters:\n",
    "$$ w1 = w1 - {\\alpha} * \\frac{\\partial J}{\\partial w1} $$\n",
    "$$ b1 = b1- {\\alpha}  * \\frac{\\partial J}{\\partial b1} $$\n",
    "$$ w2 = w2 - {\\alpha} * \\frac{\\partial J}{\\partial w2} $$\n",
    "$$ b2 = b2- {\\alpha}  * \\frac{\\partial J}{\\partial b2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_update(parameters: dict, grads: dict, learning_rate=0.9):\n",
    "    W1, b1, W2, b2 = parameters.values()\n",
    "    dW1, db1, dW2, db2 = grads.values()\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    return dict(W1=W1, b1=b1, W2=W2, b2=b2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': <tf.Tensor: shape=(4, 2), dtype=float64, numpy=\n",
       " array([[ 0.88581486,  0.56033622],\n",
       "        [ 1.01630652,  0.87030374],\n",
       "        [ 1.71587335, -2.8153271 ],\n",
       "        [-0.35634019, -0.59909797]])>,\n",
       " 'b1': <tf.Tensor: shape=(4, 1), dtype=float64, numpy=\n",
       " array([[-0.11428892],\n",
       "        [ 0.04621698],\n",
       "        [ 0.00012848],\n",
       "        [-0.07346044]])>,\n",
       " 'W2': <tf.Tensor: shape=(1, 4), dtype=float64, numpy=array([[-0.78026217,  1.69725438,  1.1010304 , -0.6444263 ]])>,\n",
       " 'b2': <tf.Tensor: shape=(1, 1), dtype=float64, numpy=array([[0.28233924]])>}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state, A2 = forward_propagation(X, params)\n",
    "grads = back_propagation(params, state, X, Y)\n",
    "state_update(params, grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleLayerNN:\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, learning_rate=0.09):\n",
    "        self.params = parameters_initializer(input_dim, hidden_dim, output_dim)\n",
    "        self.state = dict()\n",
    "        self.grads = dict()\n",
    "        self.costs = []\n",
    "        self.learning_rate = learning_rate\n",
    "        self.performace = []\n",
    "\n",
    "    def parameters_initializer(input_dim: int, hidden_dim: int, output_dim: int):\n",
    "        W1 = tf.random.normal([hidden_dim, input_dim], dtYpe=tf.float64)\n",
    "        b1 = tf.zeros([hidden_dim, 1], dtYpe=tf.float64)\n",
    "        W2 = tf.random.normal([output_dim, hidden_dim], dtYpe=tf.float64)\n",
    "        b2 = tf.zeros([output_dim, 1], dtYpe=tf.float64)\n",
    "        params = {\n",
    "            'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2\n",
    "        }\n",
    "        return params\n",
    "\n",
    "    def forward_propagation(self, X: tf.Tensor):\n",
    "        W1, b1, W2, b2 = self.params.values()\n",
    "        Z1 = tf.tensordot(W1, X, axes=1) + b1\n",
    "        A1 = tf.nn.tanh(Z1)\n",
    "        Z2 = tf.tensordot(W2, A1, axes=1) + b2\n",
    "        A2 = tf.nn.sigmoid(Z2)\n",
    "        state = {\n",
    "            'Z1': Z1, 'A1': A1, 'Z2': Z2, 'A2': A2\n",
    "        }\n",
    "        return state, A2\n",
    "\n",
    "    def back_propagation(self, X: tf.Tensor, Y: tf.Tensor):\n",
    "        W1, b1, W2, b2 = self.params.values()\n",
    "        Z1, A1, Z2, A2 = self.state.values()\n",
    "        m = X.shape[1]\n",
    "        # Gradients\n",
    "        dZ2 = A2 - Y\n",
    "        dW2 = (1/m) * tf.tensordot(dZ2, tf.transpose(A1), axes=1)\n",
    "        db2 = (1/m) * tf.reduce_sum(dZ2, axis=1, keepdims=True)\n",
    "        dZ1 = tf.tensordot(tf.transpose(W2), dZ2, axes=1) * (1 - tf.pow(A1, 2))\n",
    "        dW1 = (1/m) * tf.tensordot(dZ1, tf.transpose(X), axes=1)\n",
    "        db1 = (1/m) * tf.reduce_sum(dZ1, axis=1, keepdims=True)\n",
    "        grads = {\n",
    "            'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2\n",
    "        }\n",
    "        return grads\n",
    "\n",
    "    def state_update(self):\n",
    "        W1, b1, W2, b2 = self.params.values()\n",
    "        dW1, db1, dW2, db2 = self.grads.values()\n",
    "        W1 = W1 - self.learning_rate * dW1\n",
    "        b1 = b1 - self.learning_rate * db1\n",
    "        W2 = W2 - self.learning_rate * dW2\n",
    "        b2 = b2 - self.learning_rate * db2\n",
    "        return dict(W1=W1, b1=b1, W2=W2, b2=b2)\n",
    "\n",
    "    def compute_cost(self, Y: tf.Tensor, Yhat: tf.Tensor):\n",
    "        loss = tf.reduce_sum((Y * tf.math.log(Yhat)) +\n",
    "                             ((1-Y) * tf.math.log(1-Yhat)))\n",
    "        cost = (-1/Y.shape[0]) * loss\n",
    "        return float(tf.squeeze(cost))\n",
    "\n",
    "    def optimize(self, X: tf.Tensor, Y: tf.Tensor, validation: tuple, epochs=100):\n",
    "        m = X.shape[1]\n",
    "        for epoch in range(epochs):\n",
    "            self.state, A2 = self.forward_propagation(X)\n",
    "            self.grads = self.back_propagation(X, Y)\n",
    "            self.params = self.state_update()\n",
    "            cost = self.compute_cost(Y, A2)\n",
    "            self.costs.append(cost)\n",
    "            if epoch % 10 == 0:\n",
    "                y_train_pred = self.predict(X)\n",
    "                y_test_pred = self.predict(validation[0])\n",
    "                train_acc = 100 - np.mean(np.abs(y_train_pred - Y)) * 100\n",
    "                test_acc = 100 - \\\n",
    "                    np.mean(np.abs(y_test_pred - validation[1])) * 100\n",
    "                self.performace.append([train_acc, test_acc])\n",
    "                print(\n",
    "                    f\"epoch: {epoch} || Cost: {cost: .5f} || Train Accuracy: {train_acc: .2f}% || Test Accuracy: {test_acc:.2f}%\")\n",
    "        return np.array(self.performace)\n",
    "\n",
    "    def predict(self, X: tf.Tensor):\n",
    "        _, A2 = self.forward_propagation(X)\n",
    "        return tf.round(A2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 || Cost:  2.04711 || Train Accuracy:  33.33% || Test Accuracy: 33.33%\n",
      "epoch: 10 || Cost:  0.77332 || Train Accuracy:  100.00% || Test Accuracy: 100.00%\n",
      "epoch: 20 || Cost:  0.47288 || Train Accuracy:  100.00% || Test Accuracy: 100.00%\n",
      "epoch: 30 || Cost:  0.33659 || Train Accuracy:  100.00% || Test Accuracy: 100.00%\n",
      "epoch: 40 || Cost:  0.25984 || Train Accuracy:  100.00% || Test Accuracy: 100.00%\n",
      "epoch: 50 || Cost:  0.21099 || Train Accuracy:  100.00% || Test Accuracy: 100.00%\n",
      "epoch: 60 || Cost:  0.17731 || Train Accuracy:  100.00% || Test Accuracy: 100.00%\n",
      "epoch: 70 || Cost:  0.15276 || Train Accuracy:  100.00% || Test Accuracy: 100.00%\n",
      "epoch: 80 || Cost:  0.13409 || Train Accuracy:  100.00% || Test Accuracy: 100.00%\n",
      "epoch: 90 || Cost:  0.11943 || Train Accuracy:  100.00% || Test Accuracy: 100.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 33.33333333,  33.33333333],\n",
       "       [100.        , 100.        ],\n",
       "       [100.        , 100.        ],\n",
       "       [100.        , 100.        ],\n",
       "       [100.        , 100.        ],\n",
       "       [100.        , 100.        ],\n",
       "       [100.        , 100.        ],\n",
       "       [100.        , 100.        ],\n",
       "       [100.        , 100.        ],\n",
       "       [100.        , 100.        ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xv = np.array([[1., -1.8, -1.5], [3.1, 1.5, -3.2]])\n",
    "Yv = np.array([[1, 1, 0]])\n",
    "nn = SingleLayerNN(2, 4, 1)\n",
    "nn.optimize(X, Y,validation=(Xv,Yv), epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "x, y = datasets.load_breast_cancer(return_X_y=True)\n",
    "x = StandardScaler().fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(\n",
    "    x, y, random_state=33, test_size=0.3\n",
    ")\n",
    "xtrain = tf.Variable(xtrain.T, dtype=tf.float64)\n",
    "xtest = tf.Variable(xtest.T, dtype=tf.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 || Cost:  1.86479 || Train Accuracy:  35.68% || Test Accuracy: 25.73%\n",
      "epoch: 10 || Cost:  0.76683 || Train Accuracy:  63.82% || Test Accuracy: 58.48%\n",
      "epoch: 20 || Cost:  0.40012 || Train Accuracy:  78.89% || Test Accuracy: 77.19%\n",
      "epoch: 30 || Cost:  0.29575 || Train Accuracy:  84.67% || Test Accuracy: 84.21%\n",
      "epoch: 40 || Cost:  0.24896 || Train Accuracy:  87.69% || Test Accuracy: 88.30%\n",
      "epoch: 50 || Cost:  0.21486 || Train Accuracy:  90.20% || Test Accuracy: 89.47%\n",
      "epoch: 60 || Cost:  0.19451 || Train Accuracy:  91.21% || Test Accuracy: 90.64%\n",
      "epoch: 70 || Cost:  0.18173 || Train Accuracy:  91.46% || Test Accuracy: 90.64%\n",
      "epoch: 80 || Cost:  0.17232 || Train Accuracy:  91.96% || Test Accuracy: 90.64%\n",
      "epoch: 90 || Cost:  0.16486 || Train Accuracy:  92.71% || Test Accuracy: 90.64%\n",
      "epoch: 100 || Cost:  0.15859 || Train Accuracy:  92.96% || Test Accuracy: 91.23%\n",
      "epoch: 110 || Cost:  0.15311 || Train Accuracy:  93.72% || Test Accuracy: 91.23%\n",
      "epoch: 120 || Cost:  0.14821 || Train Accuracy:  93.97% || Test Accuracy: 91.81%\n",
      "epoch: 130 || Cost:  0.14382 || Train Accuracy:  94.22% || Test Accuracy: 91.81%\n",
      "epoch: 140 || Cost:  0.13993 || Train Accuracy:  94.47% || Test Accuracy: 91.81%\n",
      "epoch: 150 || Cost:  0.13651 || Train Accuracy:  94.72% || Test Accuracy: 91.81%\n",
      "epoch: 160 || Cost:  0.13349 || Train Accuracy:  94.97% || Test Accuracy: 92.40%\n",
      "epoch: 170 || Cost:  0.13080 || Train Accuracy:  95.23% || Test Accuracy: 92.98%\n",
      "epoch: 180 || Cost:  0.12840 || Train Accuracy:  95.23% || Test Accuracy: 92.98%\n",
      "epoch: 190 || Cost:  0.12623 || Train Accuracy:  95.23% || Test Accuracy: 93.57%\n",
      "epoch: 200 || Cost:  0.12426 || Train Accuracy:  95.23% || Test Accuracy: 93.57%\n",
      "epoch: 210 || Cost:  0.12244 || Train Accuracy:  95.48% || Test Accuracy: 93.57%\n",
      "epoch: 220 || Cost:  0.12076 || Train Accuracy:  95.48% || Test Accuracy: 93.57%\n",
      "epoch: 230 || Cost:  0.11919 || Train Accuracy:  95.48% || Test Accuracy: 93.57%\n",
      "epoch: 240 || Cost:  0.11770 || Train Accuracy:  95.48% || Test Accuracy: 94.15%\n",
      "epoch: 250 || Cost:  0.11628 || Train Accuracy:  95.73% || Test Accuracy: 94.15%\n",
      "epoch: 260 || Cost:  0.11491 || Train Accuracy:  95.73% || Test Accuracy: 94.74%\n",
      "epoch: 270 || Cost:  0.11356 || Train Accuracy:  95.73% || Test Accuracy: 94.74%\n",
      "epoch: 280 || Cost:  0.11223 || Train Accuracy:  95.73% || Test Accuracy: 94.74%\n",
      "epoch: 290 || Cost:  0.11087 || Train Accuracy:  95.73% || Test Accuracy: 94.74%\n",
      "epoch: 300 || Cost:  0.10947 || Train Accuracy:  95.98% || Test Accuracy: 94.74%\n",
      "epoch: 310 || Cost:  0.10797 || Train Accuracy:  95.98% || Test Accuracy: 94.74%\n",
      "epoch: 320 || Cost:  0.10635 || Train Accuracy:  95.98% || Test Accuracy: 94.74%\n",
      "epoch: 330 || Cost:  0.10456 || Train Accuracy:  95.98% || Test Accuracy: 95.32%\n",
      "epoch: 340 || Cost:  0.10265 || Train Accuracy:  95.98% || Test Accuracy: 95.32%\n",
      "epoch: 350 || Cost:  0.10075 || Train Accuracy:  96.23% || Test Accuracy: 94.74%\n",
      "epoch: 360 || Cost:  0.09904 || Train Accuracy:  96.48% || Test Accuracy: 94.74%\n",
      "epoch: 370 || Cost:  0.09756 || Train Accuracy:  96.48% || Test Accuracy: 94.74%\n",
      "epoch: 380 || Cost:  0.09624 || Train Accuracy:  96.48% || Test Accuracy: 94.74%\n",
      "epoch: 390 || Cost:  0.09502 || Train Accuracy:  96.73% || Test Accuracy: 94.15%\n",
      "epoch: 400 || Cost:  0.09381 || Train Accuracy:  96.73% || Test Accuracy: 94.15%\n",
      "epoch: 410 || Cost:  0.09260 || Train Accuracy:  96.73% || Test Accuracy: 94.74%\n",
      "epoch: 420 || Cost:  0.09149 || Train Accuracy:  96.73% || Test Accuracy: 94.74%\n",
      "epoch: 430 || Cost:  0.09051 || Train Accuracy:  96.73% || Test Accuracy: 95.32%\n",
      "epoch: 440 || Cost:  0.08964 || Train Accuracy:  96.73% || Test Accuracy: 95.32%\n",
      "epoch: 450 || Cost:  0.08882 || Train Accuracy:  96.73% || Test Accuracy: 95.32%\n",
      "epoch: 460 || Cost:  0.08805 || Train Accuracy:  96.73% || Test Accuracy: 95.32%\n",
      "epoch: 470 || Cost:  0.08730 || Train Accuracy:  96.73% || Test Accuracy: 95.32%\n",
      "epoch: 480 || Cost:  0.08659 || Train Accuracy:  96.73% || Test Accuracy: 95.32%\n",
      "epoch: 490 || Cost:  0.08589 || Train Accuracy:  96.73% || Test Accuracy: 95.32%\n",
      "epoch: 500 || Cost:  0.08522 || Train Accuracy:  96.73% || Test Accuracy: 95.32%\n",
      "epoch: 510 || Cost:  0.08457 || Train Accuracy:  96.73% || Test Accuracy: 95.32%\n",
      "epoch: 520 || Cost:  0.08393 || Train Accuracy:  96.73% || Test Accuracy: 95.32%\n",
      "epoch: 530 || Cost:  0.08331 || Train Accuracy:  96.73% || Test Accuracy: 95.32%\n",
      "epoch: 540 || Cost:  0.08271 || Train Accuracy:  96.73% || Test Accuracy: 95.32%\n",
      "epoch: 550 || Cost:  0.08213 || Train Accuracy:  96.73% || Test Accuracy: 95.32%\n",
      "epoch: 560 || Cost:  0.08156 || Train Accuracy:  96.73% || Test Accuracy: 95.32%\n",
      "epoch: 570 || Cost:  0.08101 || Train Accuracy:  96.73% || Test Accuracy: 95.32%\n",
      "epoch: 580 || Cost:  0.08047 || Train Accuracy:  96.73% || Test Accuracy: 95.32%\n",
      "epoch: 590 || Cost:  0.07995 || Train Accuracy:  96.73% || Test Accuracy: 95.32%\n",
      "epoch: 600 || Cost:  0.07944 || Train Accuracy:  96.73% || Test Accuracy: 95.32%\n",
      "epoch: 610 || Cost:  0.07894 || Train Accuracy:  96.73% || Test Accuracy: 95.32%\n",
      "epoch: 620 || Cost:  0.07845 || Train Accuracy:  96.98% || Test Accuracy: 95.32%\n",
      "epoch: 630 || Cost:  0.07797 || Train Accuracy:  96.98% || Test Accuracy: 95.32%\n",
      "epoch: 640 || Cost:  0.07751 || Train Accuracy:  96.98% || Test Accuracy: 95.32%\n",
      "epoch: 650 || Cost:  0.07705 || Train Accuracy:  96.98% || Test Accuracy: 95.32%\n",
      "epoch: 660 || Cost:  0.07660 || Train Accuracy:  96.98% || Test Accuracy: 95.32%\n",
      "epoch: 670 || Cost:  0.07617 || Train Accuracy:  96.98% || Test Accuracy: 95.32%\n",
      "epoch: 680 || Cost:  0.07573 || Train Accuracy:  96.98% || Test Accuracy: 95.32%\n",
      "epoch: 690 || Cost:  0.07531 || Train Accuracy:  97.24% || Test Accuracy: 95.32%\n",
      "epoch: 700 || Cost:  0.07489 || Train Accuracy:  97.49% || Test Accuracy: 95.32%\n",
      "epoch: 710 || Cost:  0.07448 || Train Accuracy:  97.49% || Test Accuracy: 95.32%\n",
      "epoch: 720 || Cost:  0.07408 || Train Accuracy:  97.49% || Test Accuracy: 95.32%\n",
      "epoch: 730 || Cost:  0.07368 || Train Accuracy:  97.49% || Test Accuracy: 95.32%\n",
      "epoch: 740 || Cost:  0.07328 || Train Accuracy:  97.49% || Test Accuracy: 95.32%\n",
      "epoch: 750 || Cost:  0.07289 || Train Accuracy:  97.49% || Test Accuracy: 95.32%\n",
      "epoch: 760 || Cost:  0.07250 || Train Accuracy:  97.49% || Test Accuracy: 95.32%\n",
      "epoch: 770 || Cost:  0.07211 || Train Accuracy:  97.49% || Test Accuracy: 95.32%\n",
      "epoch: 780 || Cost:  0.07172 || Train Accuracy:  97.49% || Test Accuracy: 95.32%\n",
      "epoch: 790 || Cost:  0.07133 || Train Accuracy:  97.49% || Test Accuracy: 95.32%\n",
      "epoch: 800 || Cost:  0.07094 || Train Accuracy:  97.49% || Test Accuracy: 95.32%\n",
      "epoch: 810 || Cost:  0.07055 || Train Accuracy:  97.49% || Test Accuracy: 95.32%\n",
      "epoch: 820 || Cost:  0.07015 || Train Accuracy:  97.49% || Test Accuracy: 95.32%\n",
      "epoch: 830 || Cost:  0.06976 || Train Accuracy:  97.49% || Test Accuracy: 95.32%\n",
      "epoch: 840 || Cost:  0.06936 || Train Accuracy:  97.49% || Test Accuracy: 95.32%\n",
      "epoch: 850 || Cost:  0.06896 || Train Accuracy:  97.49% || Test Accuracy: 95.32%\n",
      "epoch: 860 || Cost:  0.06855 || Train Accuracy:  97.49% || Test Accuracy: 95.32%\n",
      "epoch: 870 || Cost:  0.06815 || Train Accuracy:  97.74% || Test Accuracy: 95.32%\n",
      "epoch: 880 || Cost:  0.06774 || Train Accuracy:  97.74% || Test Accuracy: 95.32%\n",
      "epoch: 890 || Cost:  0.06733 || Train Accuracy:  97.74% || Test Accuracy: 95.32%\n",
      "epoch: 900 || Cost:  0.06693 || Train Accuracy:  97.74% || Test Accuracy: 95.32%\n",
      "epoch: 910 || Cost:  0.06653 || Train Accuracy:  97.74% || Test Accuracy: 95.32%\n",
      "epoch: 920 || Cost:  0.06612 || Train Accuracy:  97.74% || Test Accuracy: 95.32%\n",
      "epoch: 930 || Cost:  0.06573 || Train Accuracy:  97.74% || Test Accuracy: 95.32%\n",
      "epoch: 940 || Cost:  0.06534 || Train Accuracy:  97.74% || Test Accuracy: 95.32%\n",
      "epoch: 950 || Cost:  0.06495 || Train Accuracy:  97.74% || Test Accuracy: 95.32%\n",
      "epoch: 960 || Cost:  0.06458 || Train Accuracy:  97.74% || Test Accuracy: 95.32%\n",
      "epoch: 970 || Cost:  0.06420 || Train Accuracy:  97.74% || Test Accuracy: 95.32%\n",
      "epoch: 980 || Cost:  0.06384 || Train Accuracy:  97.74% || Test Accuracy: 95.32%\n",
      "epoch: 990 || Cost:  0.06348 || Train Accuracy:  97.74% || Test Accuracy: 95.32%\n",
      "epoch: 1000 || Cost:  0.06313 || Train Accuracy:  97.74% || Test Accuracy: 95.32%\n",
      "epoch: 1010 || Cost:  0.06279 || Train Accuracy:  97.74% || Test Accuracy: 95.32%\n",
      "epoch: 1020 || Cost:  0.06246 || Train Accuracy:  97.74% || Test Accuracy: 95.32%\n",
      "epoch: 1030 || Cost:  0.06213 || Train Accuracy:  97.74% || Test Accuracy: 95.32%\n",
      "epoch: 1040 || Cost:  0.06181 || Train Accuracy:  97.74% || Test Accuracy: 95.32%\n",
      "epoch: 1050 || Cost:  0.06150 || Train Accuracy:  97.74% || Test Accuracy: 95.32%\n",
      "epoch: 1060 || Cost:  0.06119 || Train Accuracy:  97.74% || Test Accuracy: 95.32%\n",
      "epoch: 1070 || Cost:  0.06089 || Train Accuracy:  97.74% || Test Accuracy: 95.32%\n",
      "epoch: 1080 || Cost:  0.06060 || Train Accuracy:  97.74% || Test Accuracy: 95.32%\n",
      "epoch: 1090 || Cost:  0.06032 || Train Accuracy:  97.74% || Test Accuracy: 95.32%\n",
      "epoch: 1100 || Cost:  0.06004 || Train Accuracy:  97.74% || Test Accuracy: 95.32%\n",
      "epoch: 1110 || Cost:  0.05976 || Train Accuracy:  97.74% || Test Accuracy: 95.32%\n",
      "epoch: 1120 || Cost:  0.05949 || Train Accuracy:  97.74% || Test Accuracy: 95.32%\n",
      "epoch: 1130 || Cost:  0.05923 || Train Accuracy:  97.74% || Test Accuracy: 95.32%\n",
      "epoch: 1140 || Cost:  0.05897 || Train Accuracy:  97.99% || Test Accuracy: 95.32%\n",
      "epoch: 1150 || Cost:  0.05871 || Train Accuracy:  97.99% || Test Accuracy: 95.32%\n",
      "epoch: 1160 || Cost:  0.05846 || Train Accuracy:  97.99% || Test Accuracy: 95.32%\n",
      "epoch: 1170 || Cost:  0.05821 || Train Accuracy:  97.99% || Test Accuracy: 95.32%\n",
      "epoch: 1180 || Cost:  0.05796 || Train Accuracy:  97.99% || Test Accuracy: 95.32%\n",
      "epoch: 1190 || Cost:  0.05772 || Train Accuracy:  97.99% || Test Accuracy: 95.32%\n",
      "epoch: 1200 || Cost:  0.05747 || Train Accuracy:  97.99% || Test Accuracy: 95.32%\n",
      "epoch: 1210 || Cost:  0.05724 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 1220 || Cost:  0.05700 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 1230 || Cost:  0.05676 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 1240 || Cost:  0.05652 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 1250 || Cost:  0.05629 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 1260 || Cost:  0.05605 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 1270 || Cost:  0.05582 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 1280 || Cost:  0.05558 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 1290 || Cost:  0.05534 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 1300 || Cost:  0.05510 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 1310 || Cost:  0.05485 || Train Accuracy:  97.74% || Test Accuracy: 95.91%\n",
      "epoch: 1320 || Cost:  0.05461 || Train Accuracy:  97.74% || Test Accuracy: 95.91%\n",
      "epoch: 1330 || Cost:  0.05435 || Train Accuracy:  97.74% || Test Accuracy: 95.91%\n",
      "epoch: 1340 || Cost:  0.05410 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 1350 || Cost:  0.05384 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 1360 || Cost:  0.05357 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 1370 || Cost:  0.05329 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 1380 || Cost:  0.05301 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 1390 || Cost:  0.05272 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 1400 || Cost:  0.05243 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 1410 || Cost:  0.05213 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 1420 || Cost:  0.05183 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 1430 || Cost:  0.05153 || Train Accuracy:  97.99% || Test Accuracy: 95.91%\n",
      "epoch: 1440 || Cost:  0.05123 || Train Accuracy:  98.24% || Test Accuracy: 95.91%\n",
      "epoch: 1450 || Cost:  0.05094 || Train Accuracy:  98.24% || Test Accuracy: 95.91%\n",
      "epoch: 1460 || Cost:  0.05065 || Train Accuracy:  98.24% || Test Accuracy: 95.91%\n",
      "epoch: 1470 || Cost:  0.05037 || Train Accuracy:  98.24% || Test Accuracy: 95.91%\n",
      "epoch: 1480 || Cost:  0.05010 || Train Accuracy:  98.24% || Test Accuracy: 95.91%\n",
      "epoch: 1490 || Cost:  0.04985 || Train Accuracy:  98.24% || Test Accuracy: 95.91%\n",
      "epoch: 1500 || Cost:  0.04960 || Train Accuracy:  98.24% || Test Accuracy: 95.91%\n",
      "epoch: 1510 || Cost:  0.04937 || Train Accuracy:  98.24% || Test Accuracy: 95.91%\n",
      "epoch: 1520 || Cost:  0.04914 || Train Accuracy:  98.24% || Test Accuracy: 95.91%\n",
      "epoch: 1530 || Cost:  0.04893 || Train Accuracy:  98.24% || Test Accuracy: 95.91%\n",
      "epoch: 1540 || Cost:  0.04872 || Train Accuracy:  98.24% || Test Accuracy: 95.91%\n",
      "epoch: 1550 || Cost:  0.04852 || Train Accuracy:  98.24% || Test Accuracy: 95.91%\n",
      "epoch: 1560 || Cost:  0.04833 || Train Accuracy:  98.24% || Test Accuracy: 95.91%\n",
      "epoch: 1570 || Cost:  0.04815 || Train Accuracy:  98.24% || Test Accuracy: 95.91%\n",
      "epoch: 1580 || Cost:  0.04797 || Train Accuracy:  98.24% || Test Accuracy: 95.91%\n",
      "epoch: 1590 || Cost:  0.04779 || Train Accuracy:  98.24% || Test Accuracy: 95.91%\n",
      "epoch: 1600 || Cost:  0.04762 || Train Accuracy:  98.24% || Test Accuracy: 95.91%\n",
      "epoch: 1610 || Cost:  0.04746 || Train Accuracy:  98.24% || Test Accuracy: 95.91%\n",
      "epoch: 1620 || Cost:  0.04730 || Train Accuracy:  98.24% || Test Accuracy: 95.91%\n",
      "epoch: 1630 || Cost:  0.04714 || Train Accuracy:  98.24% || Test Accuracy: 95.91%\n",
      "epoch: 1640 || Cost:  0.04698 || Train Accuracy:  98.24% || Test Accuracy: 95.91%\n",
      "epoch: 1650 || Cost:  0.04682 || Train Accuracy:  98.24% || Test Accuracy: 95.91%\n",
      "epoch: 1660 || Cost:  0.04667 || Train Accuracy:  98.24% || Test Accuracy: 95.91%\n",
      "epoch: 1670 || Cost:  0.04652 || Train Accuracy:  98.24% || Test Accuracy: 95.91%\n",
      "epoch: 1680 || Cost:  0.04637 || Train Accuracy:  98.24% || Test Accuracy: 95.91%\n",
      "epoch: 1690 || Cost:  0.04622 || Train Accuracy:  98.24% || Test Accuracy: 95.91%\n",
      "epoch: 1700 || Cost:  0.04607 || Train Accuracy:  98.24% || Test Accuracy: 95.91%\n",
      "epoch: 1710 || Cost:  0.04592 || Train Accuracy:  98.24% || Test Accuracy: 95.91%\n",
      "epoch: 1720 || Cost:  0.04577 || Train Accuracy:  98.24% || Test Accuracy: 95.91%\n",
      "epoch: 1730 || Cost:  0.04562 || Train Accuracy:  98.24% || Test Accuracy: 95.91%\n",
      "epoch: 1740 || Cost:  0.04547 || Train Accuracy:  98.24% || Test Accuracy: 95.91%\n",
      "epoch: 1750 || Cost:  0.04531 || Train Accuracy:  98.24% || Test Accuracy: 95.91%\n",
      "epoch: 1760 || Cost:  0.04515 || Train Accuracy:  98.24% || Test Accuracy: 95.91%\n",
      "epoch: 1770 || Cost:  0.04499 || Train Accuracy:  98.24% || Test Accuracy: 95.91%\n",
      "epoch: 1780 || Cost:  0.04482 || Train Accuracy:  98.24% || Test Accuracy: 95.91%\n",
      "epoch: 1790 || Cost:  0.04463 || Train Accuracy:  98.24% || Test Accuracy: 95.91%\n",
      "epoch: 1800 || Cost:  0.04443 || Train Accuracy:  98.24% || Test Accuracy: 95.91%\n",
      "epoch: 1810 || Cost:  0.04420 || Train Accuracy:  98.24% || Test Accuracy: 95.91%\n",
      "epoch: 1820 || Cost:  0.04392 || Train Accuracy:  98.24% || Test Accuracy: 95.91%\n",
      "epoch: 1830 || Cost:  0.04354 || Train Accuracy:  98.24% || Test Accuracy: 95.91%\n",
      "epoch: 1840 || Cost:  0.04300 || Train Accuracy:  98.24% || Test Accuracy: 95.91%\n",
      "epoch: 1850 || Cost:  0.04223 || Train Accuracy:  98.24% || Test Accuracy: 95.91%\n",
      "epoch: 1860 || Cost:  0.04140 || Train Accuracy:  98.49% || Test Accuracy: 95.91%\n",
      "epoch: 1870 || Cost:  0.04082 || Train Accuracy:  98.49% || Test Accuracy: 95.91%\n",
      "epoch: 1880 || Cost:  0.04044 || Train Accuracy:  98.49% || Test Accuracy: 95.91%\n",
      "epoch: 1890 || Cost:  0.04015 || Train Accuracy:  98.49% || Test Accuracy: 95.91%\n",
      "epoch: 1900 || Cost:  0.03991 || Train Accuracy:  98.49% || Test Accuracy: 95.91%\n",
      "epoch: 1910 || Cost:  0.03969 || Train Accuracy:  98.49% || Test Accuracy: 95.91%\n",
      "epoch: 1920 || Cost:  0.03948 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1930 || Cost:  0.03929 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1940 || Cost:  0.03909 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1950 || Cost:  0.03891 || Train Accuracy:  98.49% || Test Accuracy: 96.49%\n",
      "epoch: 1960 || Cost:  0.03872 || Train Accuracy:  98.74% || Test Accuracy: 96.49%\n",
      "epoch: 1970 || Cost:  0.03855 || Train Accuracy:  98.74% || Test Accuracy: 96.49%\n",
      "epoch: 1980 || Cost:  0.03837 || Train Accuracy:  98.74% || Test Accuracy: 96.49%\n",
      "epoch: 1990 || Cost:  0.03820 || Train Accuracy:  98.74% || Test Accuracy: 96.49%\n"
     ]
    }
   ],
   "source": [
    "nn2 = SingleLayerNN(30, 4, 1)\n",
    "acc = nn2.optimize(xtrain, ytrain,validation=(xtest, ytest), epochs=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(acc[:, 0], label='Train Accuracy')\n",
    "plt.plot(acc[:, 1], label='Test Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2be5faf79681da6f2a61fdfdd5405d65d042280f7fba6178067603e3a2925119"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
