{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mathematical expression of the algorithm**:\n",
    "\n",
    "For one example $x^{(i)}$:\n",
    "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
    "$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$ \n",
    "$$ \\mathcal{L}(yhat^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(yhat^{(i)}) + (1-y^{(i)} )  \\log(1-yhat^{(i)})\\tag{3}$$\n",
    "\n",
    "The cost is then computing:\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(yhat^{(i)}, y^{(i)})\\tag{6}$$\n",
    "\n",
    "Gradient Computing:\n",
    "- $$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(yhat-Y)^T\\tag{7}$$\n",
    "- $$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (yhat^{(i)}-y^{(i)})\\tag{8}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.randn(3,2)\n",
    "\n",
    "B = np.sum(A, axis = 1, keepdims = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight and bias initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_b_init(dim: int):\n",
    "    w = tf.zeros([dim, 1], dtype=tf.float64)\n",
    "    b = 0.0\n",
    "    return w, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 1), dtype=float64, numpy=\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.]])>,\n",
       " 0.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, b = w_b_init(3)\n",
    "w, b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate $z$ for all $x^{(i)}$:\n",
    "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_calc(w: tf.Tensor, b: tf.float64, x: tf.Tensor):\n",
    "    z = tf.tensordot(tf.transpose(w), x, axes=1) + b\n",
    "    return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float64, numpy=array([0.])>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w , b = w_b_init(3)\n",
    "x = tf.Variable([3, 2, 4], dtype=tf.float64)\n",
    "z_calc(w, b, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid Funtion\n",
    "compute $sigmoid(z) = \\frac{1}{1 + e^{-z}}$ for $z = w^T x + b$ to make predictions. Use np.exp() or tf.exp()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z: tf.Tensor) -> tf.Tensor:\n",
    "    s = 1/(1 + tf.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float64, numpy=array([0.5])>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = z_calc(w, b, x)\n",
    "sigmoid(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the Cost :\n",
    " $J = -\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}\\log(yhat^{(i)})+(1-y^{(i)})\\log(1-yhat^{(i)}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(yhat:tf.Tensor, y:tf.Tensor):\n",
    "    loss = tf.reduce_sum((y * tf.math.log(yhat)) + ((1-y) * tf.math.log(1-yhat)))\n",
    "    c = (-1/yhat.shape[0]) * loss\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation:\n",
    "- You get X\n",
    "- You compute $yhat = \\sigma(w^T X + b) $\n",
    "- You calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}\\log(yhat^{(i)})+(1-y^{(i)})\\log(1-yhat^{(i)}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(w: tf.Tensor, b: tf.float64, x: tf.Tensor, y: tf.Tensor):\n",
    "    z = z_calc(w, b, x)\n",
    "    yhat = sigmoid(z)\n",
    "    c = cost(yhat, y)\n",
    "    return yhat, tf.squeeze(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation: \n",
    "\n",
    "- $$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(yhat-y)^T\\tag{7}$$\n",
    "- $$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (yhat^{(i)}-y^{(i)})\\tag{8}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(x: tf.Tensor, y: tf.Tensor, yhat: tf.Tensor):\n",
    "    m = y.shape[0]\n",
    "    dw = (1/m) * tf.tensordot(x, tf.transpose(yhat - y), axes=1)\n",
    "    db = (1/m) * tf.reduce_sum(yhat - y)\n",
    "    return {'dw': dw, 'db': db}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, x, y, num_iterations=100, learning_rate=0.009):\n",
    "\n",
    "    for epoch in range(num_iterations):\n",
    "        yhat, loss = forward_propagation(w, b, x, y)\n",
    "        grads = back_propagation(x, y, yhat)\n",
    "        w = w - learning_rate * grads['dw']\n",
    "        b = b - learning_rate * grads['db']\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch: {epoch} => Loss: {loss}\")\n",
    "    return w, b, grads, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 => Loss: 0.47701613123077213\n",
      "Epoch: 10 => Loss: 0.4164531274535226\n",
      "Epoch: 20 => Loss: 0.367523498948689\n",
      "Epoch: 30 => Loss: 0.3275456740982752\n",
      "Epoch: 40 => Loss: 0.2945102061378818\n",
      "Epoch: 50 => Loss: 0.26691094275228944\n",
      "Epoch: 60 => Loss: 0.24361332072642702\n",
      "Epoch: 70 => Loss: 0.2237560544308519\n",
      "Epoch: 80 => Loss: 0.20667926683465648\n",
      "Epoch: 90 => Loss: 0.19187237397489834\n",
      "Epoch: 100 => Loss: 0.17893631895908704\n",
      "Epoch: 110 => Loss: 0.167556102524381\n",
      "Epoch: 120 => Loss: 0.15748067635971635\n",
      "Epoch: 130 => Loss: 0.14850811044843798\n",
      "Epoch: 140 => Loss: 0.14047455659043903\n",
      "Epoch: 150 => Loss: 0.13324596273006736\n",
      "Epoch: 160 => Loss: 0.12671179611607164\n",
      "Epoch: 170 => Loss: 0.12078024571170254\n",
      "Epoch: 180 => Loss: 0.1153745232473739\n",
      "Epoch: 190 => Loss: 0.11042998727123679\n",
      "Epoch: 200 => Loss: 0.10589188895569433\n",
      "Epoch: 210 => Loss: 0.10171359152781864\n",
      "Epoch: 220 => Loss: 0.09785515338863691\n",
      "Epoch: 230 => Loss: 0.09428219267242786\n",
      "Epoch: 240 => Loss: 0.0909649712233753\n",
      "Epoch: 250 => Loss: 0.08787765085935872\n",
      "Epoch: 260 => Loss: 0.08499768584279371\n",
      "Epoch: 270 => Loss: 0.08230532373964133\n",
      "Epoch: 280 => Loss: 0.07978319306936461\n",
      "Epoch: 290 => Loss: 0.07741596086766094\n",
      "Epoch: 300 => Loss: 0.07519004688783387\n",
      "Epoch: 310 => Loss: 0.07309338393745153\n",
      "Epoch: 320 => Loss: 0.07111521599074429\n",
      "Epoch: 330 => Loss: 0.06924592738615201\n",
      "Epoch: 340 => Loss: 0.06747689772540502\n",
      "Epoch: 350 => Loss: 0.06580037811986968\n",
      "Epoch: 360 => Loss: 0.06420938524502097\n",
      "Epoch: 370 => Loss: 0.0626976103128104\n",
      "Epoch: 380 => Loss: 0.06125934059089404\n",
      "Epoch: 390 => Loss: 0.05988939151509914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 1), dtype=float64, numpy=\n",
       " array([[0.0966967 ],\n",
       "        [2.27572291]])>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=1.9455882442473833>,\n",
       " {'dw': <tf.Tensor: shape=(2, 1), dtype=float64, numpy=\n",
       "  array([[ 0.10104914],\n",
       "         [-0.04072141]])>,\n",
       "  'db': <tf.Tensor: shape=(), dtype=float64, numpy=-0.04856141334469418>},\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=0.058710945448840236>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w =  np.array([[1.], [2]])\n",
    "b = 1.5\n",
    "X = np.array([[1., -2., -1.], [3., 0.5, -3.2]])\n",
    "Y = np.array([[1, 1, 0]])\n",
    "optimize(w, b, X, Y, num_iterations=400, learning_rate = 0.009)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "- Initialize $$ w,b $$\n",
    "- Forward Propagation:\n",
    "    - You get X\n",
    "    - You compute $yhat = \\sigma(w^T X + b) $\n",
    "    - You calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}\\log(yhat^{(i)})+(1-y^{(i)})\\log(1-yhat^{(i)}))$\n",
    "- Back Propagation: \n",
    "    - $$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(yhat-y)^T\\tag{7}$$\n",
    "    - $$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (yhat^{(i)}-y^{(i)})\\tag{8}$$\n",
    "- Update weights:\n",
    "    - $$ w = w - {\\alpha} * \\frac{\\partial J}{\\partial w} $$\n",
    "    - $$ b = b- {\\alpha}  * \\frac{\\partial J}{\\partial b} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradienDescent:\n",
    "    def __init__(self, dim: int):\n",
    "        self.w, self.b = tf.random.normal(\n",
    "            [dim, 1], dtype=tf.float64, seed=42), 0.0\n",
    "        self.params = dict()\n",
    "        self.cost = None\n",
    "        self.losses = list()\n",
    "        self.metric = [[0.0, ], [0.0, ]]\n",
    "\n",
    "    def forward(self, x: tf.Tensor):\n",
    "        z = tf.tensordot(tf.transpose(self.w), x, axes=1) + self.b\n",
    "        return 1/(1 + tf.exp(-z))\n",
    "\n",
    "    def forward_propagation(self, w: tf.Tensor, b: tf.float64, x: tf.Tensor, y: tf.Tensor):\n",
    "        yhat = self.forward(x)\n",
    "        loss = tf.reduce_sum((y * tf.math.log(yhat)) +\n",
    "                             ((1-y) * tf.math.log(1-yhat)))\n",
    "        cost = (-1/y.shape[0]) * loss\n",
    "        return yhat, tf.squeeze(cost)\n",
    "\n",
    "    def back_propagation(self, x: tf.Tensor, y: tf.Tensor, yhat: tf.Tensor):\n",
    "        m = y.shape[0]\n",
    "        dw = (1/m) * tf.tensordot(x, tf.transpose(yhat - y), axes=1)\n",
    "        db = (1/m) * tf.reduce_sum(yhat - y)\n",
    "        return {'dw': dw, 'db': db}\n",
    "\n",
    "    def evaluate(self, ytrain, ytrain_pred, ytest, ytest_pred):\n",
    "        train_acc = 100 - np.mean(np.abs(ytrain_pred - ytrain)) * 100\n",
    "        test_acc = 100 - np.mean(np.abs(ytest_pred - ytest)) * 100\n",
    "        return train_acc, test_acc\n",
    "\n",
    "    def optimize(self, x, y, validation=None, num_iterations=100, learning_rate=0.009):\n",
    "        for epoch in range(num_iterations):\n",
    "            yhat, self.cost = self.forward_propagation(\n",
    "                self.w, self.b, x, y)\n",
    "            grads = self.back_propagation(x, y, yhat)\n",
    "            self.w = self.w - learning_rate * grads['dw']\n",
    "            self.b = self.b - learning_rate * grads['db']\n",
    "            self.losses.append(self.cost)\n",
    "            if epoch % 10 == 0:\n",
    "                ytest_pred = self.predict(validation[0])\n",
    "                train_acc, test_acc = self.evaluate(\n",
    "                    y, tf.round(yhat), validation[1], ytest_pred)\n",
    "                print(\n",
    "                    f\"Epoch: {epoch} => Train Accuraccy: {train_acc:.2f}% || Test Accuracy:{test_acc:.2f}% || Loss: {self.cost}\")\n",
    "                self.metric[0].append(train_acc)\n",
    "                self.metric[1].append(test_acc)\n",
    "        self.params['w'] = self.w\n",
    "        self.params['b'] = self.b\n",
    "        return self.w, self.b, grads, self.cost\n",
    "\n",
    "    def predict(self, x: tf.Tensor):\n",
    "        return tf.round(self.forward(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 => Train Accuraccy: 25.00% || Test Accuracy:50.00% || Loss: 1.315912193032579\n",
      "Epoch: 10 => Train Accuraccy: 25.00% || Test Accuracy:50.00% || Loss: 1.130260498619716\n",
      "Epoch: 20 => Train Accuraccy: 25.00% || Test Accuracy:50.00% || Loss: 0.9856022946926002\n",
      "Epoch: 30 => Train Accuraccy: 25.00% || Test Accuracy:50.00% || Loss: 0.8726790689316903\n",
      "Epoch: 40 => Train Accuraccy: 50.00% || Test Accuracy:100.00% || Loss: 0.7836236894709461\n",
      "Epoch: 50 => Train Accuraccy: 75.00% || Test Accuracy:100.00% || Loss: 0.7122113259589877\n",
      "Epoch: 60 => Train Accuraccy: 75.00% || Test Accuracy:100.00% || Loss: 0.653783792902152\n",
      "Epoch: 70 => Train Accuraccy: 75.00% || Test Accuracy:100.00% || Loss: 0.6049739804328483\n",
      "Epoch: 80 => Train Accuraccy: 75.00% || Test Accuracy:100.00% || Loss: 0.5633875293801394\n",
      "Epoch: 90 => Train Accuraccy: 75.00% || Test Accuracy:100.00% || Loss: 0.5273272263749174\n",
      "Epoch: 100 => Train Accuraccy: 75.00% || Test Accuracy:100.00% || Loss: 0.49558341659807226\n",
      "Epoch: 110 => Train Accuraccy: 75.00% || Test Accuracy:100.00% || Loss: 0.4672844909046534\n",
      "Epoch: 120 => Train Accuraccy: 75.00% || Test Accuracy:100.00% || Loss: 0.4417934100841411\n",
      "Epoch: 130 => Train Accuraccy: 75.00% || Test Accuracy:100.00% || Loss: 0.4186369268474747\n",
      "Epoch: 140 => Train Accuraccy: 75.00% || Test Accuracy:100.00% || Loss: 0.39745723616407513\n",
      "Epoch: 150 => Train Accuraccy: 75.00% || Test Accuracy:100.00% || Loss: 0.3779787978173999\n",
      "Epoch: 160 => Train Accuraccy: 75.00% || Test Accuracy:100.00% || Loss: 0.3599853975276035\n",
      "Epoch: 170 => Train Accuraccy: 75.00% || Test Accuracy:100.00% || Loss: 0.3433041442116412\n",
      "Epoch: 180 => Train Accuraccy: 75.00% || Test Accuracy:100.00% || Loss: 0.3277942003844543\n",
      "Epoch: 190 => Train Accuraccy: 75.00% || Test Accuracy:100.00% || Loss: 0.3133387710442907\n",
      "Epoch: 200 => Train Accuraccy: 75.00% || Test Accuracy:100.00% || Loss: 0.299839356719864\n",
      "Epoch: 210 => Train Accuraccy: 75.00% || Test Accuracy:100.00% || Loss: 0.28721159401395074\n",
      "Epoch: 220 => Train Accuraccy: 75.00% || Test Accuracy:100.00% || Loss: 0.27538221853536077\n",
      "Epoch: 230 => Train Accuraccy: 75.00% || Test Accuracy:100.00% || Loss: 0.2642868273471792\n",
      "Epoch: 240 => Train Accuraccy: 75.00% || Test Accuracy:100.00% || Loss: 0.25386821471496523\n",
      "Epoch: 250 => Train Accuraccy: 75.00% || Test Accuracy:100.00% || Loss: 0.24407512134751813\n",
      "Epoch: 260 => Train Accuraccy: 75.00% || Test Accuracy:100.00% || Loss: 0.2348612834395567\n",
      "Epoch: 270 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.22618470016715198\n",
      "Epoch: 280 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.21800706116440052\n",
      "Epoch: 290 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.21029329181019152\n",
      "Epoch: 300 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.20301118583436045\n",
      "Epoch: 310 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.19613110315757631\n",
      "Epoch: 320 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.18962571694419048\n",
      "Epoch: 330 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.18346979822999848\n",
      "Epoch: 340 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.1776400296551861\n",
      "Epoch: 350 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.1721148421219027\n",
      "Epoch: 360 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.16687426984787743\n",
      "Epoch: 370 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.1618998204776419\n",
      "Epoch: 380 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.15717435776880273\n",
      "Epoch: 390 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.1526819949852569\n",
      "Epoch: 400 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.1484079975698905\n",
      "Epoch: 410 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.14433869398527846\n",
      "Epoch: 420 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.1404613938378618\n",
      "Epoch: 430 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.13676431256478308\n",
      "Epoch: 440 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.133236502081533\n",
      "Epoch: 450 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.12986778687608194\n",
      "Epoch: 460 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.126648705100665\n",
      "Epoch: 470 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.12357045426253987\n",
      "Epoch: 480 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.12062484115451058\n",
      "Epoch: 490 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.11780423569805892\n",
      "Epoch: 500 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.1151015283987935\n",
      "Epoch: 510 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.11251009113712268\n",
      "Epoch: 520 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.11002374103762225\n",
      "Epoch: 530 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.10763670717917315\n",
      "Epoch: 540 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.10534359992503198\n",
      "Epoch: 550 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.10313938266786736\n",
      "Epoch: 560 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.10101934579961558\n",
      "Epoch: 570 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.09897908272991462\n",
      "Epoch: 580 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.09701446778992222\n",
      "Epoch: 590 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.09512163587058152\n",
      "Epoch: 600 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.09329696365588933\n",
      "Epoch: 610 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.09153705232247937\n",
      "Epoch: 620 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.08983871158689202\n",
      "Epoch: 630 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.08819894499127273\n",
      "Epoch: 640 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.08661493632697106\n",
      "Epoch: 650 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.08508403710360728\n",
      "Epoch: 660 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.08360375497868669\n",
      "Epoch: 670 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.0821717430697817\n",
      "Epoch: 680 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.08078579007771908\n",
      "Epoch: 690 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.07944381115512036\n",
      "Epoch: 700 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.07814383946009083\n",
      "Epoch: 710 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.07688401833986093\n",
      "Epoch: 720 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.07566259409378243\n",
      "Epoch: 730 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.07447790926930917\n",
      "Epoch: 740 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.07332839644846012\n",
      "Epoch: 750 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.07221257248581654\n",
      "Epoch: 760 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.07112903316235433\n",
      "Epoch: 770 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.07007644822238679\n",
      "Epoch: 780 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.06905355676362229\n",
      "Epoch: 790 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.06805916295282757\n",
      "Epoch: 800 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.06709213204186983\n",
      "Epoch: 810 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.06615138666099529\n",
      "Epoch: 820 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.06523590336810556\n",
      "Epoch: 830 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.06434470943453913\n",
      "Epoch: 840 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.0634768798494564\n",
      "Epoch: 850 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.06263153452638573\n",
      "Epoch: 860 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.06180783569682066\n",
      "Epoch: 870 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.061004985476976845\n",
      "Epoch: 880 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.060222223594935\n",
      "Epoch: 890 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.05945882526641478\n",
      "Epoch: 900 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.058714099208359564\n",
      "Epoch: 910 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.05798738578037059\n",
      "Epoch: 920 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.05727805524480788\n",
      "Epoch: 930 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.0565855061370967\n",
      "Epoch: 940 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.055909163738437254\n",
      "Epoch: 950 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.05524847864371426\n",
      "Epoch: 960 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.05460292541796336\n",
      "Epoch: 970 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.05397200133525138\n",
      "Epoch: 980 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.053355225194301414\n",
      "Epoch: 990 => Train Accuraccy: 100.00% || Test Accuracy:100.00% || Loss: 0.05275213620561568\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 1), dtype=float64, numpy=\n",
       " array([[-0.16857786],\n",
       "        [ 1.38673895]])>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=0.8258281430689851>,\n",
       " {'dw': <tf.Tensor: shape=(2, 1), dtype=float64, numpy=\n",
       "  array([[ 0.03774538],\n",
       "         [-0.06254585]])>,\n",
       "  'db': <tf.Tensor: shape=(), dtype=float64, numpy=-0.03399722003777138>},\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=0.052220693409986675>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.Variable([\n",
    "    [1., -2., -1., 3.4],\n",
    "    [3., 0.5, -3.2, 2.5]\n",
    "], dtype=tf.float64)\n",
    "Y = np.array([1, 1, 0, 1])\n",
    "xv = tf.Variable([\n",
    "    [-1.3, 3.1],\n",
    "    [-3.6, 2.1]\n",
    "], dtype=tf.float64)\n",
    "yv = np.array([0, 1])\n",
    "model = GradienDescent(2)\n",
    "model.optimize(X, Y, validation=(xv, yv), num_iterations=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x115c8a09150>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model.losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "x, y = datasets.load_breast_cancer(return_X_y=True)\n",
    "x = StandardScaler().fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(\n",
    "    x, y, random_state=33, test_size=0.3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = tf.Variable(xtrain.T, dtype=tf.float64)\n",
    "xtest = tf.Variable(xtest.T, dtype=tf.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 => Train Accuraccy: 20.35% || Test Accuracy:23.39% || Loss: 4.673907349138036\n",
      "Epoch: 10 => Train Accuraccy: 22.86% || Test Accuracy:26.32% || Loss: 4.122652431218166\n",
      "Epoch: 20 => Train Accuraccy: 26.13% || Test Accuracy:29.24% || Loss: 3.6136960053111182\n",
      "Epoch: 30 => Train Accuraccy: 30.40% || Test Accuracy:35.09% || Loss: 3.1510475079728857\n",
      "Epoch: 40 => Train Accuraccy: 34.92% || Test Accuracy:40.35% || Loss: 2.7374617955835268\n",
      "Epoch: 50 => Train Accuraccy: 38.19% || Test Accuracy:45.03% || Loss: 2.3766072811085808\n",
      "Epoch: 60 => Train Accuraccy: 43.72% || Test Accuracy:50.88% || Loss: 2.0705915695216692\n",
      "Epoch: 70 => Train Accuraccy: 48.49% || Test Accuracy:56.14% || Loss: 1.8160489198457603\n",
      "Epoch: 80 => Train Accuraccy: 51.76% || Test Accuracy:59.06% || Loss: 1.6069260867106772\n",
      "Epoch: 90 => Train Accuraccy: 55.03% || Test Accuracy:63.16% || Loss: 1.4369891165928244\n",
      "Epoch: 100 => Train Accuraccy: 59.55% || Test Accuracy:65.50% || Loss: 1.3001557901099674\n",
      "Epoch: 110 => Train Accuraccy: 63.32% || Test Accuracy:67.84% || Loss: 1.190143613039097\n",
      "Epoch: 120 => Train Accuraccy: 65.33% || Test Accuracy:70.76% || Loss: 1.1011180032544365\n",
      "Epoch: 130 => Train Accuraccy: 67.84% || Test Accuracy:73.68% || Loss: 1.0279451893807503\n",
      "Epoch: 140 => Train Accuraccy: 68.84% || Test Accuracy:75.44% || Loss: 0.9660898503325214\n",
      "Epoch: 150 => Train Accuraccy: 70.60% || Test Accuracy:76.02% || Loss: 0.912484966740333\n",
      "Epoch: 160 => Train Accuraccy: 71.86% || Test Accuracy:76.61% || Loss: 0.8653378270709746\n",
      "Epoch: 170 => Train Accuraccy: 73.62% || Test Accuracy:77.19% || Loss: 0.823515562367395\n",
      "Epoch: 180 => Train Accuraccy: 75.13% || Test Accuracy:78.36% || Loss: 0.7862125000852307\n",
      "Epoch: 190 => Train Accuraccy: 75.38% || Test Accuracy:78.36% || Loss: 0.7528118445832006\n",
      "Epoch: 200 => Train Accuraccy: 76.38% || Test Accuracy:80.70% || Loss: 0.7228241106801122\n",
      "Epoch: 210 => Train Accuraccy: 77.64% || Test Accuracy:81.29% || Loss: 0.6958498388061441\n",
      "Epoch: 220 => Train Accuraccy: 79.65% || Test Accuracy:82.46% || Loss: 0.6715461147832594\n",
      "Epoch: 230 => Train Accuraccy: 80.40% || Test Accuracy:84.21% || Loss: 0.6495962039637526\n",
      "Epoch: 240 => Train Accuraccy: 81.91% || Test Accuracy:84.21% || Loss: 0.6296938102326298\n",
      "Epoch: 250 => Train Accuraccy: 82.66% || Test Accuracy:84.21% || Loss: 0.6115475161253707\n",
      "Epoch: 260 => Train Accuraccy: 83.67% || Test Accuracy:84.80% || Loss: 0.594894920771301\n",
      "Epoch: 270 => Train Accuraccy: 84.17% || Test Accuracy:84.80% || Loss: 0.5795125375889968\n",
      "Epoch: 280 => Train Accuraccy: 84.67% || Test Accuracy:84.80% || Loss: 0.5652168934448073\n",
      "Epoch: 290 => Train Accuraccy: 84.67% || Test Accuracy:85.38% || Loss: 0.5518597502402108\n",
      "Epoch: 300 => Train Accuraccy: 84.67% || Test Accuracy:85.38% || Loss: 0.5393214129599577\n",
      "Epoch: 310 => Train Accuraccy: 84.92% || Test Accuracy:85.38% || Loss: 0.5275044183708538\n",
      "Epoch: 320 => Train Accuraccy: 86.18% || Test Accuracy:85.96% || Loss: 0.516328399297725\n",
      "Epoch: 330 => Train Accuraccy: 86.43% || Test Accuracy:86.55% || Loss: 0.5057261708891195\n",
      "Epoch: 340 => Train Accuraccy: 86.93% || Test Accuracy:86.55% || Loss: 0.49564083066966647\n",
      "Epoch: 350 => Train Accuraccy: 87.19% || Test Accuracy:86.55% || Loss: 0.48602362772269875\n",
      "Epoch: 360 => Train Accuraccy: 87.19% || Test Accuracy:87.13% || Loss: 0.4768323919149851\n",
      "Epoch: 370 => Train Accuraccy: 87.19% || Test Accuracy:87.13% || Loss: 0.468030362970463\n",
      "Epoch: 380 => Train Accuraccy: 87.19% || Test Accuracy:87.72% || Loss: 0.4595853026346171\n",
      "Epoch: 390 => Train Accuraccy: 87.19% || Test Accuracy:87.72% || Loss: 0.45146880719661764\n",
      "Epoch: 400 => Train Accuraccy: 87.19% || Test Accuracy:88.89% || Loss: 0.4436557628789413\n",
      "Epoch: 410 => Train Accuraccy: 87.19% || Test Accuracy:89.47% || Loss: 0.43612390471902485\n",
      "Epoch: 420 => Train Accuraccy: 87.19% || Test Accuracy:90.06% || Loss: 0.42885345220737425\n",
      "Epoch: 430 => Train Accuraccy: 87.44% || Test Accuracy:90.06% || Loss: 0.42182680352813784\n",
      "Epoch: 440 => Train Accuraccy: 87.94% || Test Accuracy:90.64% || Loss: 0.4150282759227962\n",
      "Epoch: 450 => Train Accuraccy: 88.19% || Test Accuracy:90.64% || Loss: 0.40844388336582915\n",
      "Epoch: 460 => Train Accuraccy: 88.44% || Test Accuracy:90.64% || Loss: 0.4020611450773182\n",
      "Epoch: 470 => Train Accuraccy: 88.69% || Test Accuracy:90.64% || Loss: 0.39586891988239653\n",
      "Epoch: 480 => Train Accuraccy: 88.69% || Test Accuracy:90.64% || Loss: 0.38985726238656876\n",
      "Epoch: 490 => Train Accuraccy: 88.94% || Test Accuracy:90.64% || Loss: 0.3840172975769061\n",
      "Epoch: 500 => Train Accuraccy: 88.94% || Test Accuracy:90.64% || Loss: 0.37834111090695344\n",
      "Epoch: 510 => Train Accuraccy: 89.20% || Test Accuracy:90.64% || Loss: 0.3728216512489183\n",
      "Epoch: 520 => Train Accuraccy: 89.45% || Test Accuracy:90.64% || Loss: 0.367452644339777\n",
      "Epoch: 530 => Train Accuraccy: 89.70% || Test Accuracy:91.23% || Loss: 0.3622285145329268\n",
      "Epoch: 540 => Train Accuraccy: 89.70% || Test Accuracy:91.23% || Loss: 0.35714431281496173\n",
      "Epoch: 550 => Train Accuraccy: 89.70% || Test Accuracy:91.23% || Loss: 0.35219564918321145\n",
      "Epoch: 560 => Train Accuraccy: 89.70% || Test Accuracy:91.23% || Loss: 0.3473786276367278\n",
      "Epoch: 570 => Train Accuraccy: 89.70% || Test Accuracy:91.23% || Loss: 0.3426897822515226\n",
      "Epoch: 580 => Train Accuraccy: 89.70% || Test Accuracy:91.23% || Loss: 0.33812601313251095\n",
      "Epoch: 590 => Train Accuraccy: 89.70% || Test Accuracy:91.23% || Loss: 0.33368452149420513\n",
      "Epoch: 600 => Train Accuraccy: 89.70% || Test Accuracy:91.23% || Loss: 0.32936274373218394\n",
      "Epoch: 610 => Train Accuraccy: 89.70% || Test Accuracy:91.23% || Loss: 0.32515828508271716\n",
      "Epoch: 620 => Train Accuraccy: 89.70% || Test Accuracy:91.23% || Loss: 0.3210688542553303\n",
      "Epoch: 630 => Train Accuraccy: 89.95% || Test Accuracy:91.23% || Loss: 0.31709220114247416\n",
      "Epoch: 640 => Train Accuraccy: 89.95% || Test Accuracy:91.23% || Loss: 0.3132260602130666\n",
      "Epoch: 650 => Train Accuraccy: 89.95% || Test Accuracy:91.23% || Loss: 0.3094681023422763\n",
      "Epoch: 660 => Train Accuraccy: 89.95% || Test Accuracy:91.81% || Loss: 0.3058158975321837\n",
      "Epoch: 670 => Train Accuraccy: 89.95% || Test Accuracy:91.81% || Loss: 0.3022668902429089\n",
      "Epoch: 680 => Train Accuraccy: 90.20% || Test Accuracy:91.81% || Loss: 0.2988183879927293\n",
      "Epoch: 690 => Train Accuraccy: 90.45% || Test Accuracy:91.81% || Loss: 0.2954675626927602\n",
      "Epoch: 700 => Train Accuraccy: 90.45% || Test Accuracy:91.81% || Loss: 0.2922114630815658\n",
      "Epoch: 710 => Train Accuraccy: 90.20% || Test Accuracy:91.81% || Loss: 0.2890470358098977\n",
      "Epoch: 720 => Train Accuraccy: 90.20% || Test Accuracy:91.81% || Loss: 0.2859711523070538\n",
      "Epoch: 730 => Train Accuraccy: 90.45% || Test Accuracy:92.40% || Loss: 0.2829806385500623\n",
      "Epoch: 740 => Train Accuraccy: 90.45% || Test Accuracy:92.40% || Loss: 0.2800723051824663\n",
      "Epoch: 750 => Train Accuraccy: 90.45% || Test Accuracy:92.40% || Loss: 0.27724297596924363\n",
      "Epoch: 760 => Train Accuraccy: 90.45% || Test Accuracy:92.40% || Loss: 0.2744895131979184\n",
      "Epoch: 770 => Train Accuraccy: 90.70% || Test Accuracy:92.40% || Loss: 0.2718088392348389\n",
      "Epoch: 780 => Train Accuraccy: 90.70% || Test Accuracy:92.40% || Loss: 0.2691979539491549\n",
      "Epoch: 790 => Train Accuraccy: 90.70% || Test Accuracy:92.40% || Loss: 0.26665394809304627\n",
      "Epoch: 800 => Train Accuraccy: 90.70% || Test Accuracy:92.40% || Loss: 0.2641740129736752\n",
      "Epoch: 810 => Train Accuraccy: 90.70% || Test Accuracy:92.40% || Loss: 0.26175544688777147\n",
      "Epoch: 820 => Train Accuraccy: 90.70% || Test Accuracy:92.40% || Loss: 0.25939565883986904\n",
      "Epoch: 830 => Train Accuraccy: 90.70% || Test Accuracy:92.40% || Loss: 0.257092170056859\n",
      "Epoch: 840 => Train Accuraccy: 90.70% || Test Accuracy:92.40% || Loss: 0.25484261376808814\n",
      "Epoch: 850 => Train Accuraccy: 90.70% || Test Accuracy:92.40% || Loss: 0.2526447336594248\n",
      "Epoch: 860 => Train Accuraccy: 91.21% || Test Accuracy:92.40% || Loss: 0.25049638134385593\n",
      "Epoch: 870 => Train Accuraccy: 91.46% || Test Accuracy:92.40% || Loss: 0.24839551312786828\n",
      "Epoch: 880 => Train Accuraccy: 91.46% || Test Accuracy:92.98% || Loss: 0.24634018629611545\n",
      "Epoch: 890 => Train Accuraccy: 91.46% || Test Accuracy:92.98% || Loss: 0.24432855508840584\n",
      "Epoch: 900 => Train Accuraccy: 91.46% || Test Accuracy:92.98% || Loss: 0.2423588665029342\n",
      "Epoch: 910 => Train Accuraccy: 91.46% || Test Accuracy:92.98% || Loss: 0.2404294560273615\n",
      "Epoch: 920 => Train Accuraccy: 91.46% || Test Accuracy:92.98% || Loss: 0.23853874337366604\n",
      "Epoch: 930 => Train Accuraccy: 91.46% || Test Accuracy:92.98% || Loss: 0.23668522827267294\n",
      "Epoch: 940 => Train Accuraccy: 91.46% || Test Accuracy:92.98% || Loss: 0.2348674863685742\n",
      "Epoch: 950 => Train Accuraccy: 91.46% || Test Accuracy:92.98% || Loss: 0.23308416524181422\n",
      "Epoch: 960 => Train Accuraccy: 91.46% || Test Accuracy:92.98% || Loss: 0.23133398057954185\n",
      "Epoch: 970 => Train Accuraccy: 91.46% || Test Accuracy:92.98% || Loss: 0.22961571250584895\n",
      "Epoch: 980 => Train Accuraccy: 91.71% || Test Accuracy:92.98% || Loss: 0.22792820207870138\n",
      "Epoch: 990 => Train Accuraccy: 91.96% || Test Accuracy:92.98% || Loss: 0.2262703479564773\n",
      "Epoch: 1000 => Train Accuraccy: 91.96% || Test Accuracy:92.98% || Loss: 0.22464110323400405\n",
      "Epoch: 1010 => Train Accuraccy: 91.96% || Test Accuracy:93.57% || Loss: 0.22303947244577074\n",
      "Epoch: 1020 => Train Accuraccy: 91.96% || Test Accuracy:93.57% || Loss: 0.22146450873233686\n",
      "Epoch: 1030 => Train Accuraccy: 92.46% || Test Accuracy:93.57% || Loss: 0.21991531116479734\n",
      "Epoch: 1040 => Train Accuraccy: 92.71% || Test Accuracy:93.57% || Loss: 0.2183910222213576\n",
      "Epoch: 1050 => Train Accuraccy: 92.71% || Test Accuracy:93.57% || Loss: 0.216890825409524\n",
      "Epoch: 1060 => Train Accuraccy: 92.71% || Test Accuracy:93.57% || Loss: 0.21541394302712594\n",
      "Epoch: 1070 => Train Accuraccy: 92.71% || Test Accuracy:93.57% || Loss: 0.2139596340552097\n",
      "Epoch: 1080 => Train Accuraccy: 92.71% || Test Accuracy:93.57% || Loss: 0.21252719217585922\n",
      "Epoch: 1090 => Train Accuraccy: 92.71% || Test Accuracy:94.15% || Loss: 0.21111594390806135\n",
      "Epoch: 1100 => Train Accuraccy: 92.71% || Test Accuracy:94.15% || Loss: 0.2097252468548846\n",
      "Epoch: 1110 => Train Accuraccy: 92.96% || Test Accuracy:94.15% || Loss: 0.20835448805546564\n",
      "Epoch: 1120 => Train Accuraccy: 92.96% || Test Accuracy:94.15% || Loss: 0.2070030824355261\n",
      "Epoch: 1130 => Train Accuraccy: 92.96% || Test Accuracy:94.15% || Loss: 0.20567047135042427\n",
      "Epoch: 1140 => Train Accuraccy: 92.96% || Test Accuracy:94.15% || Loss: 0.2043561212150245\n",
      "Epoch: 1150 => Train Accuraccy: 92.96% || Test Accuracy:94.15% || Loss: 0.20305952221496043\n",
      "Epoch: 1160 => Train Accuraccy: 92.96% || Test Accuracy:94.74% || Loss: 0.20178018709418094\n",
      "Epoch: 1170 => Train Accuraccy: 92.96% || Test Accuracy:94.74% || Loss: 0.2005176500139177\n",
      "Epoch: 1180 => Train Accuraccy: 92.96% || Test Accuracy:94.74% || Loss: 0.19927146547855626\n",
      "Epoch: 1190 => Train Accuraccy: 92.96% || Test Accuracy:94.74% || Loss: 0.19804120732410824\n",
      "Epoch: 1200 => Train Accuraccy: 92.96% || Test Accuracy:94.74% || Loss: 0.19682646776529397\n",
      "Epoch: 1210 => Train Accuraccy: 92.96% || Test Accuracy:94.74% || Loss: 0.19562685649747044\n",
      "Epoch: 1220 => Train Accuraccy: 92.96% || Test Accuracy:94.74% || Loss: 0.1944419998498852\n",
      "Epoch: 1230 => Train Accuraccy: 93.22% || Test Accuracy:94.74% || Loss: 0.1932715399869728\n",
      "Epoch: 1240 => Train Accuraccy: 93.22% || Test Accuracy:94.74% || Loss: 0.19211513415460812\n",
      "Epoch: 1250 => Train Accuraccy: 93.22% || Test Accuracy:94.74% || Loss: 0.19097245396843765\n",
      "Epoch: 1260 => Train Accuraccy: 93.22% || Test Accuracy:94.74% || Loss: 0.18984318474160242\n",
      "Epoch: 1270 => Train Accuraccy: 93.22% || Test Accuracy:94.74% || Loss: 0.18872702484933085\n",
      "Epoch: 1280 => Train Accuraccy: 93.22% || Test Accuracy:94.74% || Loss: 0.18762368512804767\n",
      "Epoch: 1290 => Train Accuraccy: 93.22% || Test Accuracy:94.74% || Loss: 0.186532888306797\n",
      "Epoch: 1300 => Train Accuraccy: 93.22% || Test Accuracy:94.74% || Loss: 0.1854543684689165\n",
      "Epoch: 1310 => Train Accuraccy: 93.22% || Test Accuracy:94.74% || Loss: 0.18438787054203154\n",
      "Epoch: 1320 => Train Accuraccy: 93.22% || Test Accuracy:94.74% || Loss: 0.183333149814563\n",
      "Epoch: 1330 => Train Accuraccy: 93.22% || Test Accuracy:94.74% || Loss: 0.18228997147704668\n",
      "Epoch: 1340 => Train Accuraccy: 93.22% || Test Accuracy:94.74% || Loss: 0.1812581101866813\n",
      "Epoch: 1350 => Train Accuraccy: 93.22% || Test Accuracy:94.74% || Loss: 0.18023734965360444\n",
      "Epoch: 1360 => Train Accuraccy: 93.22% || Test Accuracy:94.74% || Loss: 0.17922748224749918\n",
      "Epoch: 1370 => Train Accuraccy: 93.22% || Test Accuracy:94.74% || Loss: 0.17822830862321287\n",
      "Epoch: 1380 => Train Accuraccy: 93.22% || Test Accuracy:94.74% || Loss: 0.17723963736414905\n",
      "Epoch: 1390 => Train Accuraccy: 93.22% || Test Accuracy:94.74% || Loss: 0.17626128464226837\n",
      "Epoch: 1400 => Train Accuraccy: 93.47% || Test Accuracy:94.74% || Loss: 0.1752930738936053\n",
      "Epoch: 1410 => Train Accuraccy: 93.47% || Test Accuracy:94.74% || Loss: 0.17433483550826692\n",
      "Epoch: 1420 => Train Accuraccy: 93.47% || Test Accuracy:94.74% || Loss: 0.17338640653394824\n",
      "Epoch: 1430 => Train Accuraccy: 93.47% || Test Accuracy:94.74% || Loss: 0.17244763039205066\n",
      "Epoch: 1440 => Train Accuraccy: 93.47% || Test Accuracy:94.74% || Loss: 0.17151835660554754\n",
      "Epoch: 1450 => Train Accuraccy: 93.47% || Test Accuracy:94.74% || Loss: 0.17059844053779363\n",
      "Epoch: 1460 => Train Accuraccy: 93.47% || Test Accuracy:94.74% || Loss: 0.16968774314151874\n",
      "Epoch: 1470 => Train Accuraccy: 93.47% || Test Accuracy:95.32% || Loss: 0.1687861307173046\n",
      "Epoch: 1480 => Train Accuraccy: 93.47% || Test Accuracy:95.32% || Loss: 0.16789347468087626\n",
      "Epoch: 1490 => Train Accuraccy: 93.47% || Test Accuracy:95.32% || Loss: 0.16700965133859458\n",
      "Epoch: 1500 => Train Accuraccy: 93.72% || Test Accuracy:95.91% || Loss: 0.16613454167057082\n",
      "Epoch: 1510 => Train Accuraccy: 93.72% || Test Accuracy:95.91% || Loss: 0.16526803112087185\n",
      "Epoch: 1520 => Train Accuraccy: 93.97% || Test Accuracy:95.91% || Loss: 0.16441000939431868\n",
      "Epoch: 1530 => Train Accuraccy: 93.97% || Test Accuracy:95.91% || Loss: 0.16356037025942785\n",
      "Epoch: 1540 => Train Accuraccy: 93.97% || Test Accuracy:95.91% || Loss: 0.16271901135707673\n",
      "Epoch: 1550 => Train Accuraccy: 93.97% || Test Accuracy:95.91% || Loss: 0.16188583401451367\n",
      "Epoch: 1560 => Train Accuraccy: 94.22% || Test Accuracy:95.91% || Loss: 0.16106074306437554\n",
      "Epoch: 1570 => Train Accuraccy: 94.22% || Test Accuracy:95.32% || Loss: 0.1602436466684061\n",
      "Epoch: 1580 => Train Accuraccy: 94.22% || Test Accuracy:95.91% || Loss: 0.15943445614560592\n",
      "Epoch: 1590 => Train Accuraccy: 94.22% || Test Accuracy:95.91% || Loss: 0.15863308580458158\n",
      "Epoch: 1600 => Train Accuraccy: 94.22% || Test Accuracy:95.91% || Loss: 0.15783945277989467\n",
      "Epoch: 1610 => Train Accuraccy: 94.22% || Test Accuracy:95.91% || Loss: 0.15705347687224794\n",
      "Epoch: 1620 => Train Accuraccy: 94.22% || Test Accuracy:95.91% || Loss: 0.15627508039237098\n",
      "Epoch: 1630 => Train Accuraccy: 94.22% || Test Accuracy:95.91% || Loss: 0.15550418800851096\n",
      "Epoch: 1640 => Train Accuraccy: 94.22% || Test Accuracy:95.91% || Loss: 0.15474072659745408\n",
      "Epoch: 1650 => Train Accuraccy: 94.22% || Test Accuracy:95.91% || Loss: 0.15398462509903846\n",
      "Epoch: 1660 => Train Accuraccy: 94.22% || Test Accuracy:95.91% || Loss: 0.15323581437414843\n",
      "Epoch: 1670 => Train Accuraccy: 94.22% || Test Accuracy:95.91% || Loss: 0.1524942270661984\n",
      "Epoch: 1680 => Train Accuraccy: 94.22% || Test Accuracy:95.91% || Loss: 0.1517597974661498\n",
      "Epoch: 1690 => Train Accuraccy: 94.22% || Test Accuracy:95.91% || Loss: 0.15103246138111656\n",
      "Epoch: 1700 => Train Accuraccy: 94.22% || Test Accuracy:95.91% || Loss: 0.15031215600664458\n",
      "Epoch: 1710 => Train Accuraccy: 94.22% || Test Accuracy:95.91% || Loss: 0.1495988198027569\n",
      "Epoch: 1720 => Train Accuraccy: 94.22% || Test Accuracy:95.91% || Loss: 0.1488923923738847\n",
      "Epoch: 1730 => Train Accuraccy: 94.22% || Test Accuracy:95.91% || Loss: 0.1481928143528041\n",
      "Epoch: 1740 => Train Accuraccy: 94.22% || Test Accuracy:95.91% || Loss: 0.14750002728871864\n",
      "Epoch: 1750 => Train Accuraccy: 94.22% || Test Accuracy:95.91% || Loss: 0.14681397353962644\n",
      "Epoch: 1760 => Train Accuraccy: 94.22% || Test Accuracy:95.91% || Loss: 0.14613459616911922\n",
      "Epoch: 1770 => Train Accuraccy: 94.47% || Test Accuracy:95.91% || Loss: 0.14546183884776132\n",
      "Epoch: 1780 => Train Accuraccy: 94.47% || Test Accuracy:96.49% || Loss: 0.14479564575918913\n",
      "Epoch: 1790 => Train Accuraccy: 94.47% || Test Accuracy:96.49% || Loss: 0.1441359615110752\n",
      "Epoch: 1800 => Train Accuraccy: 94.72% || Test Accuracy:96.49% || Loss: 0.14348273105108217\n",
      "Epoch: 1810 => Train Accuraccy: 94.72% || Test Accuracy:96.49% || Loss: 0.14283589958793202\n",
      "Epoch: 1820 => Train Accuraccy: 94.72% || Test Accuracy:96.49% || Loss: 0.14219541251769166\n",
      "Epoch: 1830 => Train Accuraccy: 94.72% || Test Accuracy:96.49% || Loss: 0.14156121535536778\n",
      "Epoch: 1840 => Train Accuraccy: 94.72% || Test Accuracy:96.49% || Loss: 0.14093325367188517\n",
      "Epoch: 1850 => Train Accuraccy: 94.72% || Test Accuracy:96.49% || Loss: 0.1403114730364925\n",
      "Epoch: 1860 => Train Accuraccy: 94.72% || Test Accuracy:96.49% || Loss: 0.13969581896463543\n",
      "Epoch: 1870 => Train Accuraccy: 94.72% || Test Accuracy:96.49% || Loss: 0.1390862368712951\n",
      "Epoch: 1880 => Train Accuraccy: 94.72% || Test Accuracy:96.49% || Loss: 0.138482672029781\n",
      "Epoch: 1890 => Train Accuraccy: 94.72% || Test Accuracy:96.49% || Loss: 0.13788506953593213\n",
      "Epoch: 1900 => Train Accuraccy: 94.72% || Test Accuracy:96.49% || Loss: 0.13729337427766095\n",
      "Epoch: 1910 => Train Accuraccy: 94.72% || Test Accuracy:96.49% || Loss: 0.13670753090974988\n",
      "Epoch: 1920 => Train Accuraccy: 94.72% || Test Accuracy:96.49% || Loss: 0.13612748383377993\n",
      "Epoch: 1930 => Train Accuraccy: 94.72% || Test Accuracy:96.49% || Loss: 0.13555317718305468\n",
      "Epoch: 1940 => Train Accuraccy: 94.72% || Test Accuracy:96.49% || Loss: 0.13498455481235633\n",
      "Epoch: 1950 => Train Accuraccy: 94.72% || Test Accuracy:96.49% || Loss: 0.13442156029234756\n",
      "Epoch: 1960 => Train Accuraccy: 94.72% || Test Accuracy:96.49% || Loss: 0.1338641369084188\n",
      "Epoch: 1970 => Train Accuraccy: 94.72% || Test Accuracy:96.49% || Loss: 0.13331222766375583\n",
      "Epoch: 1980 => Train Accuraccy: 94.72% || Test Accuracy:96.49% || Loss: 0.1327657752863926\n",
      "Epoch: 1990 => Train Accuraccy: 94.72% || Test Accuracy:96.49% || Loss: 0.1322247222399983\n",
      "Epoch: 2000 => Train Accuraccy: 94.72% || Test Accuracy:96.49% || Loss: 0.13168901073813344\n",
      "Epoch: 2010 => Train Accuraccy: 94.72% || Test Accuracy:96.49% || Loss: 0.13115858276170775\n",
      "Epoch: 2020 => Train Accuraccy: 94.72% || Test Accuracy:96.49% || Loss: 0.13063338007935962\n",
      "Epoch: 2030 => Train Accuraccy: 94.72% || Test Accuracy:96.49% || Loss: 0.13011334427047372\n",
      "Epoch: 2040 => Train Accuraccy: 94.72% || Test Accuracy:96.49% || Loss: 0.12959841675055686\n",
      "Epoch: 2050 => Train Accuraccy: 94.72% || Test Accuracy:96.49% || Loss: 0.12908853879868482\n",
      "Epoch: 2060 => Train Accuraccy: 94.72% || Test Accuracy:96.49% || Loss: 0.12858365158674132\n",
      "Epoch: 2070 => Train Accuraccy: 94.72% || Test Accuracy:96.49% || Loss: 0.12808369621017324\n",
      "Epoch: 2080 => Train Accuraccy: 94.72% || Test Accuracy:96.49% || Loss: 0.12758861371999128\n",
      "Epoch: 2090 => Train Accuraccy: 94.72% || Test Accuracy:96.49% || Loss: 0.12709834515575621\n",
      "Epoch: 2100 => Train Accuraccy: 94.72% || Test Accuracy:96.49% || Loss: 0.12661283157929928\n",
      "Epoch: 2110 => Train Accuraccy: 94.72% || Test Accuracy:96.49% || Loss: 0.12613201410893515\n",
      "Epoch: 2120 => Train Accuraccy: 94.97% || Test Accuracy:96.49% || Loss: 0.12565583395394236\n",
      "Epoch: 2130 => Train Accuraccy: 94.97% || Test Accuracy:96.49% || Loss: 0.12518423244909435\n",
      "Epoch: 2140 => Train Accuraccy: 94.97% || Test Accuracy:96.49% || Loss: 0.1247171510890422\n",
      "Epoch: 2150 => Train Accuraccy: 94.97% || Test Accuracy:96.49% || Loss: 0.12425453156236368\n",
      "Epoch: 2160 => Train Accuraccy: 94.97% || Test Accuracy:96.49% || Loss: 0.12379631578510641\n",
      "Epoch: 2170 => Train Accuraccy: 94.97% || Test Accuracy:96.49% || Loss: 0.12334244593367166\n",
      "Epoch: 2180 => Train Accuraccy: 94.97% || Test Accuracy:96.49% || Loss: 0.12289286447689816\n",
      "Epoch: 2190 => Train Accuraccy: 94.97% || Test Accuracy:96.49% || Loss: 0.12244751420721946\n",
      "Epoch: 2200 => Train Accuraccy: 94.97% || Test Accuracy:96.49% || Loss: 0.12200633827078801\n",
      "Epoch: 2210 => Train Accuraccy: 94.97% || Test Accuracy:96.49% || Loss: 0.12156928019646752\n",
      "Epoch: 2220 => Train Accuraccy: 94.97% || Test Accuracy:96.49% || Loss: 0.12113628392361389\n",
      "Epoch: 2230 => Train Accuraccy: 94.97% || Test Accuracy:96.49% || Loss: 0.12070729382857696\n",
      "Epoch: 2240 => Train Accuraccy: 94.97% || Test Accuracy:96.49% || Loss: 0.12028225474986791\n",
      "Epoch: 2250 => Train Accuraccy: 94.97% || Test Accuracy:96.49% || Loss: 0.11986111201194925\n",
      "Epoch: 2260 => Train Accuraccy: 94.97% || Test Accuracy:96.49% || Loss: 0.11944381144761661\n",
      "Epoch: 2270 => Train Accuraccy: 95.23% || Test Accuracy:96.49% || Loss: 0.11903029941895296\n",
      "Epoch: 2280 => Train Accuraccy: 95.23% || Test Accuracy:96.49% || Loss: 0.1186205228368415\n",
      "Epoch: 2290 => Train Accuraccy: 95.23% || Test Accuracy:96.49% || Loss: 0.11821442917903943\n",
      "Epoch: 2300 => Train Accuraccy: 95.23% || Test Accuracy:96.49% || Loss: 0.11781196650681641\n",
      "Epoch: 2310 => Train Accuraccy: 95.23% || Test Accuracy:96.49% || Loss: 0.11741308348017306\n",
      "Epoch: 2320 => Train Accuraccy: 95.23% || Test Accuracy:96.49% || Loss: 0.11701772937166043\n",
      "Epoch: 2330 => Train Accuraccy: 95.23% || Test Accuracy:96.49% || Loss: 0.1166258540788267\n",
      "Epoch: 2340 => Train Accuraccy: 95.23% || Test Accuracy:96.49% || Loss: 0.11623740813532286\n",
      "Epoch: 2350 => Train Accuraccy: 95.23% || Test Accuracy:96.49% || Loss: 0.1158523427207046\n",
      "Epoch: 2360 => Train Accuraccy: 95.23% || Test Accuracy:96.49% || Loss: 0.11547060966896944\n",
      "Epoch: 2370 => Train Accuraccy: 95.23% || Test Accuracy:96.49% || Loss: 0.11509216147587333\n",
      "Epoch: 2380 => Train Accuraccy: 95.23% || Test Accuracy:96.49% || Loss: 0.11471695130507244\n",
      "Epoch: 2390 => Train Accuraccy: 95.23% || Test Accuracy:96.49% || Loss: 0.11434493299313774\n",
      "Epoch: 2400 => Train Accuraccy: 95.23% || Test Accuracy:96.49% || Loss: 0.11397606105349234\n",
      "Epoch: 2410 => Train Accuraccy: 95.23% || Test Accuracy:96.49% || Loss: 0.11361029067932199\n",
      "Epoch: 2420 => Train Accuraccy: 95.23% || Test Accuracy:96.49% || Loss: 0.11324757774551038\n",
      "Epoch: 2430 => Train Accuraccy: 95.23% || Test Accuracy:96.49% || Loss: 0.11288787880965033\n",
      "Epoch: 2440 => Train Accuraccy: 95.48% || Test Accuracy:96.49% || Loss: 0.11253115111218212\n",
      "Epoch: 2450 => Train Accuraccy: 95.48% || Test Accuracy:96.49% || Loss: 0.11217735257571188\n",
      "Epoch: 2460 => Train Accuraccy: 95.48% || Test Accuracy:96.49% || Loss: 0.11182644180355816\n",
      "Epoch: 2470 => Train Accuraccy: 95.48% || Test Accuracy:96.49% || Loss: 0.11147837807757807\n",
      "Epoch: 2480 => Train Accuraccy: 95.48% || Test Accuracy:96.49% || Loss: 0.11113312135532127\n",
      "Epoch: 2490 => Train Accuraccy: 95.48% || Test Accuracy:96.49% || Loss: 0.11079063226655869\n",
      "Epoch: 2500 => Train Accuraccy: 95.48% || Test Accuracy:96.49% || Loss: 0.11045087210923324\n",
      "Epoch: 2510 => Train Accuraccy: 95.48% || Test Accuracy:96.49% || Loss: 0.11011380284487564\n",
      "Epoch: 2520 => Train Accuraccy: 95.48% || Test Accuracy:96.49% || Loss: 0.10977938709352951\n",
      "Epoch: 2530 => Train Accuraccy: 95.48% || Test Accuracy:96.49% || Loss: 0.10944758812822702\n",
      "Epoch: 2540 => Train Accuraccy: 95.73% || Test Accuracy:96.49% || Loss: 0.10911836986905397\n",
      "Epoch: 2550 => Train Accuraccy: 95.73% || Test Accuracy:96.49% || Loss: 0.10879169687684304\n",
      "Epoch: 2560 => Train Accuraccy: 95.73% || Test Accuracy:96.49% || Loss: 0.10846753434653142\n",
      "Epoch: 2570 => Train Accuraccy: 95.73% || Test Accuracy:96.49% || Loss: 0.10814584810021641\n",
      "Epoch: 2580 => Train Accuraccy: 95.73% || Test Accuracy:96.49% || Loss: 0.1078266045799429\n",
      "Epoch: 2590 => Train Accuraccy: 95.73% || Test Accuracy:96.49% || Loss: 0.10750977084025241\n",
      "Epoch: 2600 => Train Accuraccy: 95.73% || Test Accuracy:96.49% || Loss: 0.10719531454052367\n",
      "Epoch: 2610 => Train Accuraccy: 95.73% || Test Accuracy:96.49% || Loss: 0.10688320393713213\n",
      "Epoch: 2620 => Train Accuraccy: 95.98% || Test Accuracy:96.49% || Loss: 0.10657340787545395\n",
      "Epoch: 2630 => Train Accuraccy: 95.98% || Test Accuracy:96.49% || Loss: 0.10626589578173906\n",
      "Epoch: 2640 => Train Accuraccy: 95.73% || Test Accuracy:96.49% || Loss: 0.10596063765487551\n",
      "Epoch: 2650 => Train Accuraccy: 95.73% || Test Accuracy:96.49% || Loss: 0.10565760405806689\n",
      "Epoch: 2660 => Train Accuraccy: 95.73% || Test Accuracy:96.49% || Loss: 0.10535676611044191\n",
      "Epoch: 2670 => Train Accuraccy: 95.73% || Test Accuracy:96.49% || Loss: 0.10505809547861499\n",
      "Epoch: 2680 => Train Accuraccy: 95.73% || Test Accuracy:96.49% || Loss: 0.10476156436821407\n",
      "Epoch: 2690 => Train Accuraccy: 95.73% || Test Accuracy:96.49% || Loss: 0.10446714551539207\n",
      "Epoch: 2700 => Train Accuraccy: 95.73% || Test Accuracy:96.49% || Loss: 0.10417481217833637\n",
      "Epoch: 2710 => Train Accuraccy: 95.73% || Test Accuracy:96.49% || Loss: 0.10388453812878873\n",
      "Epoch: 2720 => Train Accuraccy: 95.73% || Test Accuracy:96.49% || Loss: 0.1035962976435887\n",
      "Epoch: 2730 => Train Accuraccy: 95.73% || Test Accuracy:96.49% || Loss: 0.1033100654962514\n",
      "Epoch: 2740 => Train Accuraccy: 95.73% || Test Accuracy:96.49% || Loss: 0.10302581694858966\n",
      "Epoch: 2750 => Train Accuraccy: 95.73% || Test Accuracy:96.49% || Loss: 0.10274352774238936\n",
      "Epoch: 2760 => Train Accuraccy: 95.73% || Test Accuracy:96.49% || Loss: 0.10246317409114689\n",
      "Epoch: 2770 => Train Accuraccy: 95.73% || Test Accuracy:96.49% || Loss: 0.10218473267187585\n",
      "Epoch: 2780 => Train Accuraccy: 95.98% || Test Accuracy:96.49% || Loss: 0.10190818061698917\n",
      "Epoch: 2790 => Train Accuraccy: 95.98% || Test Accuracy:96.49% || Loss: 0.10163349550626329\n",
      "Epoch: 2800 => Train Accuraccy: 96.23% || Test Accuracy:96.49% || Loss: 0.1013606553588895\n",
      "Epoch: 2810 => Train Accuraccy: 96.23% || Test Accuracy:96.49% || Loss: 0.10108963862561582\n",
      "Epoch: 2820 => Train Accuraccy: 96.23% || Test Accuracy:96.49% || Loss: 0.10082042418098544\n",
      "Epoch: 2830 => Train Accuraccy: 96.23% || Test Accuracy:96.49% || Loss: 0.1005529913156727\n",
      "Epoch: 2840 => Train Accuraccy: 96.23% || Test Accuracy:96.49% || Loss: 0.10028731972892185\n",
      "Epoch: 2850 => Train Accuraccy: 96.23% || Test Accuracy:96.49% || Loss: 0.10002338952108876\n",
      "Epoch: 2860 => Train Accuraccy: 96.23% || Test Accuracy:96.49% || Loss: 0.09976118118628895\n",
      "Epoch: 2870 => Train Accuraccy: 96.23% || Test Accuracy:96.49% || Loss: 0.09950067560515283\n",
      "Epoch: 2880 => Train Accuraccy: 96.23% || Test Accuracy:96.49% || Loss: 0.09924185403768923\n",
      "Epoch: 2890 => Train Accuraccy: 96.23% || Test Accuracy:96.49% || Loss: 0.09898469811625804\n",
      "Epoch: 2900 => Train Accuraccy: 96.23% || Test Accuracy:96.49% || Loss: 0.09872918983865224\n",
      "Epoch: 2910 => Train Accuraccy: 96.23% || Test Accuracy:96.49% || Loss: 0.09847531156128946\n",
      "Epoch: 2920 => Train Accuraccy: 96.23% || Test Accuracy:96.49% || Loss: 0.09822304599251278\n",
      "Epoch: 2930 => Train Accuraccy: 96.23% || Test Accuracy:96.49% || Loss: 0.09797237618600059\n",
      "Epoch: 2940 => Train Accuraccy: 96.23% || Test Accuracy:96.49% || Loss: 0.09772328553428415\n",
      "Epoch: 2950 => Train Accuraccy: 96.23% || Test Accuracy:96.49% || Loss: 0.09747575776237247\n",
      "Epoch: 2960 => Train Accuraccy: 96.23% || Test Accuracy:96.49% || Loss: 0.0972297769214836\n",
      "Epoch: 2970 => Train Accuraccy: 96.23% || Test Accuracy:96.49% || Loss: 0.09698532738288004\n",
      "Epoch: 2980 => Train Accuraccy: 96.23% || Test Accuracy:96.49% || Loss: 0.09674239383180787\n",
      "Epoch: 2990 => Train Accuraccy: 96.23% || Test Accuracy:96.49% || Loss: 0.09650096126153747\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(30, 1), dtype=float64, numpy=\n",
       " array([[-0.25084197],\n",
       "        [-0.15146324],\n",
       "        [-2.82301782],\n",
       "        [ 0.66959668],\n",
       "        [-0.41838434],\n",
       "        [ 0.01458125],\n",
       "        [-0.68078268],\n",
       "        [ 0.03781221],\n",
       "        [-0.08904689],\n",
       "        [ 1.04804182],\n",
       "        [-0.45297844],\n",
       "        [-0.44669423],\n",
       "        [ 0.33499866],\n",
       "        [-0.16190907],\n",
       "        [ 0.64957119],\n",
       "        [-1.06896282],\n",
       "        [ 0.45315059],\n",
       "        [ 0.68547201],\n",
       "        [-0.82396279],\n",
       "        [ 0.46035011],\n",
       "        [-0.6020451 ],\n",
       "        [-1.04452213],\n",
       "        [-1.89603313],\n",
       "        [-1.99868466],\n",
       "        [-0.91446856],\n",
       "        [ 1.95683669],\n",
       "        [-0.46393393],\n",
       "        [-0.48636968],\n",
       "        [ 0.38856896],\n",
       "        [-3.07470883]])>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=0.4582427592315207>,\n",
       " {'dw': <tf.Tensor: shape=(30, 1), dtype=float64, numpy=\n",
       "  array([[ 0.00395511],\n",
       "         [ 0.00578741],\n",
       "         [ 0.00378598],\n",
       "         [ 0.00370301],\n",
       "         [ 0.00850618],\n",
       "         [-0.00286514],\n",
       "         [ 0.00586841],\n",
       "         [ 0.00920118],\n",
       "         [ 0.00367545],\n",
       "         [-0.01263389],\n",
       "         [ 0.00181697],\n",
       "         [-0.00997552],\n",
       "         [-0.00014101],\n",
       "         [ 0.00204213],\n",
       "         [ 0.02747695],\n",
       "         [-0.00666671],\n",
       "         [ 0.00240764],\n",
       "         [ 0.01417782],\n",
       "         [ 0.00423511],\n",
       "         [-0.01392706],\n",
       "         [ 0.00522776],\n",
       "         [ 0.00642986],\n",
       "         [ 0.0042464 ],\n",
       "         [ 0.00435325],\n",
       "         [ 0.01662989],\n",
       "         [-0.00444914],\n",
       "         [ 0.00157411],\n",
       "         [ 0.01081444],\n",
       "         [ 0.00911162],\n",
       "         [-0.01616507]])>,\n",
       "  'db': <tf.Tensor: shape=(), dtype=float64, numpy=0.0042685895973516895>},\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=0.0962849431274062>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GradienDescent(xtrain.shape[0])\n",
    "model.optimize(xtrain, ytrain, validation=(xtest, ytest),\n",
    "               num_iterations=3000, learning_rate=0.009)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(model.metric[0], color='darkorange')\n",
    "plt.plot(model.metric[1], color='lightgreen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w': <tf.Tensor: shape=(30, 1), dtype=float64, numpy=\n",
       " array([[-0.25084197],\n",
       "        [-0.15146324],\n",
       "        [-2.82301782],\n",
       "        [ 0.66959668],\n",
       "        [-0.41838434],\n",
       "        [ 0.01458125],\n",
       "        [-0.68078268],\n",
       "        [ 0.03781221],\n",
       "        [-0.08904689],\n",
       "        [ 1.04804182],\n",
       "        [-0.45297844],\n",
       "        [-0.44669423],\n",
       "        [ 0.33499866],\n",
       "        [-0.16190907],\n",
       "        [ 0.64957119],\n",
       "        [-1.06896282],\n",
       "        [ 0.45315059],\n",
       "        [ 0.68547201],\n",
       "        [-0.82396279],\n",
       "        [ 0.46035011],\n",
       "        [-0.6020451 ],\n",
       "        [-1.04452213],\n",
       "        [-1.89603313],\n",
       "        [-1.99868466],\n",
       "        [-0.91446856],\n",
       "        [ 1.95683669],\n",
       "        [-0.46393393],\n",
       "        [-0.48636968],\n",
       "        [ 0.38856896],\n",
       "        [-3.07470883]])>,\n",
       " 'b': <tf.Tensor: shape=(), dtype=float64, numpy=0.4582427592315207>}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(171,), dtype=bool, numpy=\n",
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False, False,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True])>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred = model.predict(xtest)[0]\n",
    "ypred == ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0,\n",
       "  20.35175879396985,\n",
       "  22.8643216080402,\n",
       "  26.130653266331663,\n",
       "  30.402010050251263,\n",
       "  34.92462311557789,\n",
       "  38.19095477386934,\n",
       "  43.71859296482412,\n",
       "  48.492462311557794,\n",
       "  51.75879396984925,\n",
       "  55.0251256281407,\n",
       "  59.54773869346734,\n",
       "  63.31658291457287,\n",
       "  65.32663316582915,\n",
       "  67.8391959798995,\n",
       "  68.84422110552764,\n",
       "  70.60301507537689,\n",
       "  71.85929648241206,\n",
       "  73.61809045226131,\n",
       "  75.12562814070353,\n",
       "  75.37688442211055,\n",
       "  76.3819095477387,\n",
       "  77.63819095477388,\n",
       "  79.64824120603015,\n",
       "  80.40201005025125,\n",
       "  81.90954773869346,\n",
       "  82.66331658291458,\n",
       "  83.66834170854271,\n",
       "  84.17085427135679,\n",
       "  84.67336683417085,\n",
       "  84.67336683417085,\n",
       "  84.67336683417085,\n",
       "  84.92462311557789,\n",
       "  86.18090452261306,\n",
       "  86.4321608040201,\n",
       "  86.93467336683418,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.1859296482412,\n",
       "  87.43718592964824,\n",
       "  87.93969849246231,\n",
       "  88.19095477386935,\n",
       "  88.44221105527637,\n",
       "  88.69346733668341,\n",
       "  88.69346733668341,\n",
       "  88.94472361809045,\n",
       "  88.94472361809045,\n",
       "  89.19597989949749,\n",
       "  89.44723618090453,\n",
       "  89.69849246231156,\n",
       "  89.69849246231156,\n",
       "  89.69849246231156,\n",
       "  89.69849246231156,\n",
       "  89.69849246231156,\n",
       "  89.69849246231156,\n",
       "  89.69849246231156,\n",
       "  89.69849246231156,\n",
       "  89.69849246231156,\n",
       "  89.69849246231156,\n",
       "  89.94974874371859,\n",
       "  89.94974874371859,\n",
       "  89.94974874371859,\n",
       "  89.94974874371859,\n",
       "  89.94974874371859,\n",
       "  90.20100502512562,\n",
       "  90.45226130653266,\n",
       "  90.45226130653266,\n",
       "  90.20100502512562,\n",
       "  90.20100502512562,\n",
       "  90.45226130653266,\n",
       "  90.45226130653266,\n",
       "  90.45226130653266,\n",
       "  90.45226130653266,\n",
       "  90.7035175879397,\n",
       "  90.7035175879397,\n",
       "  90.7035175879397,\n",
       "  90.7035175879397,\n",
       "  90.7035175879397,\n",
       "  90.7035175879397,\n",
       "  90.7035175879397,\n",
       "  90.7035175879397,\n",
       "  90.7035175879397,\n",
       "  91.20603015075378,\n",
       "  91.4572864321608,\n",
       "  91.4572864321608,\n",
       "  91.4572864321608,\n",
       "  91.4572864321608,\n",
       "  91.4572864321608,\n",
       "  91.4572864321608,\n",
       "  91.4572864321608,\n",
       "  91.4572864321608,\n",
       "  91.4572864321608,\n",
       "  91.4572864321608,\n",
       "  91.4572864321608,\n",
       "  91.70854271356784,\n",
       "  91.95979899497488,\n",
       "  91.95979899497488,\n",
       "  91.95979899497488,\n",
       "  91.95979899497488,\n",
       "  92.46231155778895,\n",
       "  92.71356783919597,\n",
       "  92.71356783919597,\n",
       "  92.71356783919597,\n",
       "  92.71356783919597,\n",
       "  92.71356783919597,\n",
       "  92.71356783919597,\n",
       "  92.71356783919597,\n",
       "  92.96482412060301,\n",
       "  92.96482412060301,\n",
       "  92.96482412060301,\n",
       "  92.96482412060301,\n",
       "  92.96482412060301,\n",
       "  92.96482412060301,\n",
       "  92.96482412060301,\n",
       "  92.96482412060301,\n",
       "  92.96482412060301,\n",
       "  92.96482412060301,\n",
       "  92.96482412060301,\n",
       "  92.96482412060301,\n",
       "  93.21608040201005,\n",
       "  93.21608040201005,\n",
       "  93.21608040201005,\n",
       "  93.21608040201005,\n",
       "  93.21608040201005,\n",
       "  93.21608040201005,\n",
       "  93.21608040201005,\n",
       "  93.21608040201005,\n",
       "  93.21608040201005,\n",
       "  93.21608040201005,\n",
       "  93.21608040201005,\n",
       "  93.21608040201005,\n",
       "  93.21608040201005,\n",
       "  93.21608040201005,\n",
       "  93.21608040201005,\n",
       "  93.21608040201005,\n",
       "  93.21608040201005,\n",
       "  93.46733668341709,\n",
       "  93.46733668341709,\n",
       "  93.46733668341709,\n",
       "  93.46733668341709,\n",
       "  93.46733668341709,\n",
       "  93.46733668341709,\n",
       "  93.46733668341709,\n",
       "  93.46733668341709,\n",
       "  93.46733668341709,\n",
       "  93.46733668341709,\n",
       "  93.71859296482413,\n",
       "  93.71859296482413,\n",
       "  93.96984924623115,\n",
       "  93.96984924623115,\n",
       "  93.96984924623115,\n",
       "  93.96984924623115,\n",
       "  94.22110552763819,\n",
       "  94.22110552763819,\n",
       "  94.22110552763819,\n",
       "  94.22110552763819,\n",
       "  94.22110552763819,\n",
       "  94.22110552763819,\n",
       "  94.22110552763819,\n",
       "  94.22110552763819,\n",
       "  94.22110552763819,\n",
       "  94.22110552763819,\n",
       "  94.22110552763819,\n",
       "  94.22110552763819,\n",
       "  94.22110552763819,\n",
       "  94.22110552763819,\n",
       "  94.22110552763819,\n",
       "  94.22110552763819,\n",
       "  94.22110552763819,\n",
       "  94.22110552763819,\n",
       "  94.22110552763819,\n",
       "  94.22110552763819,\n",
       "  94.22110552763819,\n",
       "  94.47236180904522,\n",
       "  94.47236180904522,\n",
       "  94.47236180904522,\n",
       "  94.72361809045226,\n",
       "  94.72361809045226,\n",
       "  94.72361809045226,\n",
       "  94.72361809045226,\n",
       "  94.72361809045226,\n",
       "  94.72361809045226,\n",
       "  94.72361809045226,\n",
       "  94.72361809045226,\n",
       "  94.72361809045226,\n",
       "  94.72361809045226,\n",
       "  94.72361809045226,\n",
       "  94.72361809045226,\n",
       "  94.72361809045226,\n",
       "  94.72361809045226,\n",
       "  94.72361809045226,\n",
       "  94.72361809045226,\n",
       "  94.72361809045226,\n",
       "  94.72361809045226,\n",
       "  94.72361809045226,\n",
       "  94.72361809045226,\n",
       "  94.72361809045226,\n",
       "  94.72361809045226,\n",
       "  94.72361809045226,\n",
       "  94.72361809045226,\n",
       "  94.72361809045226,\n",
       "  94.72361809045226,\n",
       "  94.72361809045226,\n",
       "  94.72361809045226,\n",
       "  94.72361809045226,\n",
       "  94.72361809045226,\n",
       "  94.72361809045226,\n",
       "  94.72361809045226,\n",
       "  94.9748743718593,\n",
       "  94.9748743718593,\n",
       "  94.9748743718593,\n",
       "  94.9748743718593,\n",
       "  94.9748743718593,\n",
       "  94.9748743718593,\n",
       "  94.9748743718593,\n",
       "  94.9748743718593,\n",
       "  94.9748743718593,\n",
       "  94.9748743718593,\n",
       "  94.9748743718593,\n",
       "  94.9748743718593,\n",
       "  94.9748743718593,\n",
       "  94.9748743718593,\n",
       "  94.9748743718593,\n",
       "  95.22613065326634,\n",
       "  95.22613065326634,\n",
       "  95.22613065326634,\n",
       "  95.22613065326634,\n",
       "  95.22613065326634,\n",
       "  95.22613065326634,\n",
       "  95.22613065326634,\n",
       "  95.22613065326634,\n",
       "  95.22613065326634,\n",
       "  95.22613065326634,\n",
       "  95.22613065326634,\n",
       "  95.22613065326634,\n",
       "  95.22613065326634,\n",
       "  95.22613065326634,\n",
       "  95.22613065326634,\n",
       "  95.22613065326634,\n",
       "  95.22613065326634,\n",
       "  95.47738693467336,\n",
       "  95.47738693467336,\n",
       "  95.47738693467336,\n",
       "  95.47738693467336,\n",
       "  95.47738693467336,\n",
       "  95.47738693467336,\n",
       "  95.47738693467336,\n",
       "  95.47738693467336,\n",
       "  95.47738693467336,\n",
       "  95.47738693467336,\n",
       "  95.7286432160804,\n",
       "  95.7286432160804,\n",
       "  95.7286432160804,\n",
       "  95.7286432160804,\n",
       "  95.7286432160804,\n",
       "  95.7286432160804,\n",
       "  95.7286432160804,\n",
       "  95.7286432160804,\n",
       "  95.97989949748744,\n",
       "  95.97989949748744,\n",
       "  95.7286432160804,\n",
       "  95.7286432160804,\n",
       "  95.7286432160804,\n",
       "  95.7286432160804,\n",
       "  95.7286432160804,\n",
       "  95.7286432160804,\n",
       "  95.7286432160804,\n",
       "  95.7286432160804,\n",
       "  95.7286432160804,\n",
       "  95.7286432160804,\n",
       "  95.7286432160804,\n",
       "  95.7286432160804,\n",
       "  95.7286432160804,\n",
       "  95.7286432160804,\n",
       "  95.97989949748744,\n",
       "  95.97989949748744,\n",
       "  96.23115577889448,\n",
       "  96.23115577889448,\n",
       "  96.23115577889448,\n",
       "  96.23115577889448,\n",
       "  96.23115577889448,\n",
       "  96.23115577889448,\n",
       "  96.23115577889448,\n",
       "  96.23115577889448,\n",
       "  96.23115577889448,\n",
       "  96.23115577889448,\n",
       "  96.23115577889448,\n",
       "  96.23115577889448,\n",
       "  96.23115577889448,\n",
       "  96.23115577889448,\n",
       "  96.23115577889448,\n",
       "  96.23115577889448,\n",
       "  96.23115577889448,\n",
       "  96.23115577889448,\n",
       "  96.23115577889448,\n",
       "  96.23115577889448],\n",
       " [0.0,\n",
       "  23.391812865497073,\n",
       "  26.31578947368422,\n",
       "  29.239766081871338,\n",
       "  35.08771929824562,\n",
       "  40.35087719298246,\n",
       "  45.02923976608187,\n",
       "  50.877192982456144,\n",
       "  56.140350877192986,\n",
       "  59.06432748538012,\n",
       "  63.15789473684211,\n",
       "  65.49707602339181,\n",
       "  67.83625730994152,\n",
       "  70.76023391812865,\n",
       "  73.6842105263158,\n",
       "  75.43859649122807,\n",
       "  76.0233918128655,\n",
       "  76.60818713450293,\n",
       "  77.19298245614036,\n",
       "  78.3625730994152,\n",
       "  78.3625730994152,\n",
       "  80.70175438596492,\n",
       "  81.28654970760235,\n",
       "  82.45614035087719,\n",
       "  84.21052631578948,\n",
       "  84.21052631578948,\n",
       "  84.21052631578948,\n",
       "  84.7953216374269,\n",
       "  84.7953216374269,\n",
       "  84.7953216374269,\n",
       "  85.38011695906432,\n",
       "  85.38011695906432,\n",
       "  85.38011695906432,\n",
       "  85.96491228070175,\n",
       "  86.54970760233918,\n",
       "  86.54970760233918,\n",
       "  86.54970760233918,\n",
       "  87.13450292397661,\n",
       "  87.13450292397661,\n",
       "  87.71929824561404,\n",
       "  87.71929824561404,\n",
       "  88.88888888888889,\n",
       "  89.47368421052632,\n",
       "  90.05847953216374,\n",
       "  90.05847953216374,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  90.64327485380117,\n",
       "  91.2280701754386,\n",
       "  91.2280701754386,\n",
       "  91.2280701754386,\n",
       "  91.2280701754386,\n",
       "  91.2280701754386,\n",
       "  91.2280701754386,\n",
       "  91.2280701754386,\n",
       "  91.2280701754386,\n",
       "  91.2280701754386,\n",
       "  91.2280701754386,\n",
       "  91.2280701754386,\n",
       "  91.2280701754386,\n",
       "  91.2280701754386,\n",
       "  91.81286549707602,\n",
       "  91.81286549707602,\n",
       "  91.81286549707602,\n",
       "  91.81286549707602,\n",
       "  91.81286549707602,\n",
       "  91.81286549707602,\n",
       "  91.81286549707602,\n",
       "  92.39766081871345,\n",
       "  92.39766081871345,\n",
       "  92.39766081871345,\n",
       "  92.39766081871345,\n",
       "  92.39766081871345,\n",
       "  92.39766081871345,\n",
       "  92.39766081871345,\n",
       "  92.39766081871345,\n",
       "  92.39766081871345,\n",
       "  92.39766081871345,\n",
       "  92.39766081871345,\n",
       "  92.39766081871345,\n",
       "  92.39766081871345,\n",
       "  92.39766081871345,\n",
       "  92.39766081871345,\n",
       "  92.98245614035088,\n",
       "  92.98245614035088,\n",
       "  92.98245614035088,\n",
       "  92.98245614035088,\n",
       "  92.98245614035088,\n",
       "  92.98245614035088,\n",
       "  92.98245614035088,\n",
       "  92.98245614035088,\n",
       "  92.98245614035088,\n",
       "  92.98245614035088,\n",
       "  92.98245614035088,\n",
       "  92.98245614035088,\n",
       "  92.98245614035088,\n",
       "  93.5672514619883,\n",
       "  93.5672514619883,\n",
       "  93.5672514619883,\n",
       "  93.5672514619883,\n",
       "  93.5672514619883,\n",
       "  93.5672514619883,\n",
       "  93.5672514619883,\n",
       "  93.5672514619883,\n",
       "  94.15204678362574,\n",
       "  94.15204678362574,\n",
       "  94.15204678362574,\n",
       "  94.15204678362574,\n",
       "  94.15204678362574,\n",
       "  94.15204678362574,\n",
       "  94.15204678362574,\n",
       "  94.73684210526316,\n",
       "  94.73684210526316,\n",
       "  94.73684210526316,\n",
       "  94.73684210526316,\n",
       "  94.73684210526316,\n",
       "  94.73684210526316,\n",
       "  94.73684210526316,\n",
       "  94.73684210526316,\n",
       "  94.73684210526316,\n",
       "  94.73684210526316,\n",
       "  94.73684210526316,\n",
       "  94.73684210526316,\n",
       "  94.73684210526316,\n",
       "  94.73684210526316,\n",
       "  94.73684210526316,\n",
       "  94.73684210526316,\n",
       "  94.73684210526316,\n",
       "  94.73684210526316,\n",
       "  94.73684210526316,\n",
       "  94.73684210526316,\n",
       "  94.73684210526316,\n",
       "  94.73684210526316,\n",
       "  94.73684210526316,\n",
       "  94.73684210526316,\n",
       "  94.73684210526316,\n",
       "  94.73684210526316,\n",
       "  94.73684210526316,\n",
       "  94.73684210526316,\n",
       "  94.73684210526316,\n",
       "  94.73684210526316,\n",
       "  94.73684210526316,\n",
       "  95.32163742690058,\n",
       "  95.32163742690058,\n",
       "  95.32163742690058,\n",
       "  95.90643274853801,\n",
       "  95.90643274853801,\n",
       "  95.90643274853801,\n",
       "  95.90643274853801,\n",
       "  95.90643274853801,\n",
       "  95.90643274853801,\n",
       "  95.90643274853801,\n",
       "  95.32163742690058,\n",
       "  95.90643274853801,\n",
       "  95.90643274853801,\n",
       "  95.90643274853801,\n",
       "  95.90643274853801,\n",
       "  95.90643274853801,\n",
       "  95.90643274853801,\n",
       "  95.90643274853801,\n",
       "  95.90643274853801,\n",
       "  95.90643274853801,\n",
       "  95.90643274853801,\n",
       "  95.90643274853801,\n",
       "  95.90643274853801,\n",
       "  95.90643274853801,\n",
       "  95.90643274853801,\n",
       "  95.90643274853801,\n",
       "  95.90643274853801,\n",
       "  95.90643274853801,\n",
       "  95.90643274853801,\n",
       "  95.90643274853801,\n",
       "  95.90643274853801,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544,\n",
       "  96.49122807017544]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9649122807017544"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_true=ytest, y_pred=ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2be5faf79681da6f2a61fdfdd5405d65d042280f7fba6178067603e3a2925119"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
